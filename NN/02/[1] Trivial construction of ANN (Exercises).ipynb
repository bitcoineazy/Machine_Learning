{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cоздание модели искусственной нейронной сети, состоящей из одного слоя\n",
    "(ANN --- Artificial Neural Networks)\n",
    "\n",
    "Напомним, что глубокое обучение базируется на искусственных нейронных сетях, которые получили своё развитие после 1960х годов (в 1965 году вышла статья Розенблатта).\n",
    "\n",
    "Модель нейрона выглядит следующим образом:\n",
    "\n",
    "<img src=\"assets/simple_neuron.png\" width=400px>\n",
    "\n",
    "Математически это можно записать как композицию функций:\n",
    "\n",
    "$$\n",
    "y = f(h) = f(w_1 x_1 + w_2 x_2 + b) = f\\left(\\sum_i w_i x_i +b \\right)\n",
    "$$\n",
    "\n",
    "$\\Sigma$ --- отвечает за умножение на веса и прибавление bias-вектора (напомним, что он необходим для того, чтобы добавить нелинейности, иначе для всех нулевых входных значений будут только нулевые выходные значения), результатом является значение $h$. Оно также может быть записано в векторной форме:\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$f$ --- функция активации, которая необходима для того, чтобы создавать более сложные конструкции и гиперповерхности, разделяющие данные.\n",
    "\n",
    "Создадим простейшую модель нейронной сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ШАГ 1. Создадим необходимые данные.\n",
    "\n",
    "Все данные хранятся в векторном виде --- они называются тензорами и основная библиотека работы PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подключить библиотеку PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Случайным образом создадим входные данные (features размера $1 \\times 5$), веса (weights такого же размера, как входные даннче) и bias-вектор (напомним, это одномерный вектор)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(5) # зафиксируем базовое число для случайной генерации\n",
    "\n",
    "features = torch.rand(1, 5)\n",
    "weights = torch.rand(5, 5)\n",
    "bias = torch.rand(1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте, что все размерности совпадают для реализации умножения матриц и сложения векторов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "torch.Size([5, 5])\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "print(features.size())\n",
    "print(weights.size())\n",
    "print(bias.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ШАГ 2. Теперь посчитаем значение h как результат операций над векторами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4975, 2.0610, 1.9144, 1.5370, 3.7060]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = features.matmul(weights) + bias\n",
    "\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ШАГ 3. Выберем функцию активации\n",
    "\n",
    "В данном примере в качестве функции активации будем использовать сигмоиду $\\sigma(x)$:\n",
    "\n",
    "$$\\large \\sigma(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L05/out/sigmoid_function.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function\n",
    "        Arguments\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ШАГ 4. Посчитаем выходное значение построенной нейронной сети.\n",
    "\n",
    "Найти значение $y = f(h)$. Какая размерность должна быть у выходного значения?\n",
    "Проверьте себя, что полученное значение верно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = activation(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8172, 0.8871, 0.8715, 0.8230, 0.9760]]) torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "print(y, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание нейронной сети, состоящей из двух слоёв\n",
    "\n",
    "Настоящая мощь нейронной сети начинает проявляться, когда мы формируем из нейронов слои, причём выходное значение для предыдущего слоя является входным значением для последующего слоя:\n",
    "\n",
    "<img src =\"assets/ANN_example.png\"  width=\"1000\">\n",
    "\n",
    "Тогда вектор весов превращается в матрицу весов, значения элементов которой нумеруются следующим образом: на первом месте стоит номер входного значения, на втором месте номер слоя.\n",
    "\n",
    "<img src='assets/multilayer_diagram_weights.png' width=\"1000\">\n",
    "\n",
    "Первый слой, состоящий из входных данных, так и называется --- **входной слой** (**input layer**). Промежуточный слой называется **скрытым слоем** (**hidden layer**) и последний слой называется **выходным слоем** (**output layer**).\n",
    "\n",
    "Нейронную сеть, представленную на картинке, можно математически записать как комбинацию функций. Например, для скрытых слоёв $h_1$ и $h_2$ можно выписать следующее выражение:\n",
    "\n",
    "$$\n",
    "\\vec{h} = [h_1 \\, h_2] =\n",
    "\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\, x_3\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "           w_{11} & w_{12} \\\\\n",
    "           w_{21} &w_{22} \\\\\n",
    "           w_{31} &w_{32}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Тогда вся нейронная сеть примет вид:\n",
    "\n",
    "$$\n",
    "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ШАГ 1. Создание необходимых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подключите необходимые библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE YOUR CODE HERE\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задайте входные значения вектором нужной размерности из случайных чисел."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7432, 0.9697, 0.0609, 0.4385, 0.9868]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## WRITE YOUR CODE HERE\n",
    "\n",
    "features = torch.rand(1, 5)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ШАГ 2. Зададим параметры архитектуры нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of each layer in our network\n",
    "n_input = 5                    # Number of input units, must match number of input features\n",
    "n_hidden = 2                   # Number of hidden units\n",
    "n_output = 5                   # Number of output units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ШАГ 3. Создадим веса и bias-векторы для каждого слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.rand(n_input, n_hidden)\n",
    "W2 = torch.rand(n_hidden, n_output)\n",
    "\n",
    "B1 = torch.rand(1, n_hidden)\n",
    "B2 = torch.rand(1, n_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ШАГ 4. Посчитаем значение выражения $h_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8813, 0.8674]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1 = features.matmul(W1) + B1\n",
    "h1 = activation(h1)\n",
    "\n",
    "h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ШАГ 5. Определим функцию активации\n",
    "\n",
    "В этом примере будем использовать функцию активации ReLU\n",
    "\n",
    "$$ ReLU(x) = max(0,x)=\n",
    "\\begin{cases}\n",
    "    x, \\text{ если } x > 0, \\\\\n",
    "    0,  \\text{ если } x \\le 0\n",
    "\\end{cases} $$\n",
    "\n",
    "<img src='assets/ReLU.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\" ReLU activation function\n",
    "        Arguments\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return torch.clip(x, min=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ШАГ 6. Применим функцию активации к значению $h_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7739, 1.4509, 0.6663, 1.3288, 2.0751]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2 = torch.matmul(h1, W2) + B2\n",
    "h2 = activation(h2)\n",
    "\n",
    "h2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ШАГ 7. Посчитаем выходное значение y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7739, 1.4509, 0.6663, 1.3288, 2.0751]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание: записать математически нейронную сеть, указанную на рисунке и закодировать основные параметры.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L05/out/modified_model.png\"  width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\large h=W_1 \\times x$$\n",
    "\n",
    "$$ \\large S=W_2 \\times f(h)=W_2 \\times f\n",
    "(W_1 \\times x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход подается тензор размерностью 3х3х72 (изображение), количество скрытых слоев - 100, количество выходов - 10. Надо подобрать такие значения весов W1 и W2 (после h) чтобы получить выходную размерность 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([648])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(3, 3, 72)\n",
    "\n",
    "x.flatten().shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First classifier shape: torch.Size([1, 100])\n",
      "Second classifier shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Параметры\n",
    "input_size = 3 * 3 * 72\n",
    "hidden_size = 100\n",
    "output_size = 10\n",
    "\n",
    "x = torch.randn(1, input_size)  # Размерность (1, input_size)\n",
    "\n",
    "W1 = torch.randn(input_size, hidden_size)  # Размерность (input_size, hidden_size)\n",
    "W2 = torch.randn(hidden_size, output_size)  # Размерность (hidden_size, output_size)\n",
    "\n",
    "\n",
    "h = torch.sigmoid(torch.mm(x, W1))\n",
    "\n",
    "S = torch.softmax(torch.mm(h, W2), dim=1) \n",
    "\n",
    "print(f\"First classifier shape: {h.shape}\")\n",
    "print(f\"Second classifier shape: {S.size()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы:\n",
    "\n",
    "Нейронная сеть в общем виде представлена следующей структурой:\n",
    "\n",
    "<center><img src='assets/nn_sheme_logits_softmax.png' width=\"1000\"><center>\n",
    "\n",
    "Остановимся подробнее на выходных значениях: после всех операций умножения на веса, прибавления bias-векторов и применения функций активации последний слой нейронной сети возвращает некоторые значения, которые можно интерпретировать как уверенность модели, что изображение принадлежит к какому-то классу.\n",
    " <img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/img_to_function_get_scores.png\" width=\"750\">\n",
    "\n",
    "\n",
    "Эти значения называются logits или логиты. Такое название взято из концепции функции $logit(x) = log \\frac{x}{1-x}$, поскольку функция logit показывает порядок отношения вероятности верного класса к вероятности неверного класса для данной картинки. То есть эти числа ассоциируются со входными значениями для функции logit.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/scores_to_probability.png\" width=\"750\">\n",
    "\n",
    "Но возникает небольшая путаница, сама функция $logit$ не используется в архитектуре нейронных сетей, но проводится аналогия с её главным свойством, которое можно увидеть на представленной картинке: изначально функция $logit$ создавалась как преобразование, которое переводит вероятности (то есть значения из интервала $(0;1)$) в метки классов (то есть значения на всей числовой оси $(-\\infty;+\\infty)$).  \n",
    "    \n",
    "А функция $softmax (s_k) = \\frac{e^{s_k}}{\\sum_je^{s_j}}$, где $s_i=f(x_i; W)$ - выходное значение логита для $i$-й картинки, как раз переводит все заданные значения, расположенные на всей числовой оси $(-\\infty;+\\infty)$ в вероятности принадлежности к классам, то есть в значения, лежащие в интервале $(0;1)$.\n",
    "\n",
    "То есть можно рассматривать функции $logit$ и $softmax$ как взаимно обратные. Грубо говоря, входными значениями для функции $softmax$ будут выходные значения функции $logit$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Резюмируем: последний линейный слой нейронной сети возвращает *логиты* — <<сырые>> значения из диапазона $(-\\infty; +\\infty)$, которые могут быть пропущены через модуль [`nn.Softmax`](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html). Пропущенные через $\\text{sofmax}$ величины могут восприниматься как вероятности, с которыми модель относит данный объект к тому или иному классу. Параметр `dim` определяет размерность, вдоль которой величины должны суммироваться к $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-11blKEOcjlk"
      },
      "source": [
        "# 2. Создание нейронной сети без использования готовых решений\n",
        "\n",
        "__Автор__: Никита Владимирович Блохин (NVBlokhin@fa.ru)\n",
        "\n",
        "Финансовый университет, 2020 г. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PqC4R7SGseKa"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J2RM8f5wP33"
      },
      "source": [
        "## 1. Создание нейронов и полносвязных слоев"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2ArJn_nsdZC"
      },
      "source": [
        "1.1. Используя операции над матрицами и векторами из библиотеки `torch`, реализовать нейрон с заданными весами `weights` и `bias`. Прогнать вектор `inputs` через нейрон и вывести результат. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4agkY9WqPwe"
      },
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "  def __init__(self, weights, bias):\n",
        "    # <создать атрибуты объекта weights и bias>\n",
        "    self.weights = weights\n",
        "    self.bias = bias\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    return (inputs * self.weights).sum() + self.bias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJRkSkHHsb7u"
      },
      "outputs": [],
      "source": [
        "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
        "weights = torch.tensor([-0.2, 0.3, -0.5, 0.7])\n",
        "bias = 3.14"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qJvnwiyty37"
      },
      "source": [
        "1.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой с заданными весами `weights` и `biases`. Прогнать вектор `inputs` через слой и вывести результат. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fVWF3a9vtx90"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "  def __init__(self, weights, biases):\n",
        "    self.weights = weights\n",
        "    self.biases = biases\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    return torch.mv(self.weights, inputs)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fo-JFnHPuFCS"
      },
      "outputs": [],
      "source": [
        "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
        "weights = torch.tensor([[-0.2, 0.3, -0.5, 0.7],\n",
        "                        [0.5, -0.91, 0.26, -0.5],\n",
        "                        [-0.26, -0.27, 0.17, 0.87]])\n",
        "\n",
        "biases = torch.tensor([3.14, 2.71, 7.2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 1.7000, -2.5400,  3.1900])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "linear = Linear(weights, biases)\n",
        "\n",
        "linear.forward(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRVH_2K7xTBC"
      },
      "source": [
        "## 2. Создание функций активации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9kngE6Fxs9D"
      },
      "source": [
        "2.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ReLU:\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/f4353f4e3e484130504049599d2e7b040793e1eb)\n",
        "\n",
        "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jZLvMRByxSTC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 0, 0, 1, 2])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class ReLU:\n",
        "  def forward(self, inputs):\n",
        "    return inputs.clip(min=0)\n",
        "\n",
        "relu = ReLU()\n",
        "\n",
        "relu.forward(torch.tensor([1, -1, -6, 1, 2]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puExCWiKyTtb"
      },
      "source": [
        "2.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации softmax:\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/6d7500d980c313da83e4117da701bf7c8f1982f5)\n",
        "\n",
        "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации. Строки матрицы трактовать как выходы линейного слоя некоторого классификатора для 4 различных примеров."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fXNcFlqqyKHl"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0414, 0.1125, 0.8310, 0.0152],\n",
              "        [0.0889, 0.2418, 0.0120, 0.6572]])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class Softmax:\n",
        "  def forward(self, inputs):\n",
        "    # <реализовать логику Softmax>\n",
        "    exp = torch.exp(inputs)\n",
        "    return exp / exp.sum(dim=1, keepdim=True)\n",
        "\n",
        "softmax = Softmax()\n",
        "\n",
        "softmax.forward(torch.tensor([[1, 2, 4, 0], [3, 4, 1., 5]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 7.],\n",
              "        [13.]])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.tensor([[1, 2, 4, 0], [3, 4, 1., 5]]).sum(1, keepdim=True)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([20.])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.sum(-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0peh8r-20Pof"
      },
      "source": [
        "## 3. Создание функции потерь"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY-k3eEs0f7f"
      },
      "source": [
        "3.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь MSE:\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e258221518869aa1c6561bb75b99476c4734108e)\n",
        "\n",
        "Посчитать значение MSE, трактуя вектор `y` как вектор правильных ответов, а `y_pred`, как вектор предсказаний."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "f9-wdj5Tz-br"
      },
      "outputs": [],
      "source": [
        "class MSELoss:\n",
        "  def forward(self, y_pred, y_true):\n",
        "    return torch.mean((y_pred - y_true)**2)/len(y_true) # <реализовать логику MSE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "NAyuDU9F1Vuz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.3333)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
        "\n",
        "y = torch.tensor([2, 3, 4], dtype=torch.float32)\n",
        "\n",
        "\n",
        "mse = MSELoss()\n",
        "\n",
        "mse.forward(y_pred, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w049ZSdR6qQi"
      },
      "source": [
        "## 4. Обратное распространение ошибки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBtCfSME9W7Q"
      },
      "source": [
        "4.1 Используя один нейрон и SGD (1 пример за шаг), решите задачу регрессии"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "4xmI-QJ66WAF"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "\n",
        "X, y, coef = make_regression(n_features=4, n_informative=4, coef=True, bias=0.5)\n",
        "X = torch.tensor(X, dtype=torch.float32) # <преобразуйте массивы numpy в тензоры torch с типом torch.float32\n",
        "y = torch.tensor(y, dtype=torch.float32) # <преобразуйте массивы numpy в тензоры torch с типом torch.float32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([56.62478423,  3.41587313, 28.41364284, 70.16964119])"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "coef"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpPSPYSpD9Ey"
      },
      "source": [
        "[Граф вычислений для этой задачи](https://i.ibb.co/2dhDxZx/photo-2021-02-15-17-18-04.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc1sXtGd_J-y"
      },
      "source": [
        "4.1.1 Модифицируйте класс `MSELoss` из __2.3.1__, реализовав расчет производной относительно предыдущего слоя\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "llFigkqd_JRU"
      },
      "outputs": [],
      "source": [
        "class MSELoss:\n",
        "  def forward(self, y_pred, y_true):\n",
        "    return torch.mean((y_pred - y_true)**2) # <реализовать логику MSE>\n",
        "\n",
        "  def backward(self, y_pred, y_true):\n",
        "    self.dinput = 2 * (y_pred - y_true) # df/dc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY7ForfM97UQ"
      },
      "source": [
        "4.1.2. Модифицируйте класс `Neuron` из __2.1.1__:\n",
        "\n",
        "  1) Сделайте так, чтобы веса нейрона инициализировались из стандартного нормального распределения\n",
        "\n",
        "  2) Реализуйте расчет градиента относительно весов `weights` и `bias`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "L0KqxPJU9kAN"
      },
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "  def __init__(self, n_inputs):\n",
        "    # <создать атрибуты объекта weights и bias>\n",
        "    # self.n_inputs = n_inputs\n",
        "    self.weights = torch.randn(n_inputs)\n",
        "    self.bias = torch.randn(1)\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    self.input = inputs\n",
        "    return (inputs * self.weights).sum() + self.bias # <реализовать логику нейрона>\n",
        "  \n",
        "  def backward(self, dvalue):\n",
        "    # dvalue - значение производной, которое приходит нейрону от следующего слоя сети\n",
        "    # в данном случае это будет значение df/dc (созданное методом backwards у объекта MSELoss)\n",
        "    self.dweights = dvalue * self.input # df/dW\n",
        "    self.dinput = dvalue * self.weights # df/wX\n",
        "    self.dbias = dvalue # df/db\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKcO4zOLACxM"
      },
      "source": [
        "4.1.3 Допишите цикл для настройки весов нейрона\n",
        "\n",
        "[SGD](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D1%81%D0%BF%D1%83%D1%81%D0%BA)\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/dda3670f8a8996a0d3bf80856bb4a166cc8db6d4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "_g_FvwvmALJd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 mean loss 473.4125061035156\n",
            "epoch 5 mean loss 78.90208435058594\n",
            "epoch 10 mean loss 43.037498474121094\n",
            "epoch 15 mean loss 29.588281631469727\n",
            "epoch 20 mean loss 22.543453216552734\n",
            "epoch 25 mean loss 18.208173751831055\n",
            "epoch 30 mean loss 15.271370887756348\n",
            "epoch 35 mean loss 13.150346755981445\n",
            "epoch 40 mean loss 11.546646118164062\n",
            "epoch 45 mean loss 10.291576385498047\n",
            "epoch 50 mean loss 9.282598495483398\n",
            "epoch 55 mean loss 8.453794479370117\n",
            "epoch 60 mean loss 7.760860443115234\n",
            "epoch 65 mean loss 7.172916889190674\n",
            "epoch 70 mean loss 6.667781829833984\n",
            "epoch 75 mean loss 6.229111671447754\n",
            "epoch 80 mean loss 5.844598770141602\n",
            "epoch 85 mean loss 5.504796504974365\n",
            "epoch 90 mean loss 5.202335357666016\n",
            "epoch 95 mean loss 4.931380271911621\n",
            "epoch 100 mean loss 4.687252521514893\n",
            "epoch 105 mean loss 4.466155529022217\n",
            "epoch 110 mean loss 4.26497745513916\n",
            "epoch 115 mean loss 4.081142425537109\n",
            "epoch 120 mean loss 3.9124999046325684\n",
            "epoch 125 mean loss 3.75724196434021\n",
            "epoch 130 mean loss 3.6138358116149902\n",
            "epoch 135 mean loss 3.4809741973876953\n",
            "epoch 140 mean loss 3.3575353622436523\n",
            "epoch 145 mean loss 3.242551326751709\n",
            "epoch 150 mean loss 3.1351821422576904\n",
            "epoch 155 mean loss 3.034695625305176\n",
            "epoch 160 mean loss 2.9404501914978027\n",
            "epoch 165 mean loss 2.8518824577331543\n",
            "epoch 170 mean loss 2.768494129180908\n",
            "epoch 175 mean loss 2.6898436546325684\n",
            "epoch 180 mean loss 2.6155385971069336\n",
            "epoch 185 mean loss 2.5452284812927246\n",
            "epoch 190 mean loss 2.4785995483398438\n",
            "epoch 195 mean loss 2.415369987487793\n",
            "epoch 200 mean loss 2.355286121368408\n",
            "epoch 205 mean loss 2.298118829727173\n",
            "epoch 210 mean loss 2.2436611652374268\n",
            "epoch 215 mean loss 2.1917245388031006\n",
            "epoch 220 mean loss 2.1421380043029785\n",
            "epoch 225 mean loss 2.094745635986328\n",
            "epoch 230 mean loss 2.0494048595428467\n",
            "epoch 235 mean loss 2.0059852600097656\n",
            "epoch 240 mean loss 1.9643672704696655\n",
            "epoch 245 mean loss 1.9244410991668701\n",
            "epoch 250 mean loss 1.8861055374145508\n",
            "epoch 255 mean loss 1.849267601966858\n",
            "epoch 260 mean loss 1.8138409852981567\n",
            "epoch 265 mean loss 1.7797462940216064\n",
            "epoch 270 mean loss 1.7469096183776855\n",
            "epoch 275 mean loss 1.7152626514434814\n",
            "epoch 280 mean loss 1.6847419738769531\n",
            "epoch 285 mean loss 1.6552884578704834\n",
            "epoch 290 mean loss 1.6268470287322998\n",
            "epoch 295 mean loss 1.599366545677185\n",
            "epoch 300 mean loss 1.5727989673614502\n",
            "epoch 305 mean loss 1.5470997095108032\n",
            "epoch 310 mean loss 1.5222266912460327\n",
            "epoch 315 mean loss 1.498140811920166\n",
            "epoch 320 mean loss 1.4748053550720215\n",
            "epoch 325 mean loss 1.4521856307983398\n",
            "epoch 330 mean loss 1.4302492141723633\n",
            "epoch 335 mean loss 1.408965826034546\n",
            "epoch 340 mean loss 1.3883064985275269\n",
            "epoch 345 mean loss 1.3682441711425781\n",
            "epoch 350 mean loss 1.348753571510315\n",
            "epoch 355 mean loss 1.329810380935669\n",
            "epoch 360 mean loss 1.3113919496536255\n",
            "epoch 365 mean loss 1.2934768199920654\n",
            "epoch 370 mean loss 1.276044487953186\n",
            "epoch 375 mean loss 1.2590757608413696\n",
            "epoch 380 mean loss 1.2425525188446045\n",
            "epoch 385 mean loss 1.2264572381973267\n",
            "epoch 390 mean loss 1.2107737064361572\n",
            "epoch 395 mean loss 1.195486068725586\n",
            "epoch 400 mean loss 1.1805797815322876\n",
            "epoch 405 mean loss 1.1660406589508057\n",
            "epoch 410 mean loss 1.151855230331421\n",
            "epoch 415 mean loss 1.138010859489441\n",
            "epoch 420 mean loss 1.124495267868042\n",
            "epoch 425 mean loss 1.1112968921661377\n",
            "epoch 430 mean loss 1.098404884338379\n",
            "epoch 435 mean loss 1.085808515548706\n",
            "epoch 440 mean loss 1.0734977722167969\n",
            "epoch 445 mean loss 1.0614629983901978\n",
            "epoch 450 mean loss 1.0496951341629028\n",
            "epoch 455 mean loss 1.0381853580474854\n",
            "epoch 460 mean loss 1.0269252061843872\n",
            "epoch 465 mean loss 1.0159066915512085\n",
            "epoch 470 mean loss 1.0051220655441284\n",
            "epoch 475 mean loss 0.9945640563964844\n",
            "epoch 480 mean loss 0.9842255711555481\n",
            "epoch 485 mean loss 0.9740998148918152\n",
            "epoch 490 mean loss 0.9641802310943604\n",
            "epoch 495 mean loss 0.9544606804847717\n",
            "epoch 500 mean loss 0.944935142993927\n",
            "epoch 505 mean loss 0.935597836971283\n",
            "epoch 510 mean loss 0.9264432191848755\n",
            "epoch 515 mean loss 0.9174661040306091\n",
            "epoch 520 mean loss 0.9086612462997437\n",
            "epoch 525 mean loss 0.9000237584114075\n",
            "epoch 530 mean loss 0.8915489912033081\n",
            "epoch 535 mean loss 0.8832322955131531\n",
            "epoch 540 mean loss 0.8750693202018738\n",
            "epoch 545 mean loss 0.8670558333396912\n",
            "epoch 550 mean loss 0.8591878414154053\n",
            "epoch 555 mean loss 0.8514613509178162\n",
            "epoch 560 mean loss 0.8438725471496582\n",
            "epoch 565 mean loss 0.8364178538322449\n",
            "epoch 570 mean loss 0.8290936946868896\n",
            "epoch 575 mean loss 0.8218966722488403\n",
            "epoch 580 mean loss 0.814823567867279\n",
            "epoch 585 mean loss 0.8078711628913879\n",
            "epoch 590 mean loss 0.8010363578796387\n",
            "epoch 595 mean loss 0.794316291809082\n",
            "epoch 600 mean loss 0.7877079844474792\n",
            "epoch 605 mean loss 0.7812087535858154\n",
            "epoch 610 mean loss 0.7748158574104309\n",
            "epoch 615 mean loss 0.7685267925262451\n",
            "epoch 620 mean loss 0.7623389959335327\n",
            "epoch 625 mean loss 0.7562500238418579\n",
            "epoch 630 mean loss 0.7502575516700745\n",
            "epoch 635 mean loss 0.7443592548370361\n",
            "epoch 640 mean loss 0.7385530471801758\n",
            "epoch 645 mean loss 0.7328366637229919\n",
            "epoch 650 mean loss 0.727208137512207\n",
            "epoch 655 mean loss 0.7216653823852539\n",
            "epoch 660 mean loss 0.7162064909934998\n",
            "epoch 665 mean loss 0.7108295559883118\n",
            "epoch 670 mean loss 0.7055327892303467\n",
            "epoch 675 mean loss 0.7003143429756165\n",
            "epoch 680 mean loss 0.6951725482940674\n",
            "epoch 685 mean loss 0.690105676651001\n",
            "epoch 690 mean loss 0.6851121783256531\n",
            "epoch 695 mean loss 0.68019038438797\n",
            "epoch 700 mean loss 0.6753388047218323\n",
            "epoch 705 mean loss 0.6705559492111206\n",
            "epoch 710 mean loss 0.6658403873443604\n",
            "epoch 715 mean loss 0.6611906290054321\n",
            "epoch 720 mean loss 0.6566054224967957\n",
            "epoch 725 mean loss 0.6520833373069763\n",
            "epoch 730 mean loss 0.6476231217384338\n",
            "epoch 735 mean loss 0.6432235240936279\n",
            "epoch 740 mean loss 0.6388832926750183\n",
            "epoch 745 mean loss 0.6346012353897095\n",
            "epoch 750 mean loss 0.6303761601448059\n",
            "epoch 755 mean loss 0.6262069940567017\n",
            "epoch 760 mean loss 0.6220926642417908\n",
            "epoch 765 mean loss 0.6180319786071777\n",
            "epoch 770 mean loss 0.6140239834785461\n",
            "epoch 775 mean loss 0.6100676655769348\n",
            "epoch 780 mean loss 0.6061619520187378\n",
            "epoch 785 mean loss 0.6023060083389282\n",
            "epoch 790 mean loss 0.5984987616539001\n",
            "epoch 795 mean loss 0.5947393178939819\n",
            "epoch 800 mean loss 0.5910268425941467\n",
            "epoch 805 mean loss 0.5873604416847229\n",
            "epoch 810 mean loss 0.5837392210960388\n",
            "epoch 815 mean loss 0.5801624059677124\n",
            "epoch 820 mean loss 0.576629102230072\n",
            "epoch 825 mean loss 0.5731385946273804\n",
            "epoch 830 mean loss 0.5696901082992554\n",
            "epoch 835 mean loss 0.5662828683853149\n",
            "epoch 840 mean loss 0.5629161596298218\n",
            "epoch 845 mean loss 0.5595892667770386\n",
            "epoch 850 mean loss 0.5563014149665833\n",
            "epoch 855 mean loss 0.5530520081520081\n",
            "epoch 860 mean loss 0.5498403310775757\n",
            "epoch 865 mean loss 0.5466657280921936\n",
            "epoch 870 mean loss 0.5435275435447693\n",
            "epoch 875 mean loss 0.5404252409934998\n",
            "epoch 880 mean loss 0.5373581051826477\n",
            "epoch 885 mean loss 0.5343255996704102\n",
            "epoch 890 mean loss 0.5313271880149841\n",
            "epoch 895 mean loss 0.5283621549606323\n",
            "epoch 900 mean loss 0.5254300832748413\n",
            "epoch 905 mean loss 0.5225303769111633\n",
            "epoch 910 mean loss 0.5196624398231506\n",
            "epoch 915 mean loss 0.5168258547782898\n",
            "epoch 920 mean loss 0.5140200853347778\n",
            "epoch 925 mean loss 0.5112445950508118\n",
            "epoch 930 mean loss 0.5084989070892334\n",
            "epoch 935 mean loss 0.5057826042175293\n",
            "epoch 940 mean loss 0.5030950903892517\n",
            "epoch 945 mean loss 0.5004360675811768\n",
            "epoch 950 mean loss 0.4978049397468567\n",
            "epoch 955 mean loss 0.49520134925842285\n",
            "epoch 960 mean loss 0.4926248788833618\n",
            "epoch 965 mean loss 0.4900750517845154\n",
            "epoch 970 mean loss 0.4875514805316925\n",
            "epoch 975 mean loss 0.48505377769470215\n",
            "epoch 980 mean loss 0.48258155584335327\n",
            "epoch 985 mean loss 0.48013436794281006\n",
            "epoch 990 mean loss 0.47771191596984863\n",
            "epoch 995 mean loss 0.4753137528896332\n"
          ]
        }
      ],
      "source": [
        "n_inputs = 4 # <размерность элемента выборки >\n",
        "learning_rate = 0.1 #  скорость обучения\n",
        "n_epoch = 100 #  количество эпох\n",
        "\n",
        "neuron = Neuron(n_inputs)\n",
        "loss = MSELoss()\n",
        "\n",
        "losses = []\n",
        "\n",
        "# print(zip(X, y))\n",
        "for epoch in range(1000):\n",
        "  \n",
        "  for x_example, y_example in zip(X, y):\n",
        "    # forward pass\n",
        "    y_pred = neuron.forward(x_example) # <прогон через нейрон>\n",
        "    curr_loss = loss.forward(y_pred, y_example) # <прогон через функцию потерь>\n",
        "    losses.append(curr_loss)\n",
        "\n",
        "    # backprop\n",
        "    # <вызов методов backward>\n",
        "    loss.backward(y_pred, y_example)\n",
        "    neuron.backward(loss.dinput)\n",
        "    # обратите внимание на последовательность вызовов: от конца к началу\n",
        "    # print(neuron.weights)\n",
        "    neuron.weights -= learning_rate * neuron.dweights\n",
        "    neuron.bias -= learning_rate * neuron.dbias\n",
        "    # <шаг оптимизации для весов (weights и bias) нейрона>\n",
        "\n",
        "  if epoch % 5 == 0:\n",
        "    print(f\"epoch {epoch} mean loss {torch.stack(losses).mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_ief7m1D2rJ"
      },
      "source": [
        "4.2 Работа с батчами"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr9qq4H_J3zt"
      },
      "source": [
        "4.2.1 Модифицируйте класс `MSELoss` из __3.1__, реализовав расчет производной относительно предыдущего слоя с учетом того, что теперь работа ведется с батчами, а не с индивидуальными примерами\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "L8wjk9iPMQ4x"
      },
      "outputs": [],
      "source": [
        "class MSELoss:\n",
        "  def forward(self, y_pred, y_true):\n",
        "    return torch.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "  def backward(self, y_pred, y_true):\n",
        "    self.dinput = 2 * (y_pred - y_true) / len(y_true) # df/dy^\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3fSHCEtJjX8"
      },
      "source": [
        "4.2.2. Модифицируйте класс `Neuron` из __4.1.2__:\n",
        "\n",
        "  1) Реализуйте метод `forward` таким образом, чтобы он мог принимать на вход матрицу (батч) с данными. \n",
        "\n",
        "  2) Реализуйте расчет градиента относительно весов `weights` и `bias` с учетом того, что теперь работа ведется с батчами, а не с индивидуальными примерами"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "o_OpuAP0Jpz1"
      },
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "  def __init__(self, n_inputs):\n",
        "    # <создать атрибуты объекта weights и bias>\n",
        "    self.weights = torch.randn(n_inputs)\n",
        "    self.bias = torch.tensor(1)\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    self.inputs = inputs\n",
        "    return X @ self.weights.T + self.bias # <реализовать логику нейрона>\n",
        "  \n",
        "  def backward(self, dvalue):\n",
        "    # dvalue - значение градиента, которое приходит нейрону от следующего слоя сети\n",
        "    # в данном случае это будет градиент L по y^ (созданный методом backwards у объекта MSELoss)\n",
        "    self.dweights = self.inputs.T.mv(dvalues) # df/dW\n",
        "    self.dbias = dvalues.sum() # df/db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 1.5436, -0.7334, -3.5034,  3.7786,  0.3309,  0.6941, -0.6677, -0.5985,\n",
              "         5.2927, -1.9528,  1.0723,  0.5473,  3.3397,  0.2918,  1.3232,  6.6151,\n",
              "         1.6719, -0.9794,  4.0265, -3.3048, -2.1791,  1.0269,  3.3949, -2.4096,\n",
              "         2.4610, -1.9779, -0.7896, -3.5132,  0.2139,  0.8727,  2.8040,  2.0103,\n",
              "         2.4520,  6.9876, -0.5074,  6.1217,  3.0923,  2.1367,  3.6616,  2.1603,\n",
              "        -1.8690,  3.1847,  0.7969,  1.7345, -2.0365,  3.2037,  3.4775, -1.2714,\n",
              "         0.6715,  2.3635, -0.3252, -3.7998,  0.3381,  6.1449, -4.3615,  3.5708,\n",
              "         4.6176,  1.8989,  2.8456, -2.4962,  0.9244, -2.6139,  3.8309,  7.2379,\n",
              "         2.3081,  2.6172, -3.0549,  4.5765, -3.3604,  5.2114, -1.1295,  0.1421,\n",
              "         5.6711,  4.1402,  0.3938, -0.0890, -0.3209, -0.3914, -0.8806, -2.7761,\n",
              "        -1.1531, -3.1797,  8.5684,  2.1941, -2.6391,  3.6554,  2.5856,  3.5995,\n",
              "         4.3258,  4.8613, -5.4909, -0.9576,  4.3073, -1.3862, -5.2191,  3.4535,\n",
              "         5.1568,  1.8934,  1.3001, -0.6002])"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "neuron = Neuron(4)\n",
        "\n",
        "neuron.forward(torch.randn((2, 4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVpBoMRMlM5Z"
      },
      "source": [
        "# Лабораторная (домашняя) работа 1.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZthSpoJI_dLu"
      },
      "source": [
        "## 1. Создание нейронов и полносвязных слоев"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQtsJzcxuyGd"
      },
      "source": [
        "1.3 Реализовать полносвязный слой из __2.1.2__ таким образом, чтобы он мог принимать на вход матрицу (батч) с данными. Продемонстрировать работу.\n",
        "Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8IizmtsuhO1"
      },
      "outputs": [],
      "source": [
        "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
        "                       [2, 5, -1, 2], \n",
        "                       [-1.5, 2.7, 3.3, -0.8]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ2OxH4_vBLu"
      },
      "source": [
        "1.4 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой из `n_neurons` нейронов с `n_features` весами у каждого нейрона (инициализируются из стандартного нормального распределения). Прогнать вектор `inputs` через слой и вывести результат. Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOv52EdovASs"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "  def __init__(self, n_features, n_neurons):\n",
        "    # <создать атрибуты объекта weights и biases>\n",
        "    pass\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    return # <реализовать логику слоя>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPG4UqL4wajI"
      },
      "source": [
        "1.5 Используя решение из __1.4__, создать 2 полносвязных слоя и пропустить матрицу `inputs` последовательно через эти два слоя. Количество нейронов в первом слое выбрать произвольно, количество нейронов во втором слое выбрать так, чтобы результатом прогона являлась матрица (3x7). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjjQIQlTxJE6"
      },
      "outputs": [],
      "source": [
        "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
        "                       [2, 5, -1, 2], \n",
        "                       [-1.5, 2.7, 3.3, -0.8]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4YcuWJI_gA-"
      },
      "source": [
        "## 2. Создание функций активации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxVK2TYez_Ye"
      },
      "source": [
        "2.3 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ELU:\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/eb23becd37c3602c4838e53f532163279192e4fd)\n",
        "\n",
        "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzMz7HDLySxK"
      },
      "outputs": [],
      "source": [
        "class ELU:\n",
        "  def __init__(self, alpha):\n",
        "    # <создать атрибут объекта alpha>\n",
        "    pass\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # <реализовать логику ReLU>\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aH5R4gL_ijM"
      },
      "source": [
        "## 3. Создание функции потерь"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaR7rILd1eWR"
      },
      "source": [
        "3.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь Categorical Cross-Entropy:\n",
        "\n",
        "<img src=\"https://i.ibb.co/93gy1dN/Screenshot-9.png\" width=\"200\">\n",
        "\n",
        "Создать полносвязный слой с 3 нейронами и прогнать через него батч `inputs`. Полученный результат пропустить через функцию активации softmax. Посчитать значение CCE, трактуя вектор `y` как вектор правильных ответов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQl8pJsT3HcF"
      },
      "outputs": [],
      "source": [
        "class CategoricalCrossentropyLoss:\n",
        "  def forward(self, y_pred, y_true):\n",
        "    # <реализовать логику CCE>\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7Qoupfo1ZGJ"
      },
      "outputs": [],
      "source": [
        "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
        "                        [2, 5, -1, 2], \n",
        "                        [-1.5, 2.7, 3.3, -0.8]])\n",
        "y = torch.tensor([1, 0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA6dbanf44_4"
      },
      "source": [
        "3.3 Модифицировать 2.3.1, добавив L2-регуляризацию.\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/d92ca2429275bfdc0474523babbafe014ca8b580)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADsZxD-h4_Os"
      },
      "outputs": [],
      "source": [
        "class MSELossL2:\n",
        "  def __init__(self, lambda_):\n",
        "    # <создать атрибут объекта alpha>\n",
        "    pass\n",
        "\n",
        "  def data_loss(self, y_pred, y_true):\n",
        "    # <подсчет первого слагаемого из формулы>\n",
        "    pass\n",
        "\n",
        "  def reg_loss(self, layer):\n",
        "    # используйте атрибуты объекта layer, в которых хранятся веса слоя\n",
        "    # <подсчет второго слагаемого из формулы>\n",
        "    pass\n",
        "\n",
        "  def forward(self, y_pred, y_true):\n",
        "    return self.data_loss(y_pred, y_true) + self.reg_loss(y_pred, y_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQeZW7rJ_lpN"
      },
      "source": [
        "## 4. Обратное распространение ошибки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebibge9VEgF7"
      },
      "source": [
        "4.2 Решите задачу 4.1, используя пакетный градиентный спуск"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as-QeWSdOELd"
      },
      "source": [
        "Вычисления для этой задачи: \n",
        "[1](https://i.ibb.co/rmtQT6P/photo-2021-02-15-18-00-43.jpg)\n",
        "[2](https://i.ibb.co/NmCFVnQ/photo-2021-02-15-18-01-17.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO-NZrgKMBFx"
      },
      "source": [
        "4.2.3 Допишите цикл для настройки весов нейрона"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zqwm_7eqJim1"
      },
      "outputs": [],
      "source": [
        "n_inputs = # <размерность элемента выборки >\n",
        "learning_rate = 0.1 #  скорость обучения\n",
        "n_epoch = 100 #  количество эпох\n",
        "\n",
        "neuron = Neuron(n_inputs)\n",
        "loss = MSELoss()\n",
        "\n",
        "\n",
        "for epoch in range(100):\n",
        "    # forward pass\n",
        "    y_pred = # <прогон через нейрон>\n",
        "    curr_loss = # <прогон через функцию потерь>\n",
        "    losses.append(curr_loss)\n",
        "\n",
        "    # backprop\n",
        "    # <вызов методов backward>\n",
        "    # обратите внимание на последовательность вызовов: от конца к началу\n",
        "\n",
        "    # <шаг оптимизации для весов (weights и bias) нейрона>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16VtP159OdMk"
      },
      "source": [
        "4.3  Используя один полносвязный слой и  пакетный градиетный спуск, решите задачу регрессии из __2.4.1__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj5febreSSZ7"
      },
      "source": [
        "4.3.1 Модифицируйте класс `Linear` из __1.4__. ([вычисление градиентов](https://i.ibb.co/kgVR6m6/photo-2021-02-15-21-30-28.jpg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zWuhaLdSB2_"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "  def __init__(self, n_features, n_neurons):\n",
        "    # <создать атрибуты объекта weights и biases>\n",
        "    pass\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    return # <реализовать логику слоя>\n",
        "\n",
        "  def backward(self, dvalues):\n",
        "    self.dweights = # df/dW\n",
        "    self.dbiases = # df/db\n",
        "    self.dinputs = # df/dX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3w1hT9MS_Lt"
      },
      "source": [
        "4.3.2 Создайте слой с одним нейроном. Используя класс MSELoss из 2.4.2, убедитесь, что модель обучается"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTkJV-F8TVuN"
      },
      "source": [
        "4.4 Используя наработки из 2.4, создайте нейросеть и решите задачу регрессии.\n",
        "\n",
        "Предлагаемая архитектура: \n",
        "1. Полносвязный слой с 10 нейронами\n",
        "2. Активация ReLU\n",
        "3. Полносвязный слой с 1 нейроном"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axUjpPz-SvS1"
      },
      "outputs": [],
      "source": [
        "X = torch.linspace(-1, 1, 100).view(-1, 1)\n",
        "y = X.pow(2) + 0.2 * torch.rand(X.size()) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXoiNxkpTziV"
      },
      "outputs": [],
      "source": [
        "class Activation_ReLU:\n",
        "  def forward(self, inputs):\n",
        "    self.inputs = inputs\n",
        "    self.output = inputs.clip(min=0)\n",
        "    return self.output\n",
        "  \n",
        "  def backward(self, dvalues):\n",
        "    self.dinputs = dvalues.clone()\n",
        "    self.dinputs[self.inputs <= 0] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXhspwW6T44T"
      },
      "outputs": [],
      "source": [
        "# создание компонентов сети\n",
        "# fc1 = \n",
        "# relu1 = \n",
        "# fc2 = \n",
        "\n",
        "loss = MSELoss()\n",
        "lr = 0.02\n",
        "\n",
        "ys = []\n",
        "for epoch in range(2001):\n",
        "  # <forward pass>\n",
        "  # fc1 > relu1 > fc2 > loss\n",
        "\n",
        "  data_loss = # <прогон через функцию потерь>\n",
        "\n",
        "  if epoch % 200 == 0:\n",
        "    print(f'epoch {epoch} mean loss {data_loss}')\n",
        "    ys.append(out)\n",
        "  \n",
        "  # <backprop> \n",
        "  # loss > fc2 > relu1 > fc1\n",
        "\n",
        "  # <шаг оптимизации для fc1>\n",
        "\n",
        "  # <шаг оптимизации для fc2>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpKi0OfoUkwk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(len(ys), 1, figsize=(10, 40))\n",
        "for ax, y_ in zip(axs, ys):\n",
        "  ax.scatter(X.numpy(), y.numpy(), color = \"orange\")\n",
        "  ax.plot(X.numpy(), y_.numpy(), 'g-', lw=3)\n",
        "  ax.set_xlim(-1.05, 1.5)\n",
        "  ax.set_ylim(-0.25, 1.25)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

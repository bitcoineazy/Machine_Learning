{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKMq7dp2W15Y",
        "outputId": "ce2273c5-6a96-4216-9d88-fbee51bf5ff0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/noble6/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBkqaK5bawXN",
        "outputId": "9a2f2795-0b30-4d49-cf0c-286c5a6da777"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm-QilGISxkt"
      },
      "source": [
        "## 1. Классификация фамилий (RNN)\n",
        "\n",
        "Датасет: https://disk.yandex.ru/d/frNchuaBQVLxyA?w=1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YdPr92i6k-If"
      },
      "source": [
        "1.1 Используя класс `nn.RNNCell` (абстракцию для отдельного временного шага RNN), реализуйте простейшую рекуррентную сеть Элмана в виде класса `RNN`. Используя созданный класс `RNN`, решите задачу классификации фамилий. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ir6UUkl6l4tp"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "  def __init__(self, data):\n",
        "    tokens = set()\n",
        "    max_seq_len = 0\n",
        "    for item in data:\n",
        "        max_seq_len = max(max_seq_len, len(item))\n",
        "        tokens.update(item)\n",
        "\n",
        "    self.idx_to_token = {0: '<PAD>'}\n",
        "    self.token_to_idx = {'<PAD>': 0}\n",
        "    for idx, token in enumerate(tokens, start=1):\n",
        "        self.idx_to_token[idx] = token\n",
        "        self.token_to_idx[token] = idx\n",
        "    self.vocab_len = len(self.idx_to_token)\n",
        "    self.max_seq_len = max_seq_len\n",
        "\n",
        "class SurnamesDataset(Dataset):\n",
        "  def __init__(self, X, y, vocab: Vocab):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.vocab = vocab\n",
        "\n",
        "  def vectorize(self, surname):\n",
        "    surname_t = torch.zeros(self.vocab.max_seq_len, dtype=torch.int64)\n",
        "    for i, token in enumerate(surname):\n",
        "        if i >= self.vocab.max_seq_len:\n",
        "            break\n",
        "        surname_t[i] = self.vocab.token_to_idx.get(token, 0)\n",
        "    return surname_t\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    surname = self.X.iloc[idx]\n",
        "    label = self.y.iloc[idx]\n",
        "    surname_t = self.vectorize(surname)\n",
        "    return surname_t, label\n",
        "\n",
        "\n",
        "surnames = pd.read_csv(\"data/surnames.csv\")\n",
        "surnames['nationality'], _ = pd.factorize(surnames['nationality'])\n",
        "\n",
        "X = surnames['surname'].str.lower()\n",
        "y = surnames['nationality']\n",
        "n_classes = y.nunique()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "vocab = Vocab(X)\n",
        "\n",
        "train_dataset = SurnamesDataset(X_train, y_train, vocab)\n",
        "test_dataset = SurnamesDataset(X_test, y_test, vocab)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def train_model(model, train_dataloader, test_dataloader, criterion, optimizer, num_epochs):\n",
        "    # model.to(device)\n",
        "    train_losses, test_losses = [], []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss, test_loss = 0, 0\n",
        "        for inputs, labels in train_dataloader:\n",
        "            x = inputs#.to(device)\n",
        "            y = labels#.to(device)\n",
        "            \n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "    \n",
        "        # Валидация на val_loader\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_dataloader:\n",
        "                inputs = inputs#.to(device)\n",
        "                labels = labels#.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "        \n",
        "        train_losses.append(train_loss/len(train_dataloader))\n",
        "        test_losses.append(test_loss/len(test_dataloader))\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}')\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    # model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for surnames, labels in dataloader:\n",
        "            x = surnames#.to(device)\n",
        "            y = labels#.to(device)\n",
        "\n",
        "            logits = model(x)\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            correct += (predicted == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.5f}')\n",
        "\n",
        "def predict(model, dataset, surname):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        vectorized = dataset.vectorize(surname)\n",
        "        tensor = vectorized.unsqueeze(0).to(device)\n",
        "\n",
        "        logits = model(tensor)\n",
        "        \n",
        "        probs = torch.softmax(logits, dim=1).squeeze()\n",
        "        # print(f\"probs: {probs} size: {probs.shape}\")\n",
        "        top3_probs, top3_indices = torch.topk(probs, k=3)\n",
        "        print(top3_probs, top3_indices)\n",
        "\n",
        "        top3_nationalities = _[top3_indices.detach().cpu().numpy()]\n",
        "        print(f'{surname}: {top3_nationalities[0]} ({top3_probs[0]:.4f}), {top3_nationalities[1]} ({top3_probs[1]:.4f}), {top3_nationalities[2]} ({top3_probs[2]:.4f})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.4934, -0.6052, -0.1468,  0.5293, -0.6073, -0.1523,  0.5068,\n",
            "           0.8247, -0.9923,  0.9903],\n",
            "         [-0.5761, -0.9204,  0.6897, -0.1006,  0.8048,  0.7096,  0.0751,\n",
            "          -1.5318,  2.4260,  0.4106],\n",
            "         [ 2.3055,  2.0440,  0.7741,  0.0557, -1.0849, -1.0047,  0.6631,\n",
            "           0.0242, -0.8623,  0.7762]],\n",
            "\n",
            "        [[-0.2582,  0.7932,  1.8183,  0.4992,  1.0997,  0.2566, -1.2865,\n",
            "           1.2173,  0.5199, -1.9446],\n",
            "         [-0.9500,  2.8146, -0.9235, -1.3016, -0.0598,  1.5100, -0.0168,\n",
            "          -0.7834, -0.5279, -2.1207],\n",
            "         [ 0.8666,  0.8730, -0.3235, -1.5643,  0.0539, -0.3354, -1.7087,\n",
            "           0.7488, -0.8043,  0.7545]],\n",
            "\n",
            "        [[-1.1502,  0.9670,  0.3426,  0.9749, -0.7836, -1.0436, -0.8756,\n",
            "           0.3985,  2.1161, -0.0698],\n",
            "         [-0.7284, -0.7192,  0.8987,  0.1179,  0.5274, -0.0929, -0.0300,\n",
            "          -0.3365, -0.9884, -1.1052],\n",
            "         [-0.7143,  1.2507,  0.2919, -0.6275, -1.9242,  0.5636,  0.8448,\n",
            "           0.5390, -0.8321,  0.4529]],\n",
            "\n",
            "        [[ 0.5162,  0.0874,  3.3778,  1.5947,  0.3072, -0.1082, -1.1431,\n",
            "          -1.4432, -1.4766,  0.3807],\n",
            "         [ 1.2074, -0.2406,  0.6612, -0.3421, -0.1212, -1.1920, -0.3392,\n",
            "           0.8989,  1.5035, -0.8544],\n",
            "         [ 0.9886, -0.5482, -0.9038,  0.8545, -1.0198, -1.0989, -0.6217,\n",
            "           0.0421,  0.6418,  2.3439]],\n",
            "\n",
            "        [[-0.5209,  0.4250,  0.8239, -0.5065,  1.6206, -0.2704, -0.1225,\n",
            "          -0.7982, -1.0366, -1.1794],\n",
            "         [-1.5619,  1.2588, -1.7544, -0.4207,  0.1353, -0.3522,  0.6900,\n",
            "           1.0617,  0.1484, -0.2212],\n",
            "         [-0.7479, -0.8283,  1.1495,  1.4570,  1.7319, -0.7103, -0.7574,\n",
            "          -0.4843,  1.2314,  0.4053]],\n",
            "\n",
            "        [[ 0.4655, -0.4630,  1.1460, -0.3166,  0.7571,  0.5088, -0.8957,\n",
            "           0.1450,  0.8129, -1.2315],\n",
            "         [-0.9785, -0.1937, -1.8649,  0.3964, -0.2676,  1.2266,  0.1185,\n",
            "          -0.4889, -0.7017,  0.2526],\n",
            "         [ 0.6416, -0.4495, -0.4367, -0.2484,  0.9619,  0.0479, -0.2579,\n",
            "           1.1012, -0.3412,  0.5599]]])\n",
            "torch.Size([6, 3, 10])\n",
            "tensor([[-0.1615,  1.5840,  0.6504, -0.1002,  0.4522,  0.4178, -0.5362,  1.2507,\n",
            "          0.7091, -0.9275, -0.5230, -0.3927,  0.4562,  1.0330,  0.1307, -0.9509,\n",
            "          0.0331,  0.9228,  1.7058,  0.8707],\n",
            "        [-1.1826,  1.7535, -1.0370, -0.3985, -1.8394,  0.1712,  0.8437,  0.5770,\n",
            "         -0.7150, -1.7548,  0.3958, -0.1025, -0.7035, -1.4084, -2.7837, -0.9372,\n",
            "          0.6244,  1.2197,  0.6850,  0.2850],\n",
            "        [ 0.7192, -0.2369,  0.1684, -1.0663,  1.5780,  0.0290,  0.6859, -0.8032,\n",
            "         -0.0457, -0.9630, -1.4801, -0.0130,  0.1743,  0.9781,  0.3845, -0.9645,\n",
            "         -1.3870,  1.3774, -0.0898,  1.9566]]) torch.Size([3, 20])\n",
            "tensor([[ 0.2371,  0.5623, -0.7727, -0.6724,  0.0340, -0.4343, -0.2573,  0.8637,\n",
            "          0.1100, -0.3870, -0.2946,  0.7167,  0.2330,  0.4892,  0.0975,  0.0686,\n",
            "         -0.7499,  0.7414, -0.2748, -0.3068],\n",
            "        [-0.9092, -0.7232,  0.1558,  0.9711,  0.3079, -0.5063,  0.8761, -0.9168,\n",
            "          0.2402, -0.9839,  0.8525,  0.5319,  0.2488, -0.1835, -0.4983, -0.5587,\n",
            "          0.4019, -0.2842, -0.8471, -0.7349],\n",
            "        [ 0.6643,  0.3582,  0.3260,  0.4940,  0.6982, -0.9166, -0.4587,  0.9510,\n",
            "         -0.7093,  0.7846,  0.8211,  0.4838, -0.6493, -0.0236,  0.1272, -0.4027,\n",
            "          0.3881,  0.5385, -0.2859, -0.5374]], grad_fn=<TanhBackward0>) torch.Size([3, 20])\n"
          ]
        }
      ],
      "source": [
        "rnn = nn.RNNCell(10, 20)\n",
        "input = torch.randn(6, 3, 10)\n",
        "print(input)\n",
        "print(input.shape)\n",
        "hx = torch.randn(3, 20)\n",
        "print(hx, hx.shape)\n",
        "output = []\n",
        "for i in range(1):\n",
        "    hx = rnn(input[i], hx)\n",
        "    print(hx, hx.shape)\n",
        "    output.append(hx)\n",
        "    \n",
        "# output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, embedding_dim, hidden_size, output_size):\n",
        "    super(RNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding = nn.Embedding(input_size, embedding_dim)\n",
        "    self.rnn_cell = nn.RNNCell(embedding_dim, hidden_size)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    x.shape = (batch_size, seq_len) - тензор входных данных\n",
        "    h.shape = (batch_size, hidden_size) - тензор со скрытым состоянием RNN\n",
        "    '''\n",
        "    batch_size, seq_len = x.shape\n",
        "\n",
        "    h = torch.zeros(batch_size, self.hidden_size)\n",
        "\n",
        "    hidden_states = []\n",
        "    for t in range(seq_len):\n",
        "      # получаем эмбеддинг текущего символа\n",
        "      x_t = self.embedding(x[:, t])\n",
        "      # обновляем скрытое состояние\n",
        "      h = self.rnn_cell(x_t, h)\n",
        "      hidden_states.append(h)\n",
        "\n",
        "    # конкатенируем скрытые состояния и применяем полносвязный слой\n",
        "    hidden_states = torch.stack(hidden_states, dim=1)\n",
        "    output = self.fc(hidden_states[:, -1, :])\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([36, 15, 47, 29, 42, 15, 40, 29, 46,  0,  0,  0,  0,  0,  0,  0,  0])"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset.vectorize(\"tugolukov\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = RNN(\n",
        "            input_size=vocab.vocab_len,\n",
        "            embedding_dim=256,\n",
        "            hidden_size=1024,\n",
        "            output_size=len(set(y_train)))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество обучаемых параметров: 1345554\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Количество обучаемых параметров: {total_params}\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.1120,  0.3691,  0.0479, -0.1511,  0.0336,  0.2284,  0.2708, -0.2426,\n",
              "         -0.1235, -0.4661,  0.1584, -0.0738,  0.2521, -0.1785, -0.0767,  0.1084,\n",
              "         -0.1571,  0.0140]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.forward(train_dataset.vectorize(\"tugolukov\").unsqueeze(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 1.3938, Test Loss: 1.5178\n",
            "Epoch 2, Train Loss: 1.3587, Test Loss: 1.4363\n",
            "Epoch 3, Train Loss: 1.3911, Test Loss: 1.4044\n"
          ]
        }
      ],
      "source": [
        "train_model(model, train_dataloader, test_dataloader, criterion, optimizer, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.61111\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(model, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def train_model(model, train_dataloader, test_dataloader, criterion, optimizer, num_epochs):\n",
        "    model.to(device)\n",
        "    train_losses, test_losses = [], []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss, test_loss = 0, 0\n",
        "        for inputs, labels in train_dataloader:\n",
        "            x = inputs.to(device)\n",
        "            y = labels.to(device)\n",
        "            \n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "    \n",
        "        # Валидация на val_loader\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "        \n",
        "        train_losses.append(train_loss/len(train_dataloader))\n",
        "        test_losses.append(test_loss/len(test_dataloader))\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}')\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for surnames, labels in dataloader:\n",
        "            x = surnames.to(device)\n",
        "            y = labels.to(device)\n",
        "\n",
        "            logits = model(x)\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            correct += (predicted == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.5f}')\n",
        "\n",
        "def predict(model, dataset, surname):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        vectorized = dataset.vectorize(surname)\n",
        "        tensor = vectorized.unsqueeze(0).to(device)\n",
        "\n",
        "        logits = model(tensor)\n",
        "        \n",
        "        probs = torch.softmax(logits, dim=1).squeeze()\n",
        "        # print(f\"probs: {probs} size: {probs.shape}\")\n",
        "        top3_probs, top3_indices = torch.topk(probs, k=3)\n",
        "        print(top3_probs, top3_indices)\n",
        "\n",
        "        top3_nationalities = _[top3_indices.detach().cpu().numpy()]\n",
        "        print(f'{surname}: {top3_nationalities[0]} ({top3_probs[0]:.4f}), {top3_nationalities[1]} ({top3_probs[1]:.4f}), {top3_nationalities[2]} ({top3_probs[2]:.4f})')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a2MIErKTo9aO"
      },
      "source": [
        "1.2 Замените модуль `RNN` из 1.1 на модули `nn.RNN`, `nn.LSTM` и `nn.GRU` (не забудьте указать аргумент `batch_first=True`). Сравните результаты работы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, embedding_dim, hidden_size, output_size):\n",
        "    super(RNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding = nn.Embedding(input_size, embedding_dim)\n",
        "    self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "    self.gru = nn.GRU(embedding_dim, hidden_size, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    x.shape = (batch_size, seq_len) - тензор входных данных\n",
        "    h.shape = (batch_size, hidden_size) - тензор со скрытым состоянием RNN\n",
        "    '''\n",
        "    batch_size, seq_len = x.shape\n",
        "\n",
        "    # h = torch.zeros(1, batch_size, self.hidden_size)  # RNN, GRU\n",
        "    h = (torch.zeros(1, batch_size, self.hidden_size).cuda(), torch.zeros(1, batch_size, self.hidden_size).cuda())  # LSTM\n",
        "\n",
        "    # получаем эмбеддинг всех символов\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # обновляем скрытое состояние\n",
        "    # output, h = self.rnn(x, h)\n",
        "    output, (h, c) = self.lstm(x, h)\n",
        "    # output, h = self.gru(x, h)\n",
        "\n",
        "    # применяем полносвязный слой\n",
        "    output = self.fc(output[:, -1, :])\n",
        "    # return output, h[-1, :, :]\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = RNN(\n",
        "            input_size=vocab.vocab_len,\n",
        "            embedding_dim=64,\n",
        "            hidden_size=256,\n",
        "            output_size=len(set(y_train)))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество обучаемых параметров: 667666\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Количество обучаемых параметров: {total_params}\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 1.0889, Test Loss: 1.0190\n",
            "Epoch 2, Train Loss: 0.9281, Test Loss: 0.9280\n",
            "Epoch 3, Train Loss: 0.8188, Test Loss: 0.8794\n",
            "Epoch 4, Train Loss: 0.7351, Test Loss: 0.8299\n",
            "Epoch 5, Train Loss: 0.6669, Test Loss: 0.8034\n",
            "Epoch 6, Train Loss: 0.5938, Test Loss: 0.8089\n",
            "Epoch 7, Train Loss: 0.5361, Test Loss: 0.8006\n",
            "Epoch 8, Train Loss: 0.4764, Test Loss: 0.7958\n",
            "Epoch 9, Train Loss: 0.4211, Test Loss: 0.8045\n",
            "Epoch 10, Train Loss: 0.3900, Test Loss: 0.8503\n"
          ]
        }
      ],
      "source": [
        "train_model(model, train_dataloader, test_dataloader, criterion, optimizer, 10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Результаты для nn.RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.69444\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(model, test_dataloader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Результаты для nn.LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.77550\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(model, test_dataloader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Результаты для nn.GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.77004\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(model, test_dataloader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_6YBam_3t-fO"
      },
      "source": [
        "1.3 Загрузите предобученные эмбеддинги (https://disk.yandex.ru/d/BHuT2tEXr_yBOQ?w=1) в модуль `nn.Embedding` и обучите модели из 1.2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.4180,  0.2497, -0.4124,  ..., -0.1841, -0.1151, -0.7858],\n",
              "        [ 0.0134,  0.2368, -0.1690,  ..., -0.5666,  0.0447,  0.3039],\n",
              "        [ 0.1516,  0.3018, -0.1676,  ..., -0.3565,  0.0164,  0.1022],\n",
              "        ...,\n",
              "        [-0.5118,  0.0587,  1.0913,  ..., -0.2500, -1.1250,  1.5863],\n",
              "        [-0.7590, -0.4743,  0.4737,  ...,  0.7895, -0.0141,  0.6448],\n",
              "        [ 0.0726, -0.5139,  0.4728,  ..., -0.1891, -0.5902,  0.5556]])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_path = \"data/embeddings/glove.6B.50d.txt\"\n",
        "\n",
        "embeddings = {}\n",
        "with open(embedding_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = torch.tensor([float(val) for val in values[1:]])\n",
        "        embeddings[word] = vector\n",
        "\n",
        "input_size = len(embeddings)\n",
        "embedding_dim = len(next(iter(embeddings.values())))\n",
        "embedding_matrix = torch.zeros(input_size, embedding_dim)\n",
        "for i, word in enumerate(embeddings):\n",
        "    embedding_matrix[i] = embeddings[word]\n",
        "embedding_layer = nn.Embedding(input_size, embedding_dim)\n",
        "embedding_layer.weight.data.copy_(embedding_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'the'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(embeddings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_dim, hidden_size, output_size, pretrained_embeddings):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, h=None):\n",
        "        '''\n",
        "        x.shape = (batch_size, seq_len) - тензор входных данных\n",
        "        h.shape = (batch_size, hidden_size) - тензор со скрытым состоянием RNN\n",
        "        '''\n",
        "        batch_size, seq_len = x.shape\n",
        "        \n",
        "        h = torch.zeros(1, batch_size, self.hidden_size, device=device)  # RNN, GRU\n",
        "        # h = (torch.zeros(1, batch_size, self.hidden_size, device=device), torch.zeros(1, batch_size, self.hidden_size, device=device))  # LSTM\n",
        "\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # обновляем скрытое состояние\n",
        "        # output, h = self.rnn(x, h)\n",
        "        # output, (h, c) = self.lstm(x, h)\n",
        "        output, h = self.gru(x, h)\n",
        "\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        # return output, h[-1, :, :]\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_size = vocab.vocab_len\n",
        "embedding_dim = len(next(iter(embeddings.values())))\n",
        "hidden_size = 128\n",
        "output_size = len(set(y_train))\n",
        "model = RNN(input_size, embedding_dim, hidden_size, output_size, pretrained_embeddings=embedding_matrix)\n",
        "# model = RNN(\n",
        "#             input_size=vocab.vocab_len,\n",
        "#             embedding_dim=64,\n",
        "#             hidden_size=256,\n",
        "#             pretrained_embeddings=embeddings,\n",
        "#             output_size=len(set(y_train)))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество обучаемых параметров: 186642\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Количество обучаемых параметров: {total_params}\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 1.9063, Test Loss: 1.5279\n",
            "Epoch 2, Train Loss: 1.3118, Test Loss: 1.1732\n",
            "Epoch 3, Train Loss: 1.0341, Test Loss: 0.9784\n",
            "Epoch 4, Train Loss: 0.8786, Test Loss: 0.8971\n",
            "Epoch 5, Train Loss: 0.7800, Test Loss: 0.8477\n",
            "Epoch 6, Train Loss: 0.6878, Test Loss: 0.8331\n",
            "Epoch 7, Train Loss: 0.6284, Test Loss: 0.7871\n",
            "Epoch 8, Train Loss: 0.5603, Test Loss: 0.7996\n",
            "Epoch 9, Train Loss: 0.5104, Test Loss: 0.8299\n",
            "Epoch 10, Train Loss: 0.4557, Test Loss: 0.8565\n"
          ]
        }
      ],
      "source": [
        "train_model(model, train_dataloader, test_dataloader, criterion, optimizer, 10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Результаты для nn.RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.69672\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(model, test_dataloader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Результаты для nn.LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.76138\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(model, test_dataloader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Результаты для nn.GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.77641\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(model, test_dataloader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "f7kf990U9Do-"
      },
      "source": [
        "## 2. Классификация обзоров на фильмы (RNN)\n",
        "\n",
        "Датасет: https://disk.yandex.ru/d/tdinpb0nN_Dsrg\n",
        "\n",
        "2.1 Создайте набор данных на основе файлов polarity/positive_reviews.csv (положительные отзывы) и polarity/negative_reviews.csv (отрицательные отзывы). Разбейте на обучающую и тестовую выборку.\n",
        "  * токен = __слово__\n",
        "  * данные для обучения в датасете представляются в виде последовательности индексов токенов\n",
        "  * словарь создается на основе _только_ обучающей выборки. Для корректной обработки ситуаций, когда в тестовой выборке встретится токен, который не хранится в словаре, добавьте в словарь специальный токен `<UNK>`\n",
        "  * добавьте предобработку текста\n",
        "\n",
        "2.2. Обучите классификатор.\n",
        "  \n",
        "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding` \n",
        "    - подберите адекватную размерность вектора эмбеддинга: \n",
        "    - модуль `nn.Embedding` обучается\n",
        "\n",
        "  * Используйте рекуррентные слои (`nn.RNN`, `nn.LSTM`, `nn.GRU`)\n",
        "\n",
        "\n",
        "2.3 Измерить точность на тестовой выборке. Проверить работоспособность модели: придумать небольшой отзыв, прогнать его через модель и вывести номер предсказанного класса (сделать это для явно позитивного и явно негативного отзыва)\n",
        "* Целевое значение accuracy на валидации - 70+%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"data/polarity/positive_reviews.txt\") as f:\n",
        "    positive_reviews = sent_tokenize(f.read())\n",
        "    \n",
        "with open(\"data/polarity/negative_reviews.txt\") as f:\n",
        "    negative_reviews = sent_tokenize(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6042, 5835)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(positive_reviews), len(negative_reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>simplistic , silly and tedious .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>it's so laddish and juvenile , only teenage bo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>exploitative and largely devoid of the depth o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[garbus] discards the potential for pathologic...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a visually flashy but narratively opaque and e...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11872</th>\n",
              "      <td>may prove to be [tsai's] masterpiece .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11873</th>\n",
              "      <td>mazel tov to a film about a family's joyous li...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11874</th>\n",
              "      <td>standing in the shadows of motown is the best ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11875</th>\n",
              "      <td>it's nice to see piscopo again after all these...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11876</th>\n",
              "      <td>provides a porthole into that noble , tremblin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11877 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  category\n",
              "0                       simplistic , silly and tedious .         1\n",
              "1      it's so laddish and juvenile , only teenage bo...         1\n",
              "2      exploitative and largely devoid of the depth o...         1\n",
              "3      [garbus] discards the potential for pathologic...         1\n",
              "4      a visually flashy but narratively opaque and e...         1\n",
              "...                                                  ...       ...\n",
              "11872             may prove to be [tsai's] masterpiece .         0\n",
              "11873  mazel tov to a film about a family's joyous li...         0\n",
              "11874  standing in the shadows of motown is the best ...         0\n",
              "11875  it's nice to see piscopo again after all these...         0\n",
              "11876  provides a porthole into that noble , tremblin...         0\n",
              "\n",
              "[11877 rows x 2 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reviews_df = pd.DataFrame()\n",
        "\n",
        "reviews_df[\"text\"] = positive_reviews + negative_reviews\n",
        "reviews_df[\"category\"] = [1 for i in range(len(positive_reviews))] + [0 for i in range(len(negative_reviews))]\n",
        "\n",
        "reviews_df = reviews_df\n",
        "reviews_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>simplistic , silly and tedious .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>it s so laddish and juvenile , only teenage bo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>exploitative and largely devoid of the depth o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>garbus discard the potential for pathological ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a visually flashy but narratively opaque and e...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11872</th>\n",
              "      <td>may prove to be tsai s masterpiece .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11873</th>\n",
              "      <td>mazel tov to a film about a family s joyous li...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11874</th>\n",
              "      <td>standing in the shadow of motown is the best k...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11875</th>\n",
              "      <td>it s nice to see piscopo again after all these...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11876</th>\n",
              "      <td>provides a porthole into that noble , tremblin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11877 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  category\n",
              "0                       simplistic , silly and tedious .         1\n",
              "1      it s so laddish and juvenile , only teenage bo...         1\n",
              "2      exploitative and largely devoid of the depth o...         1\n",
              "3      garbus discard the potential for pathological ...         1\n",
              "4      a visually flashy but narratively opaque and e...         1\n",
              "...                                                  ...       ...\n",
              "11872               may prove to be tsai s masterpiece .         0\n",
              "11873  mazel tov to a film about a family s joyous li...         0\n",
              "11874  standing in the shadow of motown is the best k...         0\n",
              "11875  it s nice to see piscopo again after all these...         0\n",
              "11876  provides a porthole into that noble , tremblin...         0\n",
              "\n",
              "[11877 rows x 2 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([' ' if not char.isalpha() and char not in ['.', ',', '!', '?'] else char for char in text])\n",
        "    \n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    \n",
        "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
        "    \n",
        "    return preprocessed_text\n",
        "\n",
        "reviews_df[\"text\"] = reviews_df[\"text\"].apply(lambda x: preprocess_text(x))\n",
        "reviews_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = reviews_df['text'].str.lower()\n",
        "y = reviews_df['category']\n",
        "n_classes = y.nunique()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "y_train = y_train.reset_index(drop=True)\n",
        "y_test = y_test.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(16454, 504)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class Vocab:\n",
        "  def __init__(self, data):\n",
        "    self.idx_to_token = {}\n",
        "    self.token_to_idx = {}\n",
        "    self.vocab_len = 0\n",
        "    self.max_seq_len = 0\n",
        "    \n",
        "    for item in data:\n",
        "      self.max_seq_len = max(self.max_seq_len, len(item))\n",
        "    \n",
        "    # Добавляем токен для неизвестных слов\n",
        "    self.idx_to_token = {0: '<UNK>'}\n",
        "    self.token_to_idx = {'<UNK>': 0}\n",
        "    self.vocab_len += 1\n",
        "\n",
        "    all_words = [word for sentence in data for word in word_tokenize(sentence)]\n",
        "\n",
        "    for word in all_words:\n",
        "        if word not in self.token_to_idx:\n",
        "            self.idx_to_token[self.vocab_len] = word\n",
        "            self.token_to_idx[word] = self.vocab_len\n",
        "            self.vocab_len += 1\n",
        "\n",
        "    # self.data = []\n",
        "    # for sentence in data:\n",
        "    #     tokens = [self.token_to_idx.get(word, self.token_to_idx['<UNK>']) for word in word_tokenize(sentence)]\n",
        "    #     self.data.append(tokens)\n",
        "        \n",
        "vocab = Vocab(X)\n",
        "vocab.vocab_len, vocab.max_seq_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReviewDataset(Dataset):\n",
        "  def __init__(self, X, y, vocab: Vocab):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.vocab = vocab\n",
        "\n",
        "  def vectorize(self, review):\n",
        "    '''Генерирует представление отзыва review при помощи бинарного кодирования (см. 1.2)'''\n",
        "    vec = torch.zeros(self.vocab.max_seq_len, dtype=torch.int64)\n",
        "    \n",
        "\n",
        "    for i, word in enumerate(word_tokenize(review)):\n",
        "\n",
        "      if i >= self.vocab.max_seq_len:\n",
        "        break\n",
        "\n",
        "      vec[i] = self.vocab.token_to_idx.get(word, 0)\n",
        "  \n",
        "    return vec\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    vec = self.vectorize(self.X[idx])\n",
        "    label = self.y[idx]\n",
        "    return vec, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = ReviewDataset(X_train, y_train, vocab)\n",
        "test_dataset = ReviewDataset(X_test, y_test, vocab)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([[   32, 15922,  2396,  ...,     0,     0,     0],\n",
              "         [   23,   224,   445,  ...,     0,     0,     0],\n",
              "         [   23,   125,   101,  ...,     0,     0,     0],\n",
              "         ...,\n",
              "         [  568,    61,    23,  ...,     0,     0,     0],\n",
              "         [  492,   230,  3648,  ...,     0,     0,     0],\n",
              "         [   74, 13037,   191,  ...,     0,     0,     0]]),\n",
              " tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "         0, 0, 1, 1, 1, 1, 1, 0])]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(test_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_dim, hidden_size, output_size, num_layers=1, dropout=0.2):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x.shape = (batch_size, seq_len) - тензор входных данных\n",
        "        h.shape = (batch_size, hidden_size) - тензор со скрытым состоянием RNN\n",
        "        '''\n",
        "        batch_size, seq_len = x.shape\n",
        "\n",
        "        h = torch.zeros(2*self.num_layers, batch_size, self.hidden_size).cuda()  # RNN, GRU\n",
        "        # h = (torch.zeros(2*num_layers, batch_size, self.hidden_size, device=device), torch.zeros(2*num_layers, batch_size, self.hidden_size, device=device))  # LSTM\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # output, h = self.rnn(x, h)\n",
        "        # output, (h, c) = self.lstm(x, h)\n",
        "        output, h = self.gru(x, h)\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        output = self.softmax(output)\n",
        "        # return output, h[-1, :, :]\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(504, 16454)"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab.max_seq_len, vocab.vocab_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = RNN(\n",
        "            input_size=vocab.vocab_len,\n",
        "            embedding_dim=128,\n",
        "            hidden_size=128,\n",
        "            output_size=2)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество обучаемых параметров: 2635010\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Количество обучаемых параметров: {total_params}\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 0.5778, Test Loss: 0.5996\n",
            "Epoch 2, Train Loss: 0.4761, Test Loss: 0.5934\n",
            "Epoch 3, Train Loss: 0.3757, Test Loss: 0.6360\n",
            "Epoch 4, Train Loss: 0.2995, Test Loss: 0.6612\n",
            "Epoch 5, Train Loss: 0.2405, Test Loss: 0.7440\n"
          ]
        }
      ],
      "source": [
        "train_model(model, train_dataloader, test_dataloader, criterion, optimizer, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.72348\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(model, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "rating_labels = [\"Positive\", \"Negative\"]\n",
        "\n",
        "def predict(model, dataset, review):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        vectorized = dataset.vectorize(review)\n",
        "        tensor = vectorized.unsqueeze(0).to(device)\n",
        "        logits = model(tensor)\n",
        "        probs = torch.softmax(logits, dim=1).squeeze()\n",
        "        print(probs)\n",
        "        print(f'{rating_labels[probs.argmax()]} ({probs.max():.4f}), {rating_labels[probs.argmin()]} ({probs.min():.4f}) \\n{review}')\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.9869, 0.0131], device='cuda:0')\n",
            "Positive (0.9869), Negative (0.0131) \n",
            "This restaurant is simply amazing! The food is delicious and the service is outstanding.\n"
          ]
        }
      ],
      "source": [
        "predict(model, train_dataset, \"This restaurant is simply amazing! The food is delicious and the service is outstanding.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.0128, 0.9872], device='cuda:0')\n",
            "Negative (0.9872), Positive (0.0128) \n",
            "The menu at this restaurant is very limited and the food is nothing special. I wouldn't go back.\n"
          ]
        }
      ],
      "source": [
        "predict(model, train_dataset, \"The menu at this restaurant is very limited and the food is nothing special. I wouldn't go back.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.9852, 0.0148], device='cuda:0')\n",
            "Positive (0.9852), Negative (0.0148) \n",
            "I had the best dining experience in this restaurant. The ambiance is perfect and the staff is very friendly.\n"
          ]
        }
      ],
      "source": [
        "predict(model, train_dataset, \"I had the best dining experience in this restaurant. The ambiance is perfect and the staff is very friendly.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

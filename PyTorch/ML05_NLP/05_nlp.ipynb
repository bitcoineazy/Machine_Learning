{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XtFQP3RNll3c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zonkz\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7026,
     "status": "ok",
     "timestamp": 1618423490594,
     "user": {
      "displayName": "Никита Блохин",
      "photoUrl": "",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "443CZCLp0_sE",
    "outputId": "ddda53b2-a593-4d13-ee48-cfed23e613b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zonkz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqDHq_AEjRZ1"
   },
   "source": [
    "## 1. Представление и предобработка текстовых данных "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaki7efDpmXo"
   },
   "source": [
    "1.1 Операции по предобработке:\n",
    "* токенизация\n",
    "* стемминг / лемматизация\n",
    "* удаление стоп-слов\n",
    "* удаление пунктуации\n",
    "* приведение к нижнему регистру\n",
    "* любые другие операции над текстом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nHRy4jpYphEr"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lMMzGhq0ikz1"
   },
   "outputs": [],
   "source": [
    "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUhfertRtXE5"
   },
   "source": [
    "Реализовать функцию `preprocess_text(text: str)`, которая:\n",
    "* приводит строку к нижнему регистру\n",
    "* заменяет все символы, кроме a-z, A-Z и знаков .,!? на пробел\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'select your preferences and run the install command. stable represents the most currently tested and supported version of pytorch. note that libtorch is only available for c  '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    # Приводим строку к нижнему регистру\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Заменяем все символы, кроме a-z, A-Z и знаков .,!? на пробел\n",
    "    text = ''.join([' ' if not char.isalpha() and char not in ['.', ',', '!', '?'] else char for char in text])\n",
    "    \n",
    "    return text\n",
    "\n",
    "text = preprocess_text(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'начинается  новое     приключение   совсем скоро   sap  '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(\"Начинается %новое % **приключение** совсем скоро &&SAP&&\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2Dt1ssIqckC"
   },
   "source": [
    "1.2 Представление текстовых данных при помощи бинарного кодирования\n",
    "\n",
    "\n",
    "Представить первое предложение из `text` в виде тензора `sentence_t`: `sentence_t[i] == 1`, если __слово__ с индексом `i` присуствует в предложении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'available': 0, 'stable': 1, 'c': 2, 'your': 3, 'pytorch.': 4, 'install': 5, 'is': 6, 'version': 7, 'most': 8, 'preferences': 9, 'run': 10, 'represents': 11, 'of': 12, 'select': 13, 'the': 14, 'tested': 15, 'for': 16, 'supported': 17, 'note': 18, 'that': 19, 'only': 20, 'command.': 21, 'and': 22, 'currently': 23, 'libtorch': 24}\n",
      "select your preferences and run the install command.\n",
      "select\n",
      "13\n",
      "your\n",
      "3\n",
      "preferences\n",
      "9\n",
      "and\n",
      "22\n",
      "run\n",
      "10\n",
      "the\n",
      "14\n",
      "install\n",
      "5\n",
      "command.\n",
      "21\n",
      "tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "first_sentence = sent_tokenize(text)[0]\n",
    "\n",
    "# Создаем словарь слов и присваиваем каждому уникальный индекс\n",
    "dictionary = {word: i for i, word in enumerate(set(text.split()))}\n",
    "print(dictionary)\n",
    "\n",
    "sentence_t = torch.zeros(len(dictionary))\n",
    "\n",
    "print(first_sentence)\n",
    "\n",
    "for word in first_sentence.split():\n",
    "    print(word)\n",
    "    if word in dictionary:\n",
    "        print(dictionary[word])\n",
    "        sentence_t[dictionary[word]] = 1\n",
    "\n",
    "print(sentence_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2Nz_zcgw3N4"
   },
   "source": [
    "## 2. Классификация фамилий по национальности\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/owHew8hzPc7X9Q?w=1\n",
    "\n",
    "2.1 Считать файл `surnames/surnames.csv`. \n",
    "\n",
    "2.2 Закодировать национальности числами, начиная с 0.\n",
    "\n",
    "2.3 Разбить датасет на обучающую и тестовую выборку\n",
    "\n",
    "2.4 Реализовать класс `Vocab` (токен = __символ__)\n",
    "\n",
    "2.5 Реализовать класс `SurnamesDataset`\n",
    "\n",
    "2.6. Обучить классификатор.\n",
    "\n",
    "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: прогнать несколько фамилий студентов группы через модели и проверить результат. Для каждой фамилии выводить 3 наиболее вероятных предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        surname  nationality\n",
      "0      Woodford            0\n",
      "1          Coté            1\n",
      "2          Kore            0\n",
      "3         Koury            2\n",
      "4        Lebzak            3\n",
      "...         ...          ...\n",
      "10975  Quraishi            2\n",
      "10976   Innalls            0\n",
      "10977      Król           12\n",
      "10978    Purvis            0\n",
      "10979  Messerli            9\n",
      "\n",
      "[10980 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "surnames = pd.read_csv(\"data/surnames/surnames.csv\")\n",
    "surnames['nationality'], _ = pd.factorize(surnames['nationality'])\n",
    "print(surnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(surnames['surname'].to_numpy(), surnames['nationality'].to_numpy(), test_size=0.2)\n",
    "X = surnames['surname'].str.lower()\n",
    "y = surnames['nationality']\n",
    "n_classes = y.nunique()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kUkSZkDqxNYS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'available': 0, 'stable': 1, 'c': 2, 'your': 3, 'pytorch.': 4, 'install': 5, 'is': 6, 'version': 7, 'most': 8, 'preferences': 9, 'run': 10, 'represents': 11, 'of': 12, 'select': 13, 'the': 14, 'tested': 15, 'for': 16, 'supported': 17, 'note': 18, 'that': 19, 'only': 20, 'command.': 21, 'and': 22, 'currently': 23, 'libtorch': 24}\n",
      "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Создаем словарь слов и присваиваем каждому уникальный индекс\n",
    "dictionary = {word: i for i, word in enumerate(set(text.split()))}\n",
    "print(dictionary)\n",
    "\n",
    "sentence_t = torch.zeros(len(dictionary))\n",
    "\n",
    "for word in first_sentence:\n",
    "    if word in dictionary:\n",
    "        sentence_t[dictionary[word]] = 1\n",
    "\n",
    "print(sentence_t)\n",
    "\n",
    "# class Vocab:\n",
    "#     def __init__(self, data):\n",
    "#         self.idx_to_token = {}\n",
    "#         self.token_to_idx = {}\n",
    "#         self.vocab_len = 0\n",
    "\n",
    "#         # Получаем список всех слов в данных\n",
    "#         all_words = [word for sentence in data[\"surname\"].str.lower() for word in word_tokenize(sentence)]\n",
    "#         # print(all_words)\n",
    "\n",
    "#         # Строим словарь\n",
    "#         for word in all_words:\n",
    "#             if word not in self.token_to_idx:\n",
    "#                 self.idx_to_token[self.vocab_len] = word\n",
    "#                 self.token_to_idx[word] = self.vocab_len\n",
    "#                 self.vocab_len += 1\n",
    "\n",
    "class Vocab:\n",
    "  def __init__(self, data):\n",
    "    tokens = set()\n",
    "    for item in data:\n",
    "      tokens.update(item)\n",
    "\n",
    "    self.idx_to_token = dict(enumerate(tokens))\n",
    "    self.token_to_idx = {token: idx for idx, token in self.idx_to_token.items()}\n",
    "    self.vocab_len = len(self.idx_to_token)\n",
    "    \n",
    "vocab = Vocab(surnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        woodford\n",
      "1            coté\n",
      "2            kore\n",
      "3           koury\n",
      "4          lebzak\n",
      "           ...   \n",
      "10975    quraishi\n",
      "10976     innalls\n",
      "10977        król\n",
      "10978      purvis\n",
      "10979    messerli\n",
      "Name: surname, Length: 10980, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(surnames[\"surname\"].str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "10980\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(surnames[\"surname\"].str.lower())\n",
    "\n",
    "print(vocab.vocab_len)\n",
    "print(len(surnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "executionInfo": {
     "elapsed": 1303,
     "status": "error",
     "timestamp": 1619117849212,
     "user": {
      "displayName": "Никита Блохин",
      "photoUrl": "",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "WCaRK1QHxe0A",
    "outputId": "5d1243af-d0dd-4922-9468-9618f5df4605"
   },
   "outputs": [],
   "source": [
    "class SurnamesDataset(Dataset):\n",
    "  def __init__(self, X, y, vocab: Vocab):\n",
    "    self.X = X\n",
    "    # print(len(self.X))\n",
    "    self.y = y\n",
    "    self.vocab = vocab\n",
    "    # print(self.vocab.vocab_len)\n",
    "\n",
    "  def vectorize(self, surname):\n",
    "    '''Генерирует представление фамилии surname в при помощи бинарного кодирования (см. 1.2)'''\n",
    "    # Создаем вектор длины словаря, заполненный нулями\n",
    "    \n",
    "    vec = torch.zeros(self.vocab.vocab_len)\n",
    "    \n",
    "    # Проходим по каждому слову в фамилии\n",
    "    # print(f\"tokenized surname: {word_tokenize(surname)}\")\n",
    "    for word in word_tokenize(surname):\n",
    "      # print(word)\n",
    "      # Если слово есть в словаре, устанавливаем соответствующий бит в векторе\n",
    "      if word in self.vocab.token_to_idx:\n",
    "          vec[self.vocab.token_to_idx[word]] = 1\n",
    "  \n",
    "    return vec\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # Получаем векторизованное представление фамилии\n",
    "    vec = self.vectorize(self.X[idx])\n",
    "\n",
    "    # Получаем метку класса для фамилии\n",
    "    label = self.y[idx]\n",
    "    # print(f\"vec, label: {vec, label}\")\n",
    "\n",
    "    return vec, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnamesDataset(Dataset):\n",
    "  def __init__(self, X, y, vocab: Vocab):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.vocab = vocab\n",
    "\n",
    "  def vectorize(self, surname):\n",
    "    '''Генерирует представление фамилии surname в при помощи бинарного кодирования (см. 1.2)'''\n",
    "    surname_t = torch.zeros(self.vocab.vocab_len)\n",
    "    for token in surname:\n",
    "      surname_t[self.vocab.token_to_idx[token]] = 1\n",
    "    return surname_t\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.vectorize(self.X.iloc[idx]), self.y.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(surnames[\"surname\"].str.lower())\n",
    "\n",
    "train_dataset = SurnamesDataset(X_train, y_train, vocab)\n",
    "test_dataset = SurnamesDataset(X_test, y_test, vocab)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SurnameClassifier(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim, output_dim):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         # self.fc = nn.Sequential(\n",
    "#         #     nn.Linear(embedding_dim * vocab_size, 64),\n",
    "#         #     nn.BatchNorm1d(64),\n",
    "#         #     nn.ReLU(),\n",
    "#         #     nn.Linear(64, output_dim)\n",
    "#         # )\n",
    "#         self.fc = nn.Linear(embedding_dim * vocab_size, output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         embedded = self.embedding(x)\n",
    "#         # print(embedded, embedded.shape)\n",
    "#         flattened = self.flatten(embedded)\n",
    "#         # print(flattened, flattened.shape)\n",
    "#         logits = self.fc(flattened)\n",
    "#         return logits\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_model(model, train_dataloader, test_dataloader, criterion, optimizer, num_epochs):\n",
    "    model.to(device)\n",
    "    train_losses, test_losses = [], []\n",
    "    \n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        # running_loss = 0.0\n",
    "        train_loss, test_loss = 0, 0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            x = inputs.to(device)\n",
    "            y = labels.to(device)\n",
    "            # print(x.dtype)\n",
    "\n",
    "            \n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "        # print(f'Epoch {epoch+1}, Loss: {epoch_loss:.5f}')\n",
    "    \n",
    "        # Валидация на val_loader\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "        \n",
    "        train_losses.append(train_loss/len(train_dataloader))\n",
    "        test_losses.append(test_loss/len(test_dataloader))\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}')\n",
    "    # epoch_loss = 0\n",
    "    # for X_batch, y_batch in train_dataloader:\n",
    "    #     predictions = model(X_batch.cuda())\n",
    "    #     loss = criterion(predictions, y_batch.cuda())\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "    #     optimizer.zero_grad()\n",
    "    #     epoch_loss += loss.item()\n",
    "    \n",
    "    # with torch.no_grad():\n",
    "    #     val_loss, val_acc = 0, 0\n",
    "    #     for X_batch, y_batch in test_dataloader:\n",
    "    #         predictions = model(X_batch.cuda())\n",
    "    #         loss = criterion(predictions, y_batch.cuda()).item()\n",
    "    #         acc = accuracy_score(y_batch, predictions.argmax(dim=1).cpu().detach()).item()\n",
    "    #         val_loss += loss\n",
    "    #         val_acc += acc\n",
    "    #     if num_epochs % 5 == 0:\n",
    "    #         print(f'#{num_epochs} Training loss: {epoch_loss / len(train_dataloader)} val_loss: {val_loss / len(test_dataloader)}')\n",
    "        \n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for surnames, labels in dataloader:\n",
    "            # print(batch[\"surname\"])\n",
    "            x = surnames.to(device)\n",
    "            # print(x)\n",
    "            y = labels.to(device)\n",
    "            # print(x, y)\n",
    "            \n",
    "            # x = torch.LongTensor(x)\n",
    "\n",
    "            logits = model(x)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "def predict(model, dataset, surname):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # vectorized, y = next(iter(dataloader))\n",
    "        vectorized = dataset.vectorize(surname)\n",
    "        print(vectorized)\n",
    "        tensor = vectorized.unsqueeze(0).to(device)\n",
    "        # vectorized = torch.LongTensor(vectorized)\n",
    "        # print(tensor) \n",
    "        logits = model(tensor)\n",
    "        \n",
    "        probs = torch.softmax(logits, dim=1).squeeze()\n",
    "        # print(f\"probs: {probs} size: {probs.shape}\")\n",
    "        top3_probs, top3_indices = torch.topk(probs, k=3)\n",
    "        print(top3_probs, top3_indices)\n",
    "        # print(f\"top 3 probs: {top3_probs}\")\n",
    "        # print(top3_indices)\n",
    "        # print(top3_indices.detach().cpu().numpy())\n",
    "        # top3_nationalities = [vocab.idx_to_token[idx.item()] for idx in top3_indices]\n",
    "        top3_nationalities = _[top3_indices.detach().cpu().numpy()]\n",
    "        print(f'{surname}: {top3_nationalities[0]} ({top3_probs[0]:.4f}), {top3_nationalities[1]} ({top3_probs[1]:.4f}), {top3_nationalities[2]} ({top3_probs[2]:.4f})')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['English', 'French', 'Arabic', 'Russian', 'Japanese', 'Chinese',\n",
       "       'Italian', 'Czech', 'Irish', 'German', 'Greek', 'Spanish', 'Polish',\n",
       "       'Dutch', 'Vietnamese', 'Korean', 'Portuguese', 'Scottish'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "print(vocab.vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.03051\n"
     ]
    }
   ],
   "source": [
    "# model = SurnameClassifier(vocab_size=vocab.vocab_len,\n",
    "#                            embedding_dim=32,\n",
    "#                            output_dim=len(set(y_train)))\n",
    "# model = SurnameClassifier(vocab_size=vocab.vocab_len,\n",
    "#                            output_dim=len(set(y_train)))\n",
    "model = nn.Sequential(nn.Linear(vocab.vocab_len, 300),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(300, len(set(y_train))))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "evaluate_model(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество обучаемых параметров: 22218\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Количество обучаемых параметров: {total_params}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.2121, Test Loss: 1.2772\n",
      "Epoch 2, Train Loss: 1.1959, Test Loss: 1.2701\n",
      "Epoch 3, Train Loss: 1.1833, Test Loss: 1.2440\n",
      "Epoch 4, Train Loss: 1.1775, Test Loss: 1.2553\n",
      "Epoch 5, Train Loss: 1.1638, Test Loss: 1.2567\n",
      "Epoch 6, Train Loss: 1.1531, Test Loss: 1.2438\n",
      "Epoch 7, Train Loss: 1.1424, Test Loss: 1.2323\n",
      "Epoch 8, Train Loss: 1.1342, Test Loss: 1.2283\n",
      "Epoch 9, Train Loss: 1.1247, Test Loss: 1.2146\n",
      "Epoch 10, Train Loss: 1.1169, Test Loss: 1.2114\n",
      "Epoch 11, Train Loss: 1.1093, Test Loss: 1.2377\n",
      "Epoch 12, Train Loss: 1.0973, Test Loss: 1.1956\n",
      "Epoch 13, Train Loss: 1.0910, Test Loss: 1.1952\n",
      "Epoch 14, Train Loss: 1.0847, Test Loss: 1.1963\n",
      "Epoch 15, Train Loss: 1.0758, Test Loss: 1.2046\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_dataloader, test_dataloader, criterion, optimizer, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.63889\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.64       567\n",
      "           1       0.25      0.03      0.05        36\n",
      "           2       0.78      1.00      0.88       346\n",
      "           3       0.74      0.78      0.76       482\n",
      "           4       0.63      0.62      0.62       161\n",
      "           5       0.45      0.61      0.52        36\n",
      "           6       0.42      0.39      0.40       108\n",
      "           7       0.40      0.07      0.12        81\n",
      "           8       0.64      0.17      0.27        41\n",
      "           9       0.45      0.32      0.38       118\n",
      "          10       0.67      0.31      0.43        32\n",
      "          11       0.58      0.26      0.36        57\n",
      "          12       0.58      0.44      0.50        25\n",
      "          13       0.45      0.10      0.17        49\n",
      "          14       0.00      0.00      0.00        15\n",
      "          15       0.33      0.20      0.25        15\n",
      "          16       0.00      0.00      0.00        14\n",
      "          17       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.64      2196\n",
      "   macro avg       0.44      0.34      0.35      2196\n",
      "weighted avg       0.60      0.64      0.60      2196\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zonkz\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\zonkz\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\zonkz\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True)\n",
    "X_batch, y_batch = next(iter(test_loader))\n",
    "predictions = model(X_batch).argmax(dim=1).cpu().detach()\n",
    "print(classification_report(y_batch, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        0.])\n",
      "tensor([0.9951, 0.0026, 0.0018]) tensor([ 3,  7, 10])\n",
      "kalashnikov: Russian (0.9951), Czech (0.0026), Greek (0.0018)\n"
     ]
    }
   ],
   "source": [
    "predict(model, train_dataset, \"kalashnikov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0.])\n",
      "tensor([0.9744, 0.0122, 0.0067]) tensor([3, 0, 7])\n",
      "tugolukov: Russian (0.9744), English (0.0122), Czech (0.0067)\n"
     ]
    }
   ],
   "source": [
    "predict(model, train_dataset, \"tugolukov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0.])\n",
      "tensor([0.5897, 0.1765, 0.0841]) tensor([3, 0, 7])\n",
      "popov: Russian (0.5897), English (0.1765), Czech (0.0841)\n"
     ]
    }
   ],
   "source": [
    "predict(model, train_dataset, \"popov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLmDB3fJtVox"
   },
   "source": [
    "## 3. Классификация обзоров ресторанов\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/nY1o70JtAuYa8g\n",
    "\n",
    "3.1 Считать файл `yelp/raw_train.csv`. Оставить от исходного датасета 10% строчек.\n",
    "\n",
    "3.2 Воспользоваться функцией `preprocess_text` из 1.1 для обработки текста отзыва. Закодировать рейтинг числами, начиная с 0.\n",
    "\n",
    "3.3 Разбить датасет на обучающую и тестовую выборку\n",
    "\n",
    "3.4 Реализовать класс `Vocab` (токен = слово)\n",
    "\n",
    "3.5 Реализовать класс `ReviewDataset`\n",
    "\n",
    "3.6 Обучить классификатор\n",
    "\n",
    "3.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: придумать небольшой отзыв, прогнать его через модель и вывести номер предсказанного класса (сделать это для явно позитивного и явно негативного отзыва)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zonkz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\zonkz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zonkz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([' ' if not char.isalpha() and char not in ['.', ',', '!', '?', \"'\"] else char for char in text])\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559995</th>\n",
       "      <td>2</td>\n",
       "      <td>Ryan was as good as everyone on yelp has claim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559996</th>\n",
       "      <td>2</td>\n",
       "      <td>Professional \\nFriendly\\nOn time AND affordabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559997</th>\n",
       "      <td>1</td>\n",
       "      <td>Phone calls always go to voicemail and message...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559998</th>\n",
       "      <td>1</td>\n",
       "      <td>Looks like all of the good reviews have gone t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559999</th>\n",
       "      <td>2</td>\n",
       "      <td>Ryan Rocks! I called him this morning for some...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rating                                             review\n",
       "0            1  Unfortunately, the frustration of being Dr. Go...\n",
       "1            2  Been going to Dr. Goldberg for over 10 years. ...\n",
       "2            1  I don't know what Dr. Goldberg was like before...\n",
       "3            1  I'm writing this review to give you a heads up...\n",
       "4            2  All the food is great here. But the best thing...\n",
       "...        ...                                                ...\n",
       "559995       2  Ryan was as good as everyone on yelp has claim...\n",
       "559996       2  Professional \\nFriendly\\nOn time AND affordabl...\n",
       "559997       1  Phone calls always go to voicemail and message...\n",
       "559998       1  Looks like all of the good reviews have gone t...\n",
       "559999       2  Ryan Rocks! I called him this morning for some...\n",
       "\n",
       "[560000 rows x 2 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train = pd.read_csv(\"data/yelp/raw_train.csv\", names=[\"rating\", \"review\"])\n",
    "\n",
    "raw_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21691</th>\n",
       "      <td>1</td>\n",
       "      <td>Horrible horrible horrible! Worst nail place E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21692</th>\n",
       "      <td>1</td>\n",
       "      <td>I went in her for the first time today for a g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rating                                             review\n",
       "21691       1  Horrible horrible horrible! Worst nail place E...\n",
       "21692       1  I went in her for the first time today for a g..."
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train[21691:21693]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выбор 10% случайных строк\n",
    "raw_train_10 = raw_train.sample(frac=0.005)\n",
    "# # сохранение выборки в новый файл\n",
    "# raw_train_10.to_csv('data/raw_train_10.csv', index=False)\n",
    "raw_train_10[\"review\"] = raw_train_10[\"review\"].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147948</th>\n",
       "      <td>2</td>\n",
       "      <td>amazing food , service we hope you listened to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512429</th>\n",
       "      <td>2</td>\n",
       "      <td>it 's cold in here ! and colder in the bathroom !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383534</th>\n",
       "      <td>1</td>\n",
       "      <td>to start off , the best thing about this place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221774</th>\n",
       "      <td>2</td>\n",
       "      <td>shopaholic galore ! ! ! my new favorite store ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540491</th>\n",
       "      <td>2</td>\n",
       "      <td>been here a few time , but my last visit wa th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268138</th>\n",
       "      <td>1</td>\n",
       "      <td>the first time i went to sauce , i had a sandw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538340</th>\n",
       "      <td>1</td>\n",
       "      <td>so my experience at the sl la vega resort . n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34748</th>\n",
       "      <td>1</td>\n",
       "      <td>owner would not provide confirmation number so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545897</th>\n",
       "      <td>2</td>\n",
       "      <td>the food here is good . the pho is good and we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21691</th>\n",
       "      <td>1</td>\n",
       "      <td>horrible horrible horrible ! worst nail place ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rating                                             review\n",
       "147948       2  amazing food , service we hope you listened to...\n",
       "512429       2  it 's cold in here ! and colder in the bathroom !\n",
       "383534       1  to start off , the best thing about this place...\n",
       "221774       2  shopaholic galore ! ! ! my new favorite store ...\n",
       "540491       2  been here a few time , but my last visit wa th...\n",
       "...        ...                                                ...\n",
       "268138       1  the first time i went to sauce , i had a sandw...\n",
       "538340       1  so my experience at the sl la vega resort . n ...\n",
       "34748        1  owner would not provide confirmation number so...\n",
       "545897       2  the food here is good . the pho is good and we...\n",
       "21691        1  horrible horrible horrible ! worst nail place ...\n",
       "\n",
       "[2800 rows x 2 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147948</th>\n",
       "      <td>0</td>\n",
       "      <td>amazing food , service we hope you listened to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512429</th>\n",
       "      <td>0</td>\n",
       "      <td>it 's cold in here ! and colder in the bathroom !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383534</th>\n",
       "      <td>1</td>\n",
       "      <td>to start off , the best thing about this place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221774</th>\n",
       "      <td>0</td>\n",
       "      <td>shopaholic galore ! ! ! my new favorite store ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540491</th>\n",
       "      <td>0</td>\n",
       "      <td>been here a few time , but my last visit wa th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268138</th>\n",
       "      <td>1</td>\n",
       "      <td>the first time i went to sauce , i had a sandw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538340</th>\n",
       "      <td>1</td>\n",
       "      <td>so my experience at the sl la vega resort . n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34748</th>\n",
       "      <td>1</td>\n",
       "      <td>owner would not provide confirmation number so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545897</th>\n",
       "      <td>0</td>\n",
       "      <td>the food here is good . the pho is good and we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21691</th>\n",
       "      <td>1</td>\n",
       "      <td>horrible horrible horrible ! worst nail place ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rating                                             review\n",
       "147948       0  amazing food , service we hope you listened to...\n",
       "512429       0  it 's cold in here ! and colder in the bathroom !\n",
       "383534       1  to start off , the best thing about this place...\n",
       "221774       0  shopaholic galore ! ! ! my new favorite store ...\n",
       "540491       0  been here a few time , but my last visit wa th...\n",
       "...        ...                                                ...\n",
       "268138       1  the first time i went to sauce , i had a sandw...\n",
       "538340       1  so my experience at the sl la vega resort . n ...\n",
       "34748        1  owner would not provide confirmation number so...\n",
       "545897       0  the food here is good . the pho is good and we...\n",
       "21691        1  horrible horrible horrible ! worst nail place ...\n",
       "\n",
       "[2800 rows x 2 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_10['rating'], rating_labels = pd.factorize(raw_train_10['rating'])\n",
    "raw_train_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([2, 1], dtype='int64')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "_lCTSKZgu68K"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15079"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Vocab:\n",
    "  def __init__(self, data):\n",
    "    self.idx_to_token = {}\n",
    "    self.token_to_idx = {}\n",
    "    self.vocab_len = 0\n",
    "\n",
    "    # Получаем список всех слов в данных\n",
    "    all_words = [word for sentence in data[\"review\"] for word in word_tokenize(sentence)]\n",
    "    # print(all_words)\n",
    "\n",
    "    # Строим словарь\n",
    "    for word in all_words:\n",
    "        if word not in self.token_to_idx:\n",
    "            self.idx_to_token[self.vocab_len] = word\n",
    "            self.token_to_idx[word] = self.vocab_len\n",
    "            self.vocab_len += 1\n",
    "\n",
    "vocab = Vocab(raw_train_10)\n",
    "vocab.vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'amazing',\n",
       " 1: 'food',\n",
       " 2: ',',\n",
       " 3: 'service',\n",
       " 4: 'we',\n",
       " 5: 'hope',\n",
       " 6: 'you',\n",
       " 7: 'listened',\n",
       " 8: 'to',\n",
       " 9: 'u',\n",
       " 10: 'blue',\n",
       " 11: 'and',\n",
       " 12: 'are',\n",
       " 13: 'checking',\n",
       " 14: 'this',\n",
       " 15: 'out',\n",
       " 16: '!',\n",
       " 17: 'drink',\n",
       " 18: '.',\n",
       " 19: 'our',\n",
       " 20: 'party',\n",
       " 21: 'of',\n",
       " 22: 'four',\n",
       " 23: 'arrived',\n",
       " 24: 'for',\n",
       " 25: 'reservation',\n",
       " 26: 'went',\n",
       " 27: 'the',\n",
       " 28: 'bar',\n",
       " 29: 'until',\n",
       " 30: 'were',\n",
       " 31: 'seated',\n",
       " 32: 'if',\n",
       " 33: 'like',\n",
       " 34: 'a',\n",
       " 35: 'lemon',\n",
       " 36: 'drop',\n",
       " 37: 'try',\n",
       " 38: 'lemonhead',\n",
       " 39: 'martini',\n",
       " 40: 'serving',\n",
       " 41: 'plenty',\n",
       " 42: 'on',\n",
       " 43: \"'tini\",\n",
       " 44: \"'s\",\n",
       " 45: 'did',\n",
       " 46: 'kobe',\n",
       " 47: 'beef',\n",
       " 48: 'surf',\n",
       " 49: 'turf',\n",
       " 50: 'tasting',\n",
       " 51: 'menu',\n",
       " 52: 'option',\n",
       " 53: 'since',\n",
       " 54: 'had',\n",
       " 55: 'prima',\n",
       " 56: 'card',\n",
       " 57: 'it',\n",
       " 58: 'lowered',\n",
       " 59: 'price',\n",
       " 60: 'person',\n",
       " 61: 'love',\n",
       " 62: 'that',\n",
       " 63: 'all',\n",
       " 64: 'still',\n",
       " 65: 'full',\n",
       " 66: 'from',\n",
       " 67: 'last',\n",
       " 68: 'night',\n",
       " 69: 'portion',\n",
       " 70: 'amount',\n",
       " 71: 'wa',\n",
       " 72: 'pretty',\n",
       " 73: 'i',\n",
       " 74: 'am',\n",
       " 75: 'picky',\n",
       " 76: 'eater',\n",
       " 77: 'in',\n",
       " 78: 'group',\n",
       " 79: 'they',\n",
       " 80: 'more',\n",
       " 81: 'than',\n",
       " 82: 'accommodating',\n",
       " 83: 'could',\n",
       " 84: \"n't\",\n",
       " 85: 'have',\n",
       " 86: 'made',\n",
       " 87: 'any',\n",
       " 88: 'easier',\n",
       " 89: 'n',\n",
       " 90: 'nto',\n",
       " 91: 'start',\n",
       " 92: 'really',\n",
       " 93: 'wanted',\n",
       " 94: 'lobster',\n",
       " 95: 'bisque',\n",
       " 96: 'which',\n",
       " 97: 'is',\n",
       " 98: 'included',\n",
       " 99: 'with',\n",
       " 100: 'but',\n",
       " 101: 'switched',\n",
       " 102: 'one',\n",
       " 103: 'main',\n",
       " 104: 'course',\n",
       " 105: 'so',\n",
       " 106: 'gave',\n",
       " 107: 'each',\n",
       " 108: 'small',\n",
       " 109: 'cup',\n",
       " 110: 'good',\n",
       " 111: 'chunk',\n",
       " 112: 'yummy',\n",
       " 113: 'broth',\n",
       " 114: 'first',\n",
       " 115: 'hawaiian',\n",
       " 116: 'prawn',\n",
       " 117: 'served',\n",
       " 118: 'head',\n",
       " 119: 'grilled',\n",
       " 120: 'cocktail',\n",
       " 121: 'sauce',\n",
       " 122: 'heirloom',\n",
       " 123: 'tomato',\n",
       " 124: 'salad',\n",
       " 125: 'olive',\n",
       " 126: 'oil',\n",
       " 127: 'think',\n",
       " 128: 'he',\n",
       " 129: 'said',\n",
       " 130: 'champagne',\n",
       " 131: 'vinegar',\n",
       " 132: 'fresh',\n",
       " 133: 'basil',\n",
       " 134: 'salt',\n",
       " 135: 'pepper',\n",
       " 136: 'roasted',\n",
       " 137: 'red',\n",
       " 138: 'surprisingly',\n",
       " 139: 'garlic',\n",
       " 140: 'tartare',\n",
       " 141: 'bread',\n",
       " 142: 'quail',\n",
       " 143: 'egg',\n",
       " 144: 'top',\n",
       " 145: 'yes',\n",
       " 146: 'just',\n",
       " 147: 'delicious',\n",
       " 148: 'refreshing',\n",
       " 149: 'nfor',\n",
       " 150: 'asked',\n",
       " 151: 'braised',\n",
       " 152: 'short',\n",
       " 153: 'rib',\n",
       " 154: 'traded',\n",
       " 155: 'these',\n",
       " 156: 'skirt',\n",
       " 157: 'steak',\n",
       " 158: 'there',\n",
       " 159: 'not',\n",
       " 160: 'enough',\n",
       " 161: 'word',\n",
       " 162: 'tell',\n",
       " 163: 'how',\n",
       " 164: 'absolutely',\n",
       " 165: 'succulent',\n",
       " 166: 'decadent',\n",
       " 167: 'flavorful',\n",
       " 168: 'seriously',\n",
       " 169: 'go',\n",
       " 170: 'entree',\n",
       " 171: 'flat',\n",
       " 172: 'iron',\n",
       " 173: 'filet',\n",
       " 174: 'other',\n",
       " 175: 'maine',\n",
       " 176: 'shell',\n",
       " 177: 'waiter',\n",
       " 178: 'took',\n",
       " 179: 'time',\n",
       " 180: 'take',\n",
       " 181: 'make',\n",
       " 182: 'much',\n",
       " 183: 'happy',\n",
       " 184: 'camper',\n",
       " 185: 'believe',\n",
       " 186: 'size',\n",
       " 187: 'definitely',\n",
       " 188: 'do',\n",
       " 189: 'skimp',\n",
       " 190: 'here',\n",
       " 191: 'side',\n",
       " 192: 'asparagus',\n",
       " 193: 'an',\n",
       " 194: 'assortment',\n",
       " 195: 'mushroom',\n",
       " 196: 'potato',\n",
       " 197: 'puree',\n",
       " 198: 'sweet',\n",
       " 199: 'corn',\n",
       " 200: 'table',\n",
       " 201: 'favorite',\n",
       " 202: 'nand',\n",
       " 203: 'now',\n",
       " 204: 'desert',\n",
       " 205: '....',\n",
       " 206: 'see',\n",
       " 207: 'final',\n",
       " 208: 'different',\n",
       " 209: 'type',\n",
       " 210: 'sorbet',\n",
       " 211: 'pineapple',\n",
       " 212: 'strawberry',\n",
       " 213: 'peach',\n",
       " 214: 'chocolate',\n",
       " 215: 'monkey',\n",
       " 216: 'banana',\n",
       " 217: 'ice',\n",
       " 218: 'cream',\n",
       " 219: 'rhubarb',\n",
       " 220: 'crisp',\n",
       " 221: 'tangerine',\n",
       " 222: 'vanilla',\n",
       " 223: 'bean',\n",
       " 224: 'cheesecake',\n",
       " 225: 'pavlova',\n",
       " 226: 'two',\n",
       " 227: 'plate',\n",
       " 228: 'fruit',\n",
       " 229: 'might',\n",
       " 230: 'be',\n",
       " 231: 'forgetting',\n",
       " 232: 'my',\n",
       " 233: 'well',\n",
       " 234: 'ni',\n",
       " 235: 'would',\n",
       " 236: 'also',\n",
       " 237: 'mention',\n",
       " 238: 'at',\n",
       " 239: 'craftsteak',\n",
       " 240: 'truly',\n",
       " 241: 'topnotch',\n",
       " 242: 'very',\n",
       " 243: 'knowledgeable',\n",
       " 244: 'ha',\n",
       " 245: 'some',\n",
       " 246: 'excellent',\n",
       " 247: 'suggestion',\n",
       " 248: 'staff',\n",
       " 249: 'assigned',\n",
       " 250: 'fantastic',\n",
       " 251: 'decor',\n",
       " 252: 'cool',\n",
       " 253: 'cold',\n",
       " 254: 'colder',\n",
       " 255: 'bathroom',\n",
       " 256: 'off',\n",
       " 257: 'best',\n",
       " 258: 'thing',\n",
       " 259: 'about',\n",
       " 260: 'place',\n",
       " 261: 'cleanliness',\n",
       " 262: 'friendliness',\n",
       " 263: 'incredibly',\n",
       " 264: 'affordable',\n",
       " 265: 'lunch',\n",
       " 266: 'special',\n",
       " 267: 'however',\n",
       " 268: 'nothing',\n",
       " 269: 'spectacular',\n",
       " 270: 'shopaholic',\n",
       " 271: 'galore',\n",
       " 272: 'new',\n",
       " 273: 'store',\n",
       " 274: 'shop',\n",
       " 275: 'variety',\n",
       " 276: 'choose',\n",
       " 277: 'location',\n",
       " 278: 'conveniently',\n",
       " 279: 'located',\n",
       " 280: 'la',\n",
       " 281: 'vega',\n",
       " 282: 'strip',\n",
       " 283: 'parking',\n",
       " 284: 'ample',\n",
       " 285: 'structured',\n",
       " 286: 'great',\n",
       " 287: 'eatery',\n",
       " 288: 'item',\n",
       " 289: 'bring',\n",
       " 290: 'home',\n",
       " 291: 'or',\n",
       " 292: \"'ve\",\n",
       " 293: 'forgot',\n",
       " 294: 'pack',\n",
       " 295: 'outfit',\n",
       " 296: 'deal',\n",
       " 297: 'spacious',\n",
       " 298: 'fashion',\n",
       " 299: 'show',\n",
       " 300: 'been',\n",
       " 301: 'few',\n",
       " 302: 'visit',\n",
       " 303: 'yet',\n",
       " 304: 'parent',\n",
       " 305: 'dad',\n",
       " 306: 'bday',\n",
       " 307: 'rebellion',\n",
       " 308: 'ba',\n",
       " 309: 'dunkel',\n",
       " 310: 'incredible',\n",
       " 311: 'always',\n",
       " 312: 'helpful',\n",
       " 313: 'attentive',\n",
       " 314: 'biggest',\n",
       " 315: 'fan',\n",
       " 316: 'grand',\n",
       " 317: 'opening',\n",
       " 318: 'come',\n",
       " 319: 'around',\n",
       " 320: 'beer',\n",
       " 321: 'craft',\n",
       " 322: 'brew',\n",
       " 323: 'downtown',\n",
       " 324: 'finally',\n",
       " 325: 'burger',\n",
       " 326: 'family',\n",
       " 327: 'day',\n",
       " 328: 'know',\n",
       " 329: 'handmade',\n",
       " 330: 'big',\n",
       " 331: 'everything',\n",
       " 332: 'homemade',\n",
       " 333: 'got',\n",
       " 334: 'maytag',\n",
       " 335: 'something',\n",
       " 336: 'normally',\n",
       " 337: 'server',\n",
       " 338: 'recommended',\n",
       " 339: 'want',\n",
       " 340: 'regular',\n",
       " 341: 'cheese',\n",
       " 342: 'came',\n",
       " 343: 'except',\n",
       " 344: 'piece',\n",
       " 345: 'bacon',\n",
       " 346: 'nice',\n",
       " 347: 'touch',\n",
       " 348: 'fixing',\n",
       " 349: 'darn',\n",
       " 350: 'thought',\n",
       " 351: 'little',\n",
       " 352: 'strong',\n",
       " 353: 'taste',\n",
       " 354: 'dinner',\n",
       " 355: 'cant',\n",
       " 356: 'recall',\n",
       " 357: 'what',\n",
       " 358: 'ate',\n",
       " 359: 'notch',\n",
       " 360: 'lemonade',\n",
       " 361: 'call',\n",
       " 362: 'ahead',\n",
       " 363: 'seating',\n",
       " 364: 'dont',\n",
       " 365: 'wait',\n",
       " 366: 'long',\n",
       " 367: 'busy',\n",
       " 368: 'atmosphere',\n",
       " 369: 'scottsdale',\n",
       " 370: 'cheap',\n",
       " 371: 'date',\n",
       " 372: 'case',\n",
       " 373: 'lovely',\n",
       " 374: 'wife',\n",
       " 375: 'sometimes',\n",
       " 376: 'forget',\n",
       " 377: 'live',\n",
       " 378: 'far',\n",
       " 379: 'gordon',\n",
       " 380: 'biersch',\n",
       " 381: 'before',\n",
       " 382: 'phoenix',\n",
       " 383: 'coyote',\n",
       " 384: 'game',\n",
       " 385: 'hour',\n",
       " 386: 'even',\n",
       " 387: 'unlike',\n",
       " 388: 'saddle',\n",
       " 389: 'ranch',\n",
       " 390: 'friendly',\n",
       " 391: 'micro',\n",
       " 392: 'house',\n",
       " 393: 'nmy',\n",
       " 394: 'only',\n",
       " 395: 'complaint',\n",
       " 396: 'available',\n",
       " 397: 'area',\n",
       " 398: 'otherwise',\n",
       " 399: 'solid',\n",
       " 400: 'noticed',\n",
       " 401: 'funny',\n",
       " 402: 'going',\n",
       " 403: 'portobello',\n",
       " 404: '...',\n",
       " 405: 'rennaissance',\n",
       " 406: 'part',\n",
       " 407: 'plethora',\n",
       " 408: 'cafe',\n",
       " 409: 'springing',\n",
       " 410: 'up',\n",
       " 411: 'especially',\n",
       " 412: 'edinburgh',\n",
       " 413: 'end',\n",
       " 414: 'high',\n",
       " 415: 'street',\n",
       " 416: 'decided',\n",
       " 417: 'check',\n",
       " 418: 'them',\n",
       " 419: 'chose',\n",
       " 420: 'butternut',\n",
       " 421: 'squash',\n",
       " 422: 'sounded',\n",
       " 423: 'healthy',\n",
       " 424: 'tempting',\n",
       " 425: 'stuff',\n",
       " 426: \"'m\",\n",
       " 427: 'between',\n",
       " 428: 'star',\n",
       " 429: 'gripe',\n",
       " 430: '?',\n",
       " 431: 'called',\n",
       " 432: 'making',\n",
       " 433: 'me',\n",
       " 434: 'interesting',\n",
       " 435: 'involve',\n",
       " 436: 'vegetable',\n",
       " 437: 'found',\n",
       " 438: 'standard',\n",
       " 439: 'fare',\n",
       " 440: 'tuna',\n",
       " 441: 'mayo',\n",
       " 442: 'marie',\n",
       " 443: 'rose',\n",
       " 444: 'sandwich',\n",
       " 445: 'baguette',\n",
       " 446: 'stovies',\n",
       " 447: 'roll',\n",
       " 448: 'mac',\n",
       " 449: 'pasta',\n",
       " 450: 'notice',\n",
       " 451: 'later',\n",
       " 452: 'extensive',\n",
       " 453: 'lady',\n",
       " 454: 'when',\n",
       " 455: 'turned',\n",
       " 456: 'seek',\n",
       " 457: 'ourselves',\n",
       " 458: 'pescatarian',\n",
       " 459: 'meat',\n",
       " 460: 'heavy',\n",
       " 461: 'struggle',\n",
       " 462: 'find',\n",
       " 463: 'anything',\n",
       " 464: 'nnow',\n",
       " 465: 'push',\n",
       " 466: 'baked',\n",
       " 467: 'huge',\n",
       " 468: 'albeit',\n",
       " 469: 'microwaved',\n",
       " 470: 'generous',\n",
       " 471: 'heaped',\n",
       " 472: 'bemusingly',\n",
       " 473: 'though',\n",
       " 474: 'butter',\n",
       " 475: 'surgical',\n",
       " 476: 'incision',\n",
       " 477: 'topping',\n",
       " 478: 'hot',\n",
       " 479: 'bother',\n",
       " 480: 'seemed',\n",
       " 481: 'odd',\n",
       " 482: 'serve',\n",
       " 483: 'way',\n",
       " 484: 'reasonable',\n",
       " 485: 'too',\n",
       " 486: 'nultimately',\n",
       " 487: 'because',\n",
       " 488: 'felt',\n",
       " 489: 'title',\n",
       " 490: 'belied',\n",
       " 491: 'name',\n",
       " 492: 'after',\n",
       " 493: 'then',\n",
       " 494: 'husband',\n",
       " 495: 'recently',\n",
       " 496: 'moved',\n",
       " 497: 'henderson',\n",
       " 498: 'office',\n",
       " 499: 'apartment',\n",
       " 500: 'complex',\n",
       " 501: 'their',\n",
       " 502: 'buffet',\n",
       " 503: 'she',\n",
       " 504: 'steer',\n",
       " 505: 'wrong',\n",
       " 506: 'dreamed',\n",
       " 507: 'walk',\n",
       " 508: 'heaven',\n",
       " 509: 'super',\n",
       " 510: 'wine',\n",
       " 511: 'must',\n",
       " 512: 'hard',\n",
       " 513: 'pear',\n",
       " 514: 'cider',\n",
       " 515: 'gon',\n",
       " 516: 'na',\n",
       " 517: 'waitress',\n",
       " 518: 'told',\n",
       " 519: 'pas',\n",
       " 520: 'pink',\n",
       " 521: 'color',\n",
       " 522: 'down',\n",
       " 523: 'easy',\n",
       " 524: 'unlimited',\n",
       " 525: 'eat',\n",
       " 526: 'tried',\n",
       " 527: 'get',\n",
       " 528: 'over',\n",
       " 529: 'half',\n",
       " 530: 'back',\n",
       " 531: 'soon',\n",
       " 532: 'dessert',\n",
       " 533: 'probably',\n",
       " 534: 'sure',\n",
       " 535: 'save',\n",
       " 536: 'room',\n",
       " 537: 'lot',\n",
       " 538: 'minute',\n",
       " 539: 'standing',\n",
       " 540: 'staring',\n",
       " 541: 'looking',\n",
       " 542: 'treat',\n",
       " 543: 'who',\n",
       " 544: 'can',\n",
       " 545: 'pick',\n",
       " 546: 'choice',\n",
       " 547: 'experience',\n",
       " 548: 'recommending',\n",
       " 549: 'friend',\n",
       " 550: 'sort',\n",
       " 551: 'disarray',\n",
       " 552: 'overlook',\n",
       " 553: 'cheapest',\n",
       " 554: 'gas',\n",
       " 555: 'besides',\n",
       " 556: 'circle',\n",
       " 557: 'k',\n",
       " 558: 'say',\n",
       " 559: 'old',\n",
       " 560: 'tired',\n",
       " 561: 'dated',\n",
       " 562: 'nunfortunately',\n",
       " 563: 'mgm',\n",
       " 564: 'seen',\n",
       " 565: 'brightest',\n",
       " 566: 'walked',\n",
       " 567: 'into',\n",
       " 568: 'lobby',\n",
       " 569: 'entire',\n",
       " 570: 'space',\n",
       " 571: 'sectioned',\n",
       " 572: 'possibly',\n",
       " 573: 'remodeling',\n",
       " 574: 'nthe',\n",
       " 575: 'hotel',\n",
       " 576: 'construction',\n",
       " 577: 'zone',\n",
       " 578: 'pet',\n",
       " 579: 'peeve',\n",
       " 580: 'keeping',\n",
       " 581: 'open',\n",
       " 582: 'while',\n",
       " 583: 'trying',\n",
       " 584: 'remodel',\n",
       " 585: 'simply',\n",
       " 586: 'doe',\n",
       " 587: 'work',\n",
       " 588: 'why',\n",
       " 589: 'anyone',\n",
       " 590: 'stay',\n",
       " 591: 'hammering',\n",
       " 592: 'noise',\n",
       " 593: 'dust',\n",
       " 594: 'basically',\n",
       " 595: 'fun',\n",
       " 596: 'harder',\n",
       " 597: 'where',\n",
       " 598: 'action',\n",
       " 599: 'cosmopolitan',\n",
       " 600: 'bellagio',\n",
       " 601: 'planet',\n",
       " 602: 'hollywood',\n",
       " 603: 'paris',\n",
       " 604: 'away',\n",
       " 605: 'tiger',\n",
       " 606: 'reason',\n",
       " 607: 'trip',\n",
       " 608: 'nlet',\n",
       " 609: 'complete',\n",
       " 610: 'overall',\n",
       " 611: 'better',\n",
       " 612: 'nmgm',\n",
       " 613: 'resort',\n",
       " 614: 'should',\n",
       " 615: 'focus',\n",
       " 616: 'flagship',\n",
       " 617: 'property',\n",
       " 618: 'actually',\n",
       " 619: 'modern',\n",
       " 620: 'entertaining',\n",
       " 621: 'embarrassing',\n",
       " 622: 'opinion',\n",
       " 623: 'your',\n",
       " 624: 'branding',\n",
       " 625: 'subpar',\n",
       " 626: 'most',\n",
       " 627: 'picked',\n",
       " 628: 'bus',\n",
       " 629: 'company',\n",
       " 630: 'disorganized',\n",
       " 631: 'such',\n",
       " 632: 'large',\n",
       " 633: 'people',\n",
       " 634: 'hopping',\n",
       " 635: 'ask',\n",
       " 636: 'question',\n",
       " 637: 'transferred',\n",
       " 638: 'rd',\n",
       " 639: 'continue',\n",
       " 640: 'canyon',\n",
       " 641: 'wish',\n",
       " 642: 'often',\n",
       " 643: 'spend',\n",
       " 644: 'money',\n",
       " 645: 'nsalmon',\n",
       " 646: 'kamameshi',\n",
       " 647: 'thouse',\n",
       " 648: 'enjoy',\n",
       " 649: 'salmon',\n",
       " 650: 'foie',\n",
       " 651: 'gras',\n",
       " 652: 'distinctly',\n",
       " 653: 'japanese',\n",
       " 654: 'done',\n",
       " 655: 'agedashi',\n",
       " 656: 'tofu',\n",
       " 657: 'melt',\n",
       " 658: 'month',\n",
       " 659: 'dashi',\n",
       " 660: 'nthis',\n",
       " 661: 'hurry',\n",
       " 662: 'american',\n",
       " 663: 'restaurant',\n",
       " 664: 'neverything',\n",
       " 665: 'wonderful',\n",
       " 666: 'manager',\n",
       " 667: 'ignore',\n",
       " 668: 'fact',\n",
       " 669: 'annoys',\n",
       " 670: 'gift',\n",
       " 671: 'given',\n",
       " 672: 'by',\n",
       " 673: 'inlaws',\n",
       " 674: 'spa',\n",
       " 675: 'offer',\n",
       " 676: 'local',\n",
       " 677: 'discount',\n",
       " 678: 'week',\n",
       " 679: 'espresso',\n",
       " 680: 'scrub',\n",
       " 681: 'skin',\n",
       " 682: 'use',\n",
       " 683: 'proper',\n",
       " 684: 'shedding',\n",
       " 685: 'winter',\n",
       " 686: 'named',\n",
       " 687: 'ginger',\n",
       " 688: 'soooo',\n",
       " 689: 'scrubbed',\n",
       " 690: 'front',\n",
       " 691: 'rinse',\n",
       " 692: 'rain',\n",
       " 693: 'shower',\n",
       " 694: 'rub',\n",
       " 695: 'creamy',\n",
       " 696: 'rich',\n",
       " 697: 'lotion',\n",
       " 698: 'nit',\n",
       " 699: 'exactly',\n",
       " 700: 'needed',\n",
       " 701: 'left',\n",
       " 702: 'feeling',\n",
       " 703: 'refreshed',\n",
       " 704: 'glowing',\n",
       " 705: 'inflexible',\n",
       " 706: 'reserved',\n",
       " 707: 'pairing',\n",
       " 708: 'advance',\n",
       " 709: 'understanding',\n",
       " 710: 'ntwo',\n",
       " 711: 'sent',\n",
       " 712: 'contained',\n",
       " 713: 'no',\n",
       " 714: 'chef',\n",
       " 715: 'refused',\n",
       " 716: 'alter',\n",
       " 717: 'her',\n",
       " 718: 'add',\n",
       " 719: 'non',\n",
       " 720: 'nchef',\n",
       " 721: 'self',\n",
       " 722: 'important',\n",
       " 723: 'accommodate',\n",
       " 724: 'clientele',\n",
       " 725: 'gone',\n",
       " 726: 'fairly',\n",
       " 727: 'eyebrow',\n",
       " 728: 'look',\n",
       " 729: 'job',\n",
       " 730: 'cleaning',\n",
       " 731: 'shape',\n",
       " 732: \"'ll\",\n",
       " 733: 'keep',\n",
       " 734: 'clean',\n",
       " 735: 'recommend',\n",
       " 736: 'seems',\n",
       " 737: 'idea',\n",
       " 738: 'sell',\n",
       " 739: 'scratch',\n",
       " 740: 'purchased',\n",
       " 741: 'stove',\n",
       " 742: 'sears',\n",
       " 743: 'outlet',\n",
       " 744: 'marking',\n",
       " 745: 'glass',\n",
       " 746: 'oven',\n",
       " 747: 'upon',\n",
       " 748: 'installation',\n",
       " 749: 'discovered',\n",
       " 750: 'leaking',\n",
       " 751: 'repair',\n",
       " 752: 'least',\n",
       " 753: 'installer',\n",
       " 754: 'badly',\n",
       " 755: 'damaged',\n",
       " 756: 'another',\n",
       " 757: 'put',\n",
       " 758: 'truck',\n",
       " 759: 'nsears',\n",
       " 760: 'saying',\n",
       " 761: 'fixed',\n",
       " 762: 'right',\n",
       " 763: 'failed',\n",
       " 764: 'inspect',\n",
       " 765: 'properly',\n",
       " 766: 'third',\n",
       " 767: 'charm',\n",
       " 768: 'completely',\n",
       " 769: 'helpless',\n",
       " 770: 'through',\n",
       " 771: 'process',\n",
       " 772: 'luckily',\n",
       " 773: 'leveraged',\n",
       " 774: 'corporate',\n",
       " 775: 'district',\n",
       " 776: 'semblance',\n",
       " 777: 'customer',\n",
       " 778: 'ongoing',\n",
       " 779: 'same',\n",
       " 780: 'bought',\n",
       " 781: 'range',\n",
       " 782: 'hood',\n",
       " 783: 'quality',\n",
       " 784: 'fine',\n",
       " 785: 'delivery',\n",
       " 786: 'poorly',\n",
       " 787: 'handled',\n",
       " 788: 'ended',\n",
       " 789: 'getting',\n",
       " 790: 'coupon',\n",
       " 791: 'cash',\n",
       " 792: 'nso',\n",
       " 793: 'depot',\n",
       " 794: 'lowes',\n",
       " 795: 'craigslist',\n",
       " 796: 'will',\n",
       " 797: 'establishment',\n",
       " 798: 'owner',\n",
       " 799: 'amazingly',\n",
       " 800: 'conversation',\n",
       " 801: 'disappoint',\n",
       " 802: 'honestly',\n",
       " 803: 'visitor',\n",
       " 804: 'myself',\n",
       " 805: 'give',\n",
       " 806: 'chance',\n",
       " 807: 'hand',\n",
       " 808: 'care',\n",
       " 809: 'his',\n",
       " 810: 'thanks',\n",
       " 811: 'locked',\n",
       " 812: 'laptop',\n",
       " 813: 'key',\n",
       " 814: 'min',\n",
       " 815: 'craig',\n",
       " 816: 'awesome',\n",
       " 817: 'door',\n",
       " 818: 'unlocked',\n",
       " 819: 'under',\n",
       " 820: 'extra',\n",
       " 821: 'next',\n",
       " 822: 'happens',\n",
       " 823: 'wo',\n",
       " 824: 'nthanks',\n",
       " 825: 'bill',\n",
       " 826: 'clinton',\n",
       " 827: 'ever',\n",
       " 828: 'fair',\n",
       " 829: 'city',\n",
       " 830: 'safe',\n",
       " 831: 'may',\n",
       " 832: 'stop',\n",
       " 833: 'cigar',\n",
       " 834: 'eye',\n",
       " 835: 'consider',\n",
       " 836: 'expert',\n",
       " 837: 'hobby',\n",
       " 838: 'vastrange',\n",
       " 839: 'variant',\n",
       " 840: \"'d\",\n",
       " 841: 'provides',\n",
       " 842: 'comprehensive',\n",
       " 843: 'selection',\n",
       " 844: 'those',\n",
       " 845: 'toot',\n",
       " 846: 'couple',\n",
       " 847: 'both',\n",
       " 848: 'headace',\n",
       " 849: 'nausea',\n",
       " 850: 'quite',\n",
       " 851: 'smell',\n",
       " 852: 'used',\n",
       " 853: 'imagining',\n",
       " 854: 'titanic',\n",
       " 855: 'resplendent',\n",
       " 856: 'smoking',\n",
       " 857: 'jacket',\n",
       " 858: 'brandy',\n",
       " 859: 'sinking',\n",
       " 860: 'icy',\n",
       " 861: 'depth',\n",
       " 862: 'splendid',\n",
       " 863: 'popeye',\n",
       " 864: 'wonder',\n",
       " 865: 'drawn',\n",
       " 866: 'robert',\n",
       " 867: 'm',\n",
       " 868: 'review',\n",
       " 869: 'checked',\n",
       " 870: 'qcom',\n",
       " 871: 'website',\n",
       " 872: 'ordered',\n",
       " 873: 'holiday',\n",
       " 874: 'state',\n",
       " 875: 'simple',\n",
       " 876: 'using',\n",
       " 877: 'credit',\n",
       " 878: 'secure',\n",
       " 879: 'site',\n",
       " 880: 'received',\n",
       " 881: 'confirmation',\n",
       " 882: 'via',\n",
       " 883: 'email',\n",
       " 884: 'second',\n",
       " 885: 'transaction',\n",
       " 886: 'enjoys',\n",
       " 887: 'arizona',\n",
       " 888: 'forward',\n",
       " 889: 'visiting',\n",
       " 890: 'queen',\n",
       " 891: 'creek',\n",
       " 892: 'mill',\n",
       " 893: 'sometime',\n",
       " 894: 'field',\n",
       " 895: 'bedside',\n",
       " 896: 'manner',\n",
       " 897: 'rude',\n",
       " 898: 'patient',\n",
       " 899: 'excusable',\n",
       " 900: 'furthermore',\n",
       " 901: 'initial',\n",
       " 902: 'appt',\n",
       " 903: 'paid',\n",
       " 904: 'return',\n",
       " 905: 'latrer',\n",
       " 906: 'blood',\n",
       " 907: 'test',\n",
       " 908: 'result',\n",
       " 909: 'additional',\n",
       " 910: 'never',\n",
       " 911: 'happen',\n",
       " 912: 'unfortunately',\n",
       " 913: 'knew',\n",
       " 914: 'shame',\n",
       " 915: 'being',\n",
       " 916: 'disgrace',\n",
       " 917: 'profession',\n",
       " 918: 'expiration',\n",
       " 919: 'notorious',\n",
       " 920: 'having',\n",
       " 921: 'expired',\n",
       " 922: 'stock',\n",
       " 923: 'staffed',\n",
       " 924: 'brought',\n",
       " 925: 'confronted',\n",
       " 926: 'bit',\n",
       " 927: 'interested',\n",
       " 928: 'avoid',\n",
       " 929: 'shopping',\n",
       " 930: 'poor',\n",
       " 931: 'smile',\n",
       " 932: 'cashier',\n",
       " 933: 'seem',\n",
       " 934: 'update',\n",
       " 935: 'comment',\n",
       " 936: 'post',\n",
       " 937: 'nsince',\n",
       " 938: 'wrote',\n",
       " 939: 'disrespectful',\n",
       " 940: 'dog',\n",
       " 941: 'convince',\n",
       " 942: 'veterinarian',\n",
       " 943: 'impersonate',\n",
       " 944: 'asking',\n",
       " 945: 'permission',\n",
       " 946: 'speak',\n",
       " 947: 'vet',\n",
       " 948: 'information',\n",
       " 949: 'exonerate',\n",
       " 950: 'effort',\n",
       " 951: 'somehow',\n",
       " 952: 'guilt',\n",
       " 953: 'free',\n",
       " 954: 'situation',\n",
       " 955: 'childrens',\n",
       " 956: 'necklace',\n",
       " 957: 'exaggeration',\n",
       " 958: 'truth',\n",
       " 959: 'likely',\n",
       " 960: 'blatant',\n",
       " 961: 'lie',\n",
       " 962: 'elongated',\n",
       " 963: 'plastic',\n",
       " 964: 'regardless',\n",
       " 965: 'feel',\n",
       " 966: 'biscuit',\n",
       " 967: 'based',\n",
       " 968: 'solely',\n",
       " 969: 'wear',\n",
       " 970: 'let',\n",
       " 971: 'led',\n",
       " 972: 'object',\n",
       " 973: 'arriving',\n",
       " 974: 'institution',\n",
       " 975: 'someone',\n",
       " 976: 'dropped',\n",
       " 977: 'dropping',\n",
       " 978: 'taking',\n",
       " 979: 'tour',\n",
       " 980: 'threw',\n",
       " 981: 'own',\n",
       " 982: 'vomit',\n",
       " 983: 'point',\n",
       " 984: 'numerous',\n",
       " 985: 'belligerent',\n",
       " 986: 'phone',\n",
       " 987: 'emailed',\n",
       " 988: 'content',\n",
       " 989: 'response',\n",
       " 990: 'posted',\n",
       " 991: 'le',\n",
       " 992: 'professional',\n",
       " 993: 'ungodly',\n",
       " 994: 'liar',\n",
       " 995: 'posting',\n",
       " 996: 'e',\n",
       " 997: 'mail',\n",
       " 998: 'clear',\n",
       " 999: 'hypothesis',\n",
       " ...}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.idx_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "WXLmCDvcvRmb"
   },
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "  def __init__(self, X, y, vocab: Vocab):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.vocab = vocab\n",
    "\n",
    "  def vectorize(self, review):\n",
    "    '''Генерирует представление отзыва review при помощи бинарного кодирования (см. 1.2)'''\n",
    "    # vec = torch.zeros(self.vocab.vocab_len, dtype=torch.long)\n",
    "    vec = torch.zeros(self.vocab.vocab_len)\n",
    "    \n",
    "    # Проходим по каждому слову в фамилии\n",
    "    # print(f\"tokenized surname: {word_tokenize(surname)}\")\n",
    "    for word in word_tokenize(review):\n",
    "      # print(word)\n",
    "      # Если слово есть в словаре, устанавливаем соответствующий бит в векторе\n",
    "      if word in self.vocab.token_to_idx:\n",
    "          vec[self.vocab.token_to_idx[word]] = 1\n",
    "  \n",
    "    return vec\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    vec = self.vectorize(self.X[idx])\n",
    "    label = self.y[idx]\n",
    "    return vec, label\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(raw_train_10['review'].to_numpy(), raw_train_10['rating'].to_numpy(), test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = Vocab(surnames)\n",
    "\n",
    "train_dataset = ReviewDataset(X_train, y_train, vocab)\n",
    "test_dataset = ReviewDataset(X_test, y_test, vocab)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(vocab.vocab_len, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 2),\n",
    "    nn.LogSoftmax(dim=1),\n",
    ")\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15079"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=15079, out_features=1024, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  (3): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab_size = len(vocab.token_to_idx)\n",
    "# embedding_dim = 8\n",
    "# hidden_dim = 16\n",
    "# output_dim = 2\n",
    "# n_layers = 2\n",
    "# bidirectional = True\n",
    "# dropout = 0.5\n",
    "\n",
    "# # Создание экземпляра модели\n",
    "# model = ReviewClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout)\n",
    "# model = ReviewClassifier(vocab_size=vocab.vocab_len,\n",
    "#                            embedding_dim=4,\n",
    "#                            output_dim=len(set(y_train)))\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# evaluate_model(model, test_dataloader)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество обучаемых параметров: 15443970\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Количество обучаемых параметров: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.4042, Test Loss: 0.3598\n",
      "Epoch 2, Train Loss: 0.0718, Test Loss: 0.5117\n",
      "Epoch 3, Train Loss: 0.0162, Test Loss: 0.6241\n",
      "Epoch 4, Train Loss: 0.0036, Test Loss: 0.7178\n",
      "Epoch 5, Train Loss: 0.0015, Test Loss: 0.7421\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_dataloader, test_dataloader, criterion, optimizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_labels = [\"Positive\", \"Negative\"]\n",
    "\n",
    "def predict(model, dataset, review):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        vectorized = dataset.vectorize(review)\n",
    "        tensor = vectorized.unsqueeze(0).to(device)\n",
    "        logits = model(tensor)\n",
    "        probs = torch.softmax(logits, dim=1).squeeze()\n",
    "        print(probs)\n",
    "        print(f'{rating_labels[probs.argmax()]} ({probs.max():.4f}), {rating_labels[probs.argmin()]} ({probs.min():.4f}) \\n{review}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.84107\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.82      0.85       297\n",
      "           1       0.81      0.86      0.84       263\n",
      "\n",
      "    accuracy                           0.84       560\n",
      "   macro avg       0.84      0.84      0.84       560\n",
      "weighted avg       0.84      0.84      0.84       560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True)\n",
    "X_batch, y_batch = next(iter(test_loader))\n",
    "predictions = model(X_batch).argmax(dim=1).cpu().detach()\n",
    "print(classification_report(y_batch, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.3120e-05, 9.9998e-01])\n",
      "Negative (1.0000), Positive (0.0000) \n",
      "I had a terrible experience at this restaurant. The staff was rude and the food was overpriced for the quality.\n"
     ]
    }
   ],
   "source": [
    "predict(model, train_dataset, \"I had a terrible experience at this restaurant. The staff was rude and the food was overpriced for the quality.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.9990e-01, 9.8094e-05])\n",
      "Positive (0.9999), Negative (0.0001) \n",
      "This restaurant is simply amazing! The food is delicious and the service is outstanding.\n"
     ]
    }
   ],
   "source": [
    "predict(model, train_dataset, \"This restaurant is simply amazing! The food is delicious and the service is outstanding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0057, 0.9943])\n",
      "Negative (0.9943), Positive (0.0057) \n",
      "The menu at this restaurant is very limited and the food is nothing special. I wouldn't go back.\n"
     ]
    }
   ],
   "source": [
    "predict(model, train_dataset, \"The menu at this restaurant is very limited and the food is nothing special. I wouldn't go back.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9809, 0.0191])\n",
      "Positive (0.9809), Negative (0.0191) \n",
      "I can't say enough good things about this restaurant. It's the perfect place for a romantic dinner or a night out with friends.\n"
     ]
    }
   ],
   "source": [
    "predict(model, train_dataset, \"I can't say enough good things about this restaurant. It's the perfect place for a romantic dinner or a night out with friends.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.9954e-01, 4.5701e-04])\n",
      "Positive (0.9995), Negative (0.0005) \n",
      "I had the best dining experience in this restaurant. The ambiance is perfect and the staff is very friendly.\n"
     ]
    }
   ],
   "source": [
    "predict(model, train_dataset, \"I had the best dining experience in this restaurant. The ambiance is perfect and the staff is very friendly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMJ70DQQj2X/XaG2BMq6jy8",
   "collapsed_sections": [],
   "name": "blank__05_NLP_1_intro.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

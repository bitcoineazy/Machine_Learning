{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-11blKEOcjlk"
      },
      "source": [
        "# 2. Создание нейронной сети без использования готовых решений\n",
        "\n",
        "__Автор__: Никита Владимирович Блохин (NVBlokhin@fa.ru)\n",
        "\n",
        "Финансовый университет, 2020 г. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PqC4R7SGseKa"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J2RM8f5wP33"
      },
      "source": [
        "## 1. Создание нейронов и полносвязных слоев"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2ArJn_nsdZC"
      },
      "source": [
        "1.1. Используя операции над матрицами и векторами из библиотеки `torch`, реализовать нейрон с заданными весами `weights` и `bias`. Прогнать вектор `inputs` через нейрон и вывести результат. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4agkY9WqPwe"
      },
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "  def __init__(self, weights, bias):\n",
        "    # <создать атрибуты объекта weights и bias>\n",
        "    self.weights = weights\n",
        "    self.bias = bias\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    return (inputs * self.weights).sum() + self.bias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJRkSkHHsb7u"
      },
      "outputs": [],
      "source": [
        "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
        "weights = torch.tensor([-0.2, 0.3, -0.5, 0.7])\n",
        "bias = 3.14"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qJvnwiyty37"
      },
      "source": [
        "1.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой с заданными весами `weights` и `biases`. Прогнать вектор `inputs` через слой и вывести результат. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fVWF3a9vtx90"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "  def __init__(self, weights, biases):\n",
        "    self.weights = weights\n",
        "    self.biases = biases\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    return torch.mv(self.weights, inputs)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fo-JFnHPuFCS"
      },
      "outputs": [],
      "source": [
        "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
        "weights = torch.tensor([[-0.2, 0.3, -0.5, 0.7],\n",
        "                        [0.5, -0.91, 0.26, -0.5],\n",
        "                        [-0.26, -0.27, 0.17, 0.87]])\n",
        "\n",
        "biases = torch.tensor([3.14, 2.71, 7.2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 1.7000, -2.5400,  3.1900])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "linear = Linear(weights, biases)\n",
        "\n",
        "linear.forward(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRVH_2K7xTBC"
      },
      "source": [
        "## 2. Создание функций активации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9kngE6Fxs9D"
      },
      "source": [
        "2.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ReLU:\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/f4353f4e3e484130504049599d2e7b040793e1eb)\n",
        "\n",
        "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jZLvMRByxSTC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 0, 0, 1, 2])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class ReLU:\n",
        "  def forward(self, inputs):\n",
        "    return inputs.clip(min=0)\n",
        "\n",
        "relu = ReLU()\n",
        "\n",
        "relu.forward(torch.tensor([1, -1, -6, 1, 2]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puExCWiKyTtb"
      },
      "source": [
        "2.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации softmax:\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/6d7500d980c313da83e4117da701bf7c8f1982f5)\n",
        "\n",
        "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации. Строки матрицы трактовать как выходы линейного слоя некоторого классификатора для 4 различных примеров."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "fXNcFlqqyKHl"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0414, 0.1125, 0.8310, 0.0152],\n",
              "        [0.0889, 0.2418, 0.0120, 0.6572]])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class Softmax:\n",
        "  def forward(self, inputs):\n",
        "    # <реализовать логику Softmax>\n",
        "    exp = torch.exp(inputs)\n",
        "    return exp / exp.sum(dim=1, keepdim=True)\n",
        "\n",
        "softmax = Softmax()\n",
        "\n",
        "softmax.forward(torch.tensor([[1, 2, 4, 0], [3, 4, 1., 5]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 7.],\n",
              "        [13.]])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.tensor([[1, 2, 4, 0], [3, 4, 1., 5]]).sum(1, keepdim=True)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([20.])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.sum(-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0peh8r-20Pof"
      },
      "source": [
        "## 3. Создание функции потерь"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY-k3eEs0f7f"
      },
      "source": [
        "3.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь MSE:\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e258221518869aa1c6561bb75b99476c4734108e)\n",
        "\n",
        "Посчитать значение MSE, трактуя вектор `y` как вектор правильных ответов, а `y_pred`, как вектор предсказаний."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "f9-wdj5Tz-br"
      },
      "outputs": [],
      "source": [
        "class MSELoss:\n",
        "  def forward(self, y_pred, y_true):\n",
        "    return torch.mean((y_pred - y_true)**2) # <реализовать логику MSE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "NAyuDU9F1Vuz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.3333)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
        "\n",
        "y = torch.tensor([2, 3, 4], dtype=torch.float32)\n",
        "\n",
        "\n",
        "mse = MSELoss()\n",
        "\n",
        "mse.forward(y_pred, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w049ZSdR6qQi"
      },
      "source": [
        "## 4. Обратное распространение ошибки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBtCfSME9W7Q"
      },
      "source": [
        "4.1 Используя один нейрон и SGD (1 пример за шаг), решите задачу регрессии"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "4xmI-QJ66WAF"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "\n",
        "X, y, coef = make_regression(n_features=4, n_informative=4, coef=True, bias=0.5)\n",
        "X = torch.tensor(X, dtype=torch.float32) # <преобразуйте массивы numpy в тензоры torch с типом torch.float32\n",
        "y = torch.tensor(y, dtype=torch.float32) # <преобразуйте массивы numpy в тензоры torch с типом torch.float32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([92.72010351, 81.63856342, 31.04647857, 38.91685895])"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "coef"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpPSPYSpD9Ey"
      },
      "source": [
        "[Граф вычислений для этой задачи](https://i.ibb.co/2dhDxZx/photo-2021-02-15-17-18-04.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc1sXtGd_J-y"
      },
      "source": [
        "4.1.1 Модифицируйте класс `MSELoss` из __2.3.1__, реализовав расчет производной относительно предыдущего слоя\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "llFigkqd_JRU"
      },
      "outputs": [],
      "source": [
        "class MSELoss:\n",
        "  def forward(self, y_pred, y_true):\n",
        "    return torch.mean((y_pred - y_true)**2) # <реализовать логику MSE>\n",
        "\n",
        "  def backward(self, y_pred, y_true):\n",
        "    self.dinput = 2 * (y_pred - y_true) # df/dc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY7ForfM97UQ"
      },
      "source": [
        "4.1.2. Модифицируйте класс `Neuron` из __2.1.1__:\n",
        "\n",
        "  1) Сделайте так, чтобы веса нейрона инициализировались из стандартного нормального распределения\n",
        "\n",
        "  2) Реализуйте расчет градиента относительно весов `weights` и `bias`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "L0KqxPJU9kAN"
      },
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "  def __init__(self, n_inputs):\n",
        "    # <создать атрибуты объекта weights и bias>\n",
        "    # self.n_inputs = n_inputs\n",
        "    self.weights = torch.randn(n_inputs)\n",
        "    self.bias = torch.randn(1)\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    self.input = inputs\n",
        "    return (inputs * self.weights).sum() + self.bias # <реализовать логику нейрона>\n",
        "  \n",
        "  def backward(self, dvalue):\n",
        "    # dvalue - значение производной, которое приходит нейрону от следующего слоя сети\n",
        "    # в данном случае это будет значение df/dc (созданное методом backwards у объекта MSELoss)\n",
        "    self.dweights = dvalue * self.input # df/dW\n",
        "    self.dinput = dvalue * self.weights # df/wX\n",
        "    self.dbias = dvalue # df/db\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKcO4zOLACxM"
      },
      "source": [
        "4.1.3 Допишите цикл для настройки весов нейрона\n",
        "\n",
        "[SGD](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D1%81%D0%BF%D1%83%D1%81%D0%BA)\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/dda3670f8a8996a0d3bf80856bb4a166cc8db6d4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "_g_FvwvmALJd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 mean loss 1061.6754150390625\n",
            "epoch 5 mean loss 176.94667053222656\n",
            "epoch 10 mean loss 96.51636505126953\n",
            "epoch 15 mean loss 66.3550033569336\n",
            "epoch 20 mean loss 50.556190490722656\n",
            "epoch 25 mean loss 40.83384704589844\n",
            "epoch 30 mean loss 34.24774169921875\n",
            "epoch 35 mean loss 29.491111755371094\n",
            "epoch 40 mean loss 25.894634246826172\n",
            "epoch 45 mean loss 23.079999923706055\n",
            "epoch 50 mean loss 20.8172550201416\n",
            "epoch 55 mean loss 18.95857048034668\n",
            "epoch 60 mean loss 17.404590606689453\n",
            "epoch 65 mean loss 16.086061477661133\n",
            "epoch 70 mean loss 14.953239440917969\n",
            "epoch 75 mean loss 13.969473838806152\n",
            "epoch 80 mean loss 13.107160568237305\n",
            "epoch 85 mean loss 12.34511661529541\n",
            "epoch 90 mean loss 11.666812896728516\n",
            "epoch 95 mean loss 11.05916690826416\n",
            "epoch 100 mean loss 10.511683464050293\n",
            "epoch 105 mean loss 10.015849113464355\n",
            "epoch 110 mean loss 9.564684867858887\n",
            "epoch 115 mean loss 9.152413368225098\n",
            "epoch 120 mean loss 8.774214744567871\n",
            "epoch 125 mean loss 8.426032066345215\n",
            "epoch 130 mean loss 8.104427337646484\n",
            "epoch 135 mean loss 7.8064703941345215\n",
            "epoch 140 mean loss 7.5296454429626465\n",
            "epoch 145 mean loss 7.271780967712402\n",
            "epoch 150 mean loss 7.030993461608887\n",
            "epoch 155 mean loss 6.805641174316406\n",
            "epoch 160 mean loss 6.594285488128662\n",
            "epoch 165 mean loss 6.395662784576416\n",
            "epoch 170 mean loss 6.208654880523682\n",
            "epoch 175 mean loss 6.032272815704346\n",
            "epoch 180 mean loss 5.865635395050049\n",
            "epoch 185 mean loss 5.707956790924072\n",
            "epoch 190 mean loss 5.558534145355225\n",
            "epoch 195 mean loss 5.41673469543457\n",
            "epoch 200 mean loss 5.281990051269531\n",
            "epoch 205 mean loss 5.1537861824035645\n",
            "epoch 210 mean loss 5.03165864944458\n",
            "epoch 215 mean loss 4.91518497467041\n",
            "epoch 220 mean loss 4.803981781005859\n",
            "epoch 225 mean loss 4.697699069976807\n",
            "epoch 230 mean loss 4.596017360687256\n",
            "epoch 235 mean loss 4.49864387512207\n",
            "epoch 240 mean loss 4.405311107635498\n",
            "epoch 245 mean loss 4.315772533416748\n",
            "epoch 250 mean loss 4.229800701141357\n",
            "epoch 255 mean loss 4.14718770980835\n",
            "epoch 260 mean loss 4.067739486694336\n",
            "epoch 265 mean loss 3.9912781715393066\n",
            "epoch 270 mean loss 3.9176383018493652\n",
            "epoch 275 mean loss 3.8466665744781494\n",
            "epoch 280 mean loss 3.7782206535339355\n",
            "epoch 285 mean loss 3.712167739868164\n",
            "epoch 290 mean loss 3.6483848094940186\n",
            "epoch 295 mean loss 3.586756706237793\n",
            "epoch 300 mean loss 3.5271761417388916\n",
            "epoch 305 mean loss 3.4695425033569336\n",
            "epoch 310 mean loss 3.413762092590332\n",
            "epoch 315 mean loss 3.3597469329833984\n",
            "epoch 320 mean loss 3.3074142932891846\n",
            "epoch 325 mean loss 3.2566871643066406\n",
            "epoch 330 mean loss 3.2074923515319824\n",
            "epoch 335 mean loss 3.159761905670166\n",
            "epoch 340 mean loss 3.113430976867676\n",
            "epoch 345 mean loss 3.068439245223999\n",
            "epoch 350 mean loss 3.0247292518615723\n",
            "epoch 355 mean loss 2.9822471141815186\n",
            "epoch 360 mean loss 2.94094181060791\n",
            "epoch 365 mean loss 2.9007649421691895\n",
            "epoch 370 mean loss 2.861671209335327\n",
            "epoch 375 mean loss 2.8236169815063477\n",
            "epoch 380 mean loss 2.7865617275238037\n",
            "epoch 385 mean loss 2.7504663467407227\n",
            "epoch 390 mean loss 2.715294122695923\n",
            "epoch 395 mean loss 2.6810100078582764\n",
            "epoch 400 mean loss 2.647581100463867\n",
            "epoch 405 mean loss 2.6149754524230957\n",
            "epoch 410 mean loss 2.583163022994995\n",
            "epoch 415 mean loss 2.5521154403686523\n",
            "epoch 420 mean loss 2.5218052864074707\n",
            "epoch 425 mean loss 2.492206573486328\n",
            "epoch 430 mean loss 2.463294744491577\n",
            "epoch 435 mean loss 2.4350459575653076\n",
            "epoch 440 mean loss 2.407437562942505\n",
            "epoch 445 mean loss 2.380448341369629\n",
            "epoch 450 mean loss 2.354057550430298\n",
            "epoch 455 mean loss 2.3282456398010254\n",
            "epoch 460 mean loss 2.3029935359954834\n",
            "epoch 465 mean loss 2.2782833576202393\n",
            "epoch 470 mean loss 2.2540977001190186\n",
            "epoch 475 mean loss 2.2304201126098633\n",
            "epoch 480 mean loss 2.2072348594665527\n",
            "epoch 485 mean loss 2.1845266819000244\n",
            "epoch 490 mean loss 2.162281036376953\n",
            "epoch 495 mean loss 2.140483856201172\n",
            "epoch 500 mean loss 2.119121789932251\n",
            "epoch 505 mean loss 2.09818172454834\n",
            "epoch 510 mean loss 2.0776517391204834\n",
            "epoch 515 mean loss 2.0575194358825684\n",
            "epoch 520 mean loss 2.037773609161377\n",
            "epoch 525 mean loss 2.0184030532836914\n",
            "epoch 530 mean loss 1.9993973970413208\n",
            "epoch 535 mean loss 1.9807462692260742\n",
            "epoch 540 mean loss 1.9624398946762085\n",
            "epoch 545 mean loss 1.9444688558578491\n",
            "epoch 550 mean loss 1.9268239736557007\n",
            "epoch 555 mean loss 1.9094964265823364\n",
            "epoch 560 mean loss 1.8924777507781982\n",
            "epoch 565 mean loss 1.8757597208023071\n",
            "epoch 570 mean loss 1.8593344688415527\n",
            "epoch 575 mean loss 1.8431944847106934\n",
            "epoch 580 mean loss 1.8273321390151978\n",
            "epoch 585 mean loss 1.8117406368255615\n",
            "epoch 590 mean loss 1.7964128255844116\n",
            "epoch 595 mean loss 1.7813422679901123\n",
            "epoch 600 mean loss 1.7665224075317383\n",
            "epoch 605 mean loss 1.7519471645355225\n",
            "epoch 610 mean loss 1.7376104593276978\n",
            "epoch 615 mean loss 1.7235064506530762\n",
            "epoch 620 mean loss 1.7096296548843384\n",
            "epoch 625 mean loss 1.6959744691848755\n",
            "epoch 630 mean loss 1.6825356483459473\n",
            "epoch 635 mean loss 1.6693081855773926\n",
            "epoch 640 mean loss 1.6562870740890503\n",
            "epoch 645 mean loss 1.6434675455093384\n",
            "epoch 650 mean loss 1.6308448314666748\n",
            "epoch 655 mean loss 1.6184146404266357\n",
            "epoch 660 mean loss 1.6061724424362183\n",
            "epoch 665 mean loss 1.594114065170288\n",
            "epoch 670 mean loss 1.5822354555130005\n",
            "epoch 675 mean loss 1.5705325603485107\n",
            "epoch 680 mean loss 1.5590014457702637\n",
            "epoch 685 mean loss 1.5476385354995728\n",
            "epoch 690 mean loss 1.5364398956298828\n",
            "epoch 695 mean loss 1.525402307510376\n",
            "epoch 700 mean loss 1.5145220756530762\n",
            "epoch 705 mean loss 1.5037959814071655\n",
            "epoch 710 mean loss 1.4932208061218262\n",
            "epoch 715 mean loss 1.4827933311462402\n",
            "epoch 720 mean loss 1.4725104570388794\n",
            "epoch 725 mean loss 1.4623692035675049\n",
            "epoch 730 mean loss 1.452366590499878\n",
            "epoch 735 mean loss 1.4424999952316284\n",
            "epoch 740 mean loss 1.4327665567398071\n",
            "epoch 745 mean loss 1.4231635332107544\n",
            "epoch 750 mean loss 1.4136884212493896\n",
            "epoch 755 mean loss 1.4043385982513428\n",
            "epoch 760 mean loss 1.3951116800308228\n",
            "epoch 765 mean loss 1.386005163192749\n",
            "epoch 770 mean loss 1.3770169019699097\n",
            "epoch 775 mean loss 1.3681442737579346\n",
            "epoch 780 mean loss 1.359385371208191\n",
            "epoch 785 mean loss 1.3507379293441772\n",
            "epoch 790 mean loss 1.3421998023986816\n",
            "epoch 795 mean loss 1.3337688446044922\n",
            "epoch 800 mean loss 1.325443148612976\n",
            "epoch 805 mean loss 1.3172208070755005\n",
            "epoch 810 mean loss 1.3090999126434326\n",
            "epoch 815 mean loss 1.30107843875885\n",
            "epoch 820 mean loss 1.2931547164916992\n",
            "epoch 825 mean loss 1.2853268384933472\n",
            "epoch 830 mean loss 1.2775932550430298\n",
            "epoch 835 mean loss 1.2699521780014038\n",
            "epoch 840 mean loss 1.2624019384384155\n",
            "epoch 845 mean loss 1.2549408674240112\n",
            "epoch 850 mean loss 1.2475675344467163\n",
            "epoch 855 mean loss 1.2402803897857666\n",
            "epoch 860 mean loss 1.2330777645111084\n",
            "epoch 865 mean loss 1.2259584665298462\n",
            "epoch 870 mean loss 1.2189208269119263\n",
            "epoch 875 mean loss 1.211963415145874\n",
            "epoch 880 mean loss 1.2050851583480835\n",
            "epoch 885 mean loss 1.198284387588501\n",
            "epoch 890 mean loss 1.1915600299835205\n",
            "epoch 895 mean loss 1.1849106550216675\n",
            "epoch 900 mean loss 1.178335189819336\n",
            "epoch 905 mean loss 1.1718322038650513\n",
            "epoch 910 mean loss 1.1654006242752075\n",
            "epoch 915 mean loss 1.1590392589569092\n",
            "epoch 920 mean loss 1.1527470350265503\n",
            "epoch 925 mean loss 1.1465226411819458\n",
            "epoch 930 mean loss 1.1403652429580688\n",
            "epoch 935 mean loss 1.1342735290527344\n",
            "epoch 940 mean loss 1.128246545791626\n",
            "epoch 945 mean loss 1.1222833395004272\n",
            "epoch 950 mean loss 1.1163827180862427\n",
            "epoch 955 mean loss 1.110543966293335\n",
            "epoch 960 mean loss 1.1047658920288086\n",
            "epoch 965 mean loss 1.0990476608276367\n",
            "epoch 970 mean loss 1.093388319015503\n",
            "epoch 975 mean loss 1.0877869129180908\n",
            "epoch 980 mean loss 1.0822426080703735\n",
            "epoch 985 mean loss 1.0767545700073242\n",
            "epoch 990 mean loss 1.0713218450546265\n",
            "epoch 995 mean loss 1.065943717956543\n"
          ]
        }
      ],
      "source": [
        "n_inputs = 4 # <размерность элемента выборки >\n",
        "learning_rate = 0.1 #  скорость обучения\n",
        "n_epoch = 1000 #  количество эпох\n",
        "\n",
        "neuron = Neuron(n_inputs)\n",
        "loss = MSELoss()\n",
        "\n",
        "losses = []\n",
        "\n",
        "# print(zip(X, y))\n",
        "for epoch in range(n_epoch):\n",
        "  \n",
        "  for x_example, y_example in zip(X, y):\n",
        "    # forward pass\n",
        "    y_pred = neuron.forward(x_example) # <прогон через нейрон>\n",
        "    curr_loss = loss.forward(y_pred, y_example) # <прогон через функцию потерь>\n",
        "    losses.append(curr_loss)\n",
        "\n",
        "    # backprop\n",
        "    # <вызов методов backward>\n",
        "    loss.backward(y_pred, y_example)\n",
        "    neuron.backward(loss.dinput)\n",
        "    # обратите внимание на последовательность вызовов: от конца к началу\n",
        "    # print(neuron.weights)\n",
        "    neuron.weights -= learning_rate * neuron.dweights\n",
        "    neuron.bias -= learning_rate * neuron.dbias\n",
        "    # <шаг оптимизации для весов (weights и bias) нейрона>\n",
        "\n",
        "  if epoch % 5 == 0:\n",
        "    print(f\"epoch {epoch} mean loss {torch.stack(losses).mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_ief7m1D2rJ"
      },
      "source": [
        "4.2 Работа с батчами"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr9qq4H_J3zt"
      },
      "source": [
        "4.2.1 Модифицируйте класс `MSELoss` из __3.1__, реализовав расчет производной относительно предыдущего слоя с учетом того, что теперь работа ведется с батчами, а не с индивидуальными примерами\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "L8wjk9iPMQ4x"
      },
      "outputs": [],
      "source": [
        "class MSELoss:\n",
        "  def forward(self, y_pred, y_true):\n",
        "    return torch.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "  def backward(self, y_pred, y_true):\n",
        "    self.dinput = 2 * (y_pred - y_true) / len(y_true) # df/dy^\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3fSHCEtJjX8"
      },
      "source": [
        "4.2.2. Модифицируйте класс `Neuron` из __4.1.2__:\n",
        "\n",
        "  1) Реализуйте метод `forward` таким образом, чтобы он мог принимать на вход матрицу (батч) с данными. \n",
        "\n",
        "  2) Реализуйте расчет градиента относительно весов `weights` и `bias` с учетом того, что теперь работа ведется с батчами, а не с индивидуальными примерами"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "o_OpuAP0Jpz1"
      },
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "  def __init__(self, n_inputs):\n",
        "    # <создать атрибуты объекта weights и bias>\n",
        "    self.weights = torch.randn(n_inputs)\n",
        "    self.bias = torch.tensor(1)\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    self.inputs = inputs\n",
        "    return X @ self.weights.T + self.bias # <реализовать логику нейрона>\n",
        "  \n",
        "  def backward(self, dvalue):\n",
        "    # dvalue - значение градиента, которое приходит нейрону от следующего слоя сети\n",
        "    # в данном случае это будет градиент L по y^ (созданный методом backwards у объекта MSELoss)\n",
        "    self.dweights = self.inputs.T.mv(dvalue) # df/dW\n",
        "    self.dbias = dvalue.sum() # df/db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 1.5436, -0.7334, -3.5034,  3.7786,  0.3309,  0.6941, -0.6677, -0.5985,\n",
              "         5.2927, -1.9528,  1.0723,  0.5473,  3.3397,  0.2918,  1.3232,  6.6151,\n",
              "         1.6719, -0.9794,  4.0265, -3.3048, -2.1791,  1.0269,  3.3949, -2.4096,\n",
              "         2.4610, -1.9779, -0.7896, -3.5132,  0.2139,  0.8727,  2.8040,  2.0103,\n",
              "         2.4520,  6.9876, -0.5074,  6.1217,  3.0923,  2.1367,  3.6616,  2.1603,\n",
              "        -1.8690,  3.1847,  0.7969,  1.7345, -2.0365,  3.2037,  3.4775, -1.2714,\n",
              "         0.6715,  2.3635, -0.3252, -3.7998,  0.3381,  6.1449, -4.3615,  3.5708,\n",
              "         4.6176,  1.8989,  2.8456, -2.4962,  0.9244, -2.6139,  3.8309,  7.2379,\n",
              "         2.3081,  2.6172, -3.0549,  4.5765, -3.3604,  5.2114, -1.1295,  0.1421,\n",
              "         5.6711,  4.1402,  0.3938, -0.0890, -0.3209, -0.3914, -0.8806, -2.7761,\n",
              "        -1.1531, -3.1797,  8.5684,  2.1941, -2.6391,  3.6554,  2.5856,  3.5995,\n",
              "         4.3258,  4.8613, -5.4909, -0.9576,  4.3073, -1.3862, -5.2191,  3.4535,\n",
              "         5.1568,  1.8934,  1.3001, -0.6002])"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "neuron = Neuron(4)\n",
        "\n",
        "neuron.forward(torch.randn((2, 4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVpBoMRMlM5Z"
      },
      "source": [
        "# Лабораторная (домашняя) работа 1.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZthSpoJI_dLu"
      },
      "source": [
        "## 1. Создание нейронов и полносвязных слоев"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQtsJzcxuyGd"
      },
      "source": [
        "1.3 Реализовать полносвязный слой из __2.1.2__ таким образом, чтобы он мог принимать на вход матрицу (батч) с данными. Продемонстрировать работу.\n",
        "Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Z8IizmtsuhO1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([-1.2847,  4.8706,  6.1706]),\n",
              " tensor([ 4.6422,  4.6035, -0.7359]),\n",
              " tensor([-2.9321, -3.3037, 11.1187])]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class Linear:\n",
        "  def __init__(self, weights, biases):\n",
        "    # <создать атрибуты объекта weights и biases>\n",
        "    self.weights = weights\n",
        "    self.biases = biases\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    A = []\n",
        "    for input in inputs:\n",
        "      A.append(torch.mv(self.weights, input) + self.biases) # <реализовать логику слоя>\n",
        "    return A\n",
        "\n",
        "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
        "                       [2, 5, -1, 2], \n",
        "                       [-1.5, 2.7, 3.3, -0.8]])\n",
        "weights = torch.randn((3, 4))\n",
        "biases = torch.randn(3)\n",
        "# print(weights)\n",
        "\n",
        "batchlayer = Linear(weights, biases)\n",
        "batchlayer.forward(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ2OxH4_vBLu"
      },
      "source": [
        "1.4 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой из `n_neurons` нейронов с `n_features` весами у каждого нейрона (инициализируются из стандартного нормального распределения). Прогнать вектор `inputs` через слой и вывести результат. Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "IOv52EdovASs"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  3.5617,  -0.9918,  -7.8641,   0.4858,   1.1843],\n",
              "        [  7.0173,   1.1546, -13.5138,  -3.0467,   7.0333],\n",
              "        [-12.1832,   6.8911,   0.5088,  -1.8770,  -5.0833]])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class Linear:\n",
        "  def __init__(self, n_features, n_neurons):\n",
        "    # <создать атрибуты объекта weights и biases>\n",
        "    self.weights = torch.normal(0, 1, (n_features, n_neurons))\n",
        "    self.biases = torch.zeros(1, n_neurons)\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    res = inputs @ self.weights + self.biases # <реализовать логику слоя>\n",
        "    return res\n",
        "  \n",
        "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
        "                       [2, 5, -1, 2], \n",
        "                       [-1.5, 2.7, 3.3, -0.8]])\n",
        "std_batchlayer = Linear(4, 5)\n",
        "std_batchlayer.forward(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPG4UqL4wajI"
      },
      "source": [
        "1.5 Используя решение из __1.4__, создать 2 полносвязных слоя и пропустить матрицу `inputs` последовательно через эти два слоя. Количество нейронов в первом слое выбрать произвольно, количество нейронов во втором слое выбрать так, чтобы результатом прогона являлась матрица (3x7). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "RjjQIQlTxJE6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 12.1654,   7.1405,   3.1448,   3.1827,   7.4268, -12.1658,  -0.2828],\n",
            "        [ 17.3693,  19.0116,  47.5131,  18.5378,  20.5940, -15.5535,  -7.0064],\n",
            "        [  8.8630,  11.7157, -12.0191,  -7.3838,   8.6876, -18.2906,   0.6435]])\n",
            "torch.Size([3, 7])\n"
          ]
        }
      ],
      "source": [
        "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
        "                       [2, 5, -1, 2],\n",
        "                       [-1.5, 2.7, 3.3, -0.8]])\n",
        "\n",
        "\n",
        "# создаем два полносвязных слоя\n",
        "layer1 = Linear(4, 5)\n",
        "layer2 = Linear(5, 7)\n",
        "\n",
        "# пропускаем матрицу inputs через два слоя\n",
        "hidden_layer = layer1.forward(inputs)\n",
        "output_layer = layer2.forward(hidden_layer)\n",
        "\n",
        "# выводим результат\n",
        "print(output_layer)\n",
        "print(output_layer.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4YcuWJI_gA-"
      },
      "source": [
        "## 2. Создание функций активации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxVK2TYez_Ye"
      },
      "source": [
        "2.3 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ELU:\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/eb23becd37c3602c4838e53f532163279192e4fd)\n",
        "\n",
        "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "NzMz7HDLySxK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.3964,  1.1330, -0.3533],\n",
            "        [ 0.5806,  1.8933,  1.3552],\n",
            "        [-0.6124, -0.5413, -0.0195],\n",
            "        [ 0.3653, -0.9320,  0.2052]])\n"
          ]
        }
      ],
      "source": [
        "class ELU:\n",
        "  def __init__(self, alpha):\n",
        "    # <создать атрибут объекта alpha>\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # <реализовать логику ReLU>\n",
        "    return torch.where(inputs < 0, self.alpha * (torch.exp(inputs) - 1), inputs)\n",
        "  \n",
        "elu = ELU(alpha=1.0)\n",
        "\n",
        "inputs = torch.randn((4, 3))  # создаем матрицу размера (4,3), заполненную числами из стандартного нормального распределения\n",
        "output = elu.forward(inputs)  # применяем ELU к матрице inputs\n",
        "\n",
        "print(output)  # выводим результат"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aH5R4gL_ijM"
      },
      "source": [
        "## 3. Создание функции потерь"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaR7rILd1eWR"
      },
      "source": [
        "3.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь Categorical Cross-Entropy:\n",
        "\n",
        "<img src=\"https://i.ibb.co/93gy1dN/Screenshot-9.png\" width=\"200\">\n",
        "\n",
        "Создать полносвязный слой с 3 нейронами и прогнать через него батч `inputs`. Полученный результат пропустить через функцию активации softmax. Посчитать значение CCE, трактуя вектор `y` как вектор правильных ответов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "hQl8pJsT3HcF"
      },
      "outputs": [],
      "source": [
        "class CategoricalCrossentropyLoss:\n",
        "  def forward(self, y_pred, y_true):\n",
        "    # <реализовать логику CCE>\n",
        "    return - torch.sum(y_true * torch.log(y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "s7Qoupfo1ZGJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(7.3771, grad_fn=<NegBackward0>)\n"
          ]
        }
      ],
      "source": [
        "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
        "                        [2, 5, -1, 2], \n",
        "                        [-1.5, 2.7, 3.3, -0.8]])\n",
        "y = torch.tensor([1, 0, 0])\n",
        "\n",
        "layer = torch.nn.Linear(4, 3)\n",
        "\n",
        "# Пропускаем батч через слой и функцию активации softmax\n",
        "softmax = Softmax()\n",
        "y_pred = softmax.forward(layer(inputs))\n",
        "\n",
        "\n",
        "# Вычисляем значение функции потерь CCE\n",
        "criterion = CategoricalCrossentropyLoss()\n",
        "loss = criterion.forward(y_pred, y)\n",
        "\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA6dbanf44_4"
      },
      "source": [
        "3.3 Модифицировать 2.3.1, добавив L2-регуляризацию.\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/d92ca2429275bfdc0474523babbafe014ca8b580)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ADsZxD-h4_Os"
      },
      "outputs": [],
      "source": [
        "class MSELossL2:\n",
        "  def __init__(self, lambda_):\n",
        "    # <создать атрибут объекта alpha>\n",
        "    self.alpha = lambda_\n",
        "\n",
        "  def data_loss(self, y_pred, y_true):\n",
        "    # <подсчет первого слагаемого из формулы>\n",
        "    return torch.mean((y_pred - y_true)**2)\n",
        "\n",
        "  def reg_loss(self, layer):\n",
        "    # используйте атрибуты объекта layer, в которых хранятся веса слоя\n",
        "    # <подсчет второго слагаемого из формулы>\n",
        "    return self.alpha * torch.sum(layer.weight**2)\n",
        "\n",
        "  def forward(self, y_pred, y_true):\n",
        "    return self.data_loss(y_pred, y_true) + self.reg_loss(y_pred, y_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQeZW7rJ_lpN"
      },
      "source": [
        "## 4. Обратное распространение ошибки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebibge9VEgF7"
      },
      "source": [
        "4.2 Решите задачу 4.1, используя пакетный градиентный спуск"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as-QeWSdOELd"
      },
      "source": [
        "Вычисления для этой задачи: \n",
        "[1](https://i.ibb.co/rmtQT6P/photo-2021-02-15-18-00-43.jpg)\n",
        "[2](https://i.ibb.co/NmCFVnQ/photo-2021-02-15-18-01-17.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO-NZrgKMBFx"
      },
      "source": [
        "4.2.3 Допишите цикл для настройки весов нейрона"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "  def __init__(self, n_inputs):\n",
        "    # <создать атрибуты объекта weights и bias>\n",
        "    # self.n_inputs = n_inputs\n",
        "    self.weights = torch.randn((n_inputs, 1))\n",
        "    self.bias = torch.randn(1)\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    self.input = inputs\n",
        "    return (inputs * self.weights).sum() + self.bias # <реализовать логику нейрона>\n",
        "  \n",
        "  def backward(self, dvalue):\n",
        "    # dvalue - значение производной, которое приходит нейрону от следующего слоя сети\n",
        "    # в данном случае это будет значение df/dc (созданное методом backwards у объекта MSELoss)\n",
        "    self.dweights = dvalue * self.input # df/dW\n",
        "    self.dinput = dvalue * self.weights # df/wX\n",
        "    self.dbias = dvalue # df/db\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "Zqwm_7eqJim1"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (100) must match the size of tensor b (4) at non-singleton dimension 0",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/home/noble6/DEV/Machine_Learning/KABYTE_FA/01_/02_NN_blocks_backprop_v1.ipynb Cell 62\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/noble6/DEV/Machine_Learning/KABYTE_FA/01_/02_NN_blocks_backprop_v1.ipynb#Y113sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/noble6/DEV/Machine_Learning/KABYTE_FA/01_/02_NN_blocks_backprop_v1.ipynb#Y113sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epoch):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/noble6/DEV/Machine_Learning/KABYTE_FA/01_/02_NN_blocks_backprop_v1.ipynb#Y113sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/noble6/DEV/Machine_Learning/KABYTE_FA/01_/02_NN_blocks_backprop_v1.ipynb#Y113sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m neuron\u001b[39m.\u001b[39;49mforward(X) \u001b[39m# <прогон через нейрон>\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/noble6/DEV/Machine_Learning/KABYTE_FA/01_/02_NN_blocks_backprop_v1.ipynb#Y113sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mprint\u001b[39m(neuron\u001b[39m.\u001b[39minput\u001b[39m.\u001b[39msize())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/noble6/DEV/Machine_Learning/KABYTE_FA/01_/02_NN_blocks_backprop_v1.ipynb#Y113sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mprint\u001b[39m(neuron\u001b[39m.\u001b[39mweights\u001b[39m.\u001b[39msize())\n",
            "\u001b[1;32m/home/noble6/DEV/Machine_Learning/KABYTE_FA/01_/02_NN_blocks_backprop_v1.ipynb Cell 62\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/noble6/DEV/Machine_Learning/KABYTE_FA/01_/02_NN_blocks_backprop_v1.ipynb#Y113sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/noble6/DEV/Machine_Learning/KABYTE_FA/01_/02_NN_blocks_backprop_v1.ipynb#Y113sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput \u001b[39m=\u001b[39m inputs\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/noble6/DEV/Machine_Learning/KABYTE_FA/01_/02_NN_blocks_backprop_v1.ipynb#Y113sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m (inputs \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights)\u001b[39m.\u001b[39msum() \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (100) must match the size of tensor b (4) at non-singleton dimension 0"
          ]
        }
      ],
      "source": [
        "n_inputs = 4 # <размерность элемента выборки >\n",
        "learning_rate = 0.1 # скорость обучения\n",
        "n_epoch = 100 # количество эпох\n",
        "\n",
        "neuron = Neuron(n_inputs)\n",
        "\n",
        "loss = MSELoss()\n",
        "\n",
        "# создание матрицы X и вектора y\n",
        "# X = torch.randn(100, n_inputs)\n",
        "# y = torch.randn(100)\n",
        "\n",
        "X, y, coef = make_regression(n_features=n_inputs, n_informative=n_inputs, coef=True, bias=0.5)\n",
        "X = torch.tensor(X, dtype=torch.float32) # <преобразуйте массивы numpy в тензоры torch с типом torch.float32\n",
        "y = torch.tensor(y, dtype=torch.float32) # <преобразуйте массивы numpy в тензоры torch с типом torch.float32\n",
        "# print(X.shape, y.shape)\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "    # forward pass\n",
        "    y_pred = neuron.forward(X) # <прогон через нейрон>\n",
        "    print(neuron.input.size())\n",
        "    print(neuron.weights.size())\n",
        "    curr_loss = loss.forward(y_pred, y) # <прогон через функцию потерь>\n",
        "    losses.append(curr_loss)\n",
        "\n",
        "    # backprop\n",
        "    # <вызов методов backward>\n",
        "    loss.backward(y_pred, y)\n",
        "    neuron.backward(loss.dinput)\n",
        "\n",
        "    # <шаг оптимизации для весов (weights и bias) нейрона>\n",
        "    neuron.weights -= learning_rate * neuron.dweights.mean(dim=0)\n",
        "    neuron.bias -= learning_rate * neuron.dbias.mean()\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        print(f\"epoch {epoch} mean loss {curr_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16VtP159OdMk"
      },
      "source": [
        "4.3  Используя один полносвязный слой и  пакетный градиетный спуск, решите задачу регрессии из __2.4.1__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj5febreSSZ7"
      },
      "source": [
        "4.3.1 Модифицируйте класс `Linear` из __1.4__. ([вычисление градиентов](https://i.ibb.co/kgVR6m6/photo-2021-02-15-21-30-28.jpg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Linear:\n",
        "  def __init__(self, n_features, n_neurons):\n",
        "    # <создать атрибуты объекта weights и biases>\n",
        "    self.weights = torch.normal(0, 1, (n_features, n_neurons))\n",
        "    self.biases = torch.zeros(1, n_neurons)\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    res = inputs @ self.weights + self.biases # <реализовать логику слоя>\n",
        "    return res\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "9zWuhaLdSB2_"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "  def __init__(self, n_features, n_neurons):\n",
        "    self.weights = torch.normal(0, 1, (n_features, n_neurons), requires_grad=True)\n",
        "    self.biases = torch.zeros(1, n_neurons, requires_grad=True)\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    res = inputs @ self.weights + self.biases\n",
        "    return res\n",
        "\n",
        "  def backward(self, dvalues):\n",
        "    self.dweights = self.weights.grad\n",
        "    self.dbiases = self.biases.grad\n",
        "    self.dinputs = dvalues @ self.weights.t()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3w1hT9MS_Lt"
      },
      "source": [
        "4.3.2 Создайте слой с одним нейроном. Используя класс MSELoss из 2.4.2, убедитесь, что модель обучается"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Linear:\n",
        "    def __init__(self, n_features, n_neurons):\n",
        "        self.weights = torch.normal(0, 1, (n_features, n_neurons), requires_grad=True)\n",
        "        self.biases = torch.zeros(1, n_neurons, requires_grad=True)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        res = inputs @ self.weights + self.biases\n",
        "        return res\n",
        "\n",
        "    def backward(self, dvalues):\n",
        "        self.dweights = self.weights.grad\n",
        "        self.dbiases = self.biases.grad\n",
        "        self.dinputs = dvalues @ self.weights.t()\n",
        "\n",
        "# MSELoss из 2.4.2\n",
        "class MSELoss:\n",
        "    def forward(self, y_pred, y_true):\n",
        "        return torch.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "    def backward(self, y_pred, y_true):\n",
        "        self.dinput = torch.mean(2 * (y_pred - y_true)) # df/dy^\n",
        "\n",
        "\n",
        "X, y, coef = make_regression(n_features=1, n_informative=1, coef=True, bias=0.5)\n",
        "X = torch.tensor(X, dtype=torch.float32) # <преобразуйте массивы numpy в тензоры torch с типом torch.float32\n",
        "y = torch.tensor(y, dtype=torch.float32) # <преобразуйте массивы numpy в тензоры torch с типом torch.float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTkJV-F8TVuN"
      },
      "source": [
        "4.4 Используя наработки из 2.4, создайте нейросеть и решите задачу регрессии.\n",
        "\n",
        "Предлагаемая архитектура: \n",
        "1. Полносвязный слой с 10 нейронами\n",
        "2. Активация ReLU\n",
        "3. Полносвязный слой с 1 нейроном"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axUjpPz-SvS1"
      },
      "outputs": [],
      "source": [
        "X = torch.linspace(-1, 1, 100).view(-1, 1)\n",
        "y = X.pow(2) + 0.2 * torch.rand(X.size()) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXoiNxkpTziV"
      },
      "outputs": [],
      "source": [
        "class Activation_ReLU:\n",
        "  def forward(self, inputs):\n",
        "    self.inputs = inputs\n",
        "    self.output = inputs.clip(min=0)\n",
        "    return self.output\n",
        "  \n",
        "  def backward(self, dvalues):\n",
        "    self.dinputs = dvalues.clone()\n",
        "    self.dinputs[self.inputs <= 0] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXhspwW6T44T"
      },
      "outputs": [],
      "source": [
        "# создание компонентов сети\n",
        "# fc1 = \n",
        "# relu1 = \n",
        "# fc2 = \n",
        "\n",
        "loss = MSELoss()\n",
        "lr = 0.02\n",
        "\n",
        "ys = []\n",
        "for epoch in range(2001):\n",
        "  # <forward pass>\n",
        "  # fc1 > relu1 > fc2 > loss\n",
        "\n",
        "  data_loss = # <прогон через функцию потерь>\n",
        "\n",
        "  if epoch % 200 == 0:\n",
        "    print(f'epoch {epoch} mean loss {data_loss}')\n",
        "    ys.append(out)\n",
        "  \n",
        "  # <backprop> \n",
        "  # loss > fc2 > relu1 > fc1\n",
        "\n",
        "  # <шаг оптимизации для fc1>\n",
        "\n",
        "  # <шаг оптимизации для fc2>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "kpKi0OfoUkwk"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'ys' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/noble6/DEV/Machine_Learning/KABYTE_FA/01_/02_NN_blocks_backprop_v1.ipynb Cell 73\u001b[0m in \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/noble6/DEV/Machine_Learning/KABYTE_FA/01_/02_NN_blocks_backprop_v1.ipynb#Y125sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/noble6/DEV/Machine_Learning/KABYTE_FA/01_/02_NN_blocks_backprop_v1.ipynb#Y125sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m fig, axs \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39mlen\u001b[39m(ys), \u001b[39m1\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m40\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/noble6/DEV/Machine_Learning/KABYTE_FA/01_/02_NN_blocks_backprop_v1.ipynb#Y125sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m ax, y_ \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(axs, ys):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/noble6/DEV/Machine_Learning/KABYTE_FA/01_/02_NN_blocks_backprop_v1.ipynb#Y125sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m   ax\u001b[39m.\u001b[39mscatter(X\u001b[39m.\u001b[39mnumpy(), y\u001b[39m.\u001b[39mnumpy(), color \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39morange\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ys' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(len(ys), 1, figsize=(10, 40))\n",
        "for ax, y_ in zip(axs, ys):\n",
        "  ax.scatter(X.numpy(), y.numpy(), color = \"orange\")\n",
        "  ax.plot(X.numpy(), y_.numpy(), 'g-', lw=3)\n",
        "  ax.set_xlim(-1.05, 1.5)\n",
        "  ax.set_ylim(-0.25, 1.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6ktSXdQVb9Li"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# ДЗ 3.1, 3.2,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L7Inb1JbiDy"
      },
      "source": [
        "## 3.1 Автоматическое дифференцирование в `torch`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsZ69HZ0EsI7"
      },
      "source": [
        "3.1.1 Воспользовавшись классами `Neuron` и `SquaredLoss` из задачи 2.4.1 и автоматическим дифференцированием, которое предоставляет `torch`, решить задачу регрессии. Для оптимизации использовать стохастический градиетный спуск."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zynPAaOrRKTm"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "\n",
        "X, y, coef = make_regression(n_features=4, n_informative=4, coef=True, bias=0.5)\n",
        "X = torch.from_numpy(X).to(dtype=torch.float32)\n",
        "y = torch.from_numpy(y).to(dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEGf4VcQP7zY"
      },
      "outputs": [],
      "source": [
        "class SquaredLoss:\n",
        "  def forward(self, y_pred, y_true):\n",
        "    self.y_pred = torch.tensor(y_pred, requires_grad=True)\n",
        "    self.z = (self.y_pred - y_true) ** 2\n",
        "    return self.z\n",
        "\n",
        "  def backward(self):\n",
        "    self.z.backward()\n",
        "    self.dinput =  self.y_pred.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkKvrnrzWez_"
      },
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "  def __init__(self, n_inputs):\n",
        "    self.n_inputs = n_inputs\n",
        "    self.weights = torch.randn(n_inputs)\n",
        "    self.bias = torch.randn(1)\n",
        "  \n",
        "  def forward(self, input):\n",
        "    self.input = input\n",
        "    return torch.dot(self.weights, input) + self.bias\n",
        "  \n",
        "  def backward(self, dvalue):\n",
        "    # dvalue - значение производной, которое приходит нейрону от следующего слоя сети\n",
        "    # в данном случае это будет значение df/dc (созданное методом backwards у объекта MSELoss) - по сути производная ошибки dE\n",
        "    self.dweights = dvalue * self.input # df/dW\n",
        "    self.dinput =  dvalue * self.weights # df/wX\n",
        "    self.dbias = dvalue # df/db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1m6LBQtWh5m",
        "outputId": "bdcce9aa-5720-45c6-b39a-d33cb9109c1c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-c53ef009eda7>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.y_pred = torch.tensor(y_pred, requires_grad=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 mean loss tensor([7395.1396], grad_fn=<DivBackward0>)\n",
            "epoch 1 mean loss tensor([5478.8691], grad_fn=<DivBackward0>)\n",
            "epoch 2 mean loss tensor([4286.9160], grad_fn=<DivBackward0>)\n",
            "epoch 3 mean loss tensor([3515.9370], grad_fn=<DivBackward0>)\n",
            "epoch 4 mean loss tensor([2887.9343], grad_fn=<DivBackward0>)\n",
            "epoch 5 mean loss tensor([2461.2390], grad_fn=<DivBackward0>)\n",
            "epoch 6 mean loss tensor([2134.7031], grad_fn=<DivBackward0>)\n",
            "epoch 7 mean loss tensor([1887.9072], grad_fn=<DivBackward0>)\n",
            "epoch 8 mean loss tensor([1683.5214], grad_fn=<DivBackward0>)\n",
            "epoch 9 mean loss tensor([1521.6914], grad_fn=<DivBackward0>)\n",
            "epoch 10 mean loss tensor([1388.1731], grad_fn=<DivBackward0>)\n",
            "epoch 11 mean loss tensor([1273.9130], grad_fn=<DivBackward0>)\n",
            "epoch 12 mean loss tensor([1176.4785], grad_fn=<DivBackward0>)\n",
            "epoch 13 mean loss tensor([1092.6427], grad_fn=<DivBackward0>)\n",
            "epoch 14 mean loss tensor([1019.9420], grad_fn=<DivBackward0>)\n",
            "epoch 15 mean loss tensor([956.2825], grad_fn=<DivBackward0>)\n",
            "epoch 16 mean loss tensor([900.0526], grad_fn=<DivBackward0>)\n",
            "epoch 17 mean loss tensor([850.0582], grad_fn=<DivBackward0>)\n",
            "epoch 18 mean loss tensor([805.3213], grad_fn=<DivBackward0>)\n",
            "epoch 19 mean loss tensor([765.0570], grad_fn=<DivBackward0>)\n",
            "epoch 20 mean loss tensor([728.6266], grad_fn=<DivBackward0>)\n",
            "epoch 21 mean loss tensor([695.5078], grad_fn=<DivBackward0>)\n",
            "epoch 22 mean loss tensor([665.2684], grad_fn=<DivBackward0>)\n",
            "epoch 23 mean loss tensor([637.5491], grad_fn=<DivBackward0>)\n",
            "epoch 24 mean loss tensor([612.0471], grad_fn=<DivBackward0>)\n",
            "epoch 25 mean loss tensor([588.5069], grad_fn=<DivBackward0>)\n",
            "epoch 26 mean loss tensor([566.7104], grad_fn=<DivBackward0>)\n",
            "epoch 27 mean loss tensor([546.4707], grad_fn=<DivBackward0>)\n",
            "epoch 28 mean loss tensor([527.6269], grad_fn=<DivBackward0>)\n",
            "epoch 29 mean loss tensor([510.0393], grad_fn=<DivBackward0>)\n",
            "epoch 30 mean loss tensor([493.5865], grad_fn=<DivBackward0>)\n",
            "epoch 31 mean loss tensor([478.1619], grad_fn=<DivBackward0>)\n",
            "epoch 32 mean loss tensor([463.6721], grad_fn=<DivBackward0>)\n",
            "epoch 33 mean loss tensor([450.0347], grad_fn=<DivBackward0>)\n",
            "epoch 34 mean loss tensor([437.1766], grad_fn=<DivBackward0>)\n",
            "epoch 35 mean loss tensor([425.0328], grad_fn=<DivBackward0>)\n",
            "epoch 36 mean loss tensor([413.5454], grad_fn=<DivBackward0>)\n",
            "epoch 37 mean loss tensor([402.6626], grad_fn=<DivBackward0>)\n",
            "epoch 38 mean loss tensor([392.3380], grad_fn=<DivBackward0>)\n",
            "epoch 39 mean loss tensor([382.5295], grad_fn=<DivBackward0>)\n",
            "epoch 40 mean loss tensor([373.1995], grad_fn=<DivBackward0>)\n",
            "epoch 41 mean loss tensor([364.3138], grad_fn=<DivBackward0>)\n",
            "epoch 42 mean loss tensor([355.8414], grad_fn=<DivBackward0>)\n",
            "epoch 43 mean loss tensor([347.7541], grad_fn=<DivBackward0>)\n",
            "epoch 44 mean loss tensor([340.0262], grad_fn=<DivBackward0>)\n",
            "epoch 45 mean loss tensor([332.6343], grad_fn=<DivBackward0>)\n",
            "epoch 46 mean loss tensor([325.5570], grad_fn=<DivBackward0>)\n",
            "epoch 47 mean loss tensor([318.7746], grad_fn=<DivBackward0>)\n",
            "epoch 48 mean loss tensor([312.2690], grad_fn=<DivBackward0>)\n",
            "epoch 49 mean loss tensor([306.0236], grad_fn=<DivBackward0>)\n",
            "epoch 50 mean loss tensor([300.0231], grad_fn=<DivBackward0>)\n",
            "epoch 51 mean loss tensor([294.2534], grad_fn=<DivBackward0>)\n",
            "epoch 52 mean loss tensor([288.7015], grad_fn=<DivBackward0>)\n",
            "epoch 53 mean loss tensor([283.3552], grad_fn=<DivBackward0>)\n",
            "epoch 54 mean loss tensor([278.2033], grad_fn=<DivBackward0>)\n",
            "epoch 55 mean loss tensor([273.2354], grad_fn=<DivBackward0>)\n",
            "epoch 56 mean loss tensor([268.4417], grad_fn=<DivBackward0>)\n",
            "epoch 57 mean loss tensor([263.8134], grad_fn=<DivBackward0>)\n",
            "epoch 58 mean loss tensor([259.3420], grad_fn=<DivBackward0>)\n",
            "epoch 59 mean loss tensor([255.0197], grad_fn=<DivBackward0>)\n",
            "epoch 60 mean loss tensor([250.8390], grad_fn=<DivBackward0>)\n",
            "epoch 61 mean loss tensor([246.7932], grad_fn=<DivBackward0>)\n",
            "epoch 62 mean loss tensor([242.8759], grad_fn=<DivBackward0>)\n",
            "epoch 63 mean loss tensor([239.0809], grad_fn=<DivBackward0>)\n",
            "epoch 64 mean loss tensor([235.4028], grad_fn=<DivBackward0>)\n",
            "epoch 65 mean loss tensor([231.8361], grad_fn=<DivBackward0>)\n",
            "epoch 66 mean loss tensor([228.3758], grad_fn=<DivBackward0>)\n",
            "epoch 67 mean loss tensor([225.0173], grad_fn=<DivBackward0>)\n",
            "epoch 68 mean loss tensor([221.7562], grad_fn=<DivBackward0>)\n",
            "epoch 69 mean loss tensor([218.5883], grad_fn=<DivBackward0>)\n",
            "epoch 70 mean loss tensor([215.5096], grad_fn=<DivBackward0>)\n",
            "epoch 71 mean loss tensor([212.5164], grad_fn=<DivBackward0>)\n",
            "epoch 72 mean loss tensor([209.6052], grad_fn=<DivBackward0>)\n",
            "epoch 73 mean loss tensor([206.7727], grad_fn=<DivBackward0>)\n",
            "epoch 74 mean loss tensor([204.0157], grad_fn=<DivBackward0>)\n",
            "epoch 75 mean loss tensor([201.3313], grad_fn=<DivBackward0>)\n",
            "epoch 76 mean loss tensor([198.7166], grad_fn=<DivBackward0>)\n",
            "epoch 77 mean loss tensor([196.1690], grad_fn=<DivBackward0>)\n",
            "epoch 78 mean loss tensor([193.6858], grad_fn=<DivBackward0>)\n",
            "epoch 79 mean loss tensor([191.2647], grad_fn=<DivBackward0>)\n",
            "epoch 80 mean loss tensor([188.9035], grad_fn=<DivBackward0>)\n",
            "epoch 81 mean loss tensor([186.5997], grad_fn=<DivBackward0>)\n",
            "epoch 82 mean loss tensor([184.3516], grad_fn=<DivBackward0>)\n",
            "epoch 83 mean loss tensor([182.1569], grad_fn=<DivBackward0>)\n",
            "epoch 84 mean loss tensor([180.0139], grad_fn=<DivBackward0>)\n",
            "epoch 85 mean loss tensor([177.9207], grad_fn=<DivBackward0>)\n",
            "epoch 86 mean loss tensor([175.8756], grad_fn=<DivBackward0>)\n",
            "epoch 87 mean loss tensor([173.8770], grad_fn=<DivBackward0>)\n",
            "epoch 88 mean loss tensor([171.9234], grad_fn=<DivBackward0>)\n",
            "epoch 89 mean loss tensor([170.0131], grad_fn=<DivBackward0>)\n",
            "epoch 90 mean loss tensor([168.1448], grad_fn=<DivBackward0>)\n",
            "epoch 91 mean loss tensor([166.3172], grad_fn=<DivBackward0>)\n",
            "epoch 92 mean loss tensor([164.5288], grad_fn=<DivBackward0>)\n",
            "epoch 93 mean loss tensor([162.7785], grad_fn=<DivBackward0>)\n",
            "epoch 94 mean loss tensor([161.0650], grad_fn=<DivBackward0>)\n",
            "epoch 95 mean loss tensor([159.3873], grad_fn=<DivBackward0>)\n",
            "epoch 96 mean loss tensor([157.7441], grad_fn=<DivBackward0>)\n",
            "epoch 97 mean loss tensor([156.1345], grad_fn=<DivBackward0>)\n",
            "epoch 98 mean loss tensor([154.5574], grad_fn=<DivBackward0>)\n",
            "epoch 99 mean loss tensor([153.0118], grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "n_inputs = 4 # <размерность элемента выборки >\n",
        "lr = 0.01 #  скорость обучения\n",
        "n_epoch = 100 #  количество эпох\n",
        "\n",
        "neuron = Neuron(n_inputs)\n",
        "loss = SquaredLoss()\n",
        "\n",
        "losses = []\n",
        "for epoch in range(100):\n",
        "  for x_example, y_example in random.sample(list(zip(X, y)), 20):\n",
        "    # forward pass\n",
        "    y_pred = neuron.forward(x_example) # <прогон через нейрон>\n",
        "    curr_loss = loss.forward(y_pred, y_example) # <прогон через функцию потерь>\n",
        "    losses.append(curr_loss)\n",
        "\n",
        "    # backprop\n",
        "    loss.backward()\n",
        "    neuron.backward(loss.dinput)\n",
        "    # градиентный спуск\n",
        "    neuron.weights -= lr * neuron.dweights\n",
        "    neuron.bias -= lr * neuron.dbias\n",
        "  print(f'epoch {epoch} mean loss {sum(losses) / len(losses)}')\n",
        "    # <вызов методов backward>\n",
        "    # обратите внимание на последовательность вызовов: от конца к началу\n",
        "\n",
        "    # <шаг оптимизации для весов (weights и bias) нейрона>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxWeyJw5lAqU"
      },
      "source": [
        "3.1.2 Воспользовавшись классами `Linear` и `MSELoss` из задачи 2.1.4 и 2.3.1, `ReLU` из 2.2.1 и автоматическим дифференцированием, которое предоставляет `torch`, решить задачу регрессии. Для оптимизации использовать пакетный градиентный спуск. Вывести график функции потерь в зависимости от номера эпохи. Вывести на одном графике исходные данные и предсказанные значения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnlAt1NEQoat"
      },
      "outputs": [],
      "source": [
        "X = torch.linspace(0, 1, 100).view(-1, 1)\n",
        "y = torch.sin(2 * np.pi * X) + 0.1 * torch.rand(X.size()) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nifm0FVB2y5N"
      },
      "source": [
        "## 3.2 Алгоритмы оптимизации в `torch.optim`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5PTTYou3xx8"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oBFfJpmcwfn"
      },
      "source": [
        "3.2.1 Решить задачу 3.1.1, воспользовавшись оптимизатором `optim.SDG` для применения стохастического градиентого спуска"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LFAacdy46bX"
      },
      "source": [
        "3.2.2 Решить задачу 3.1.2, воспользовавшись оптимизатором `optim.Adam` для применения пакетного градиентого спуска. Вывести график функции потерь в зависимости от номера эпохи. Вывести на одном графике исходные данные и предсказанные значения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-qUqdALiN-G"
      },
      "source": [
        "## 3.3 Построение сетей при помощи `torch.nn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vxsck-1M6TAV"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0ICJtarif3_"
      },
      "source": [
        "3.3.1 Решить задачу регрессии, соблюдая следующие условия:\n",
        "\n",
        "1. Оформить нейронную сеть в виде класса - наследника `nn.Module`\n",
        "2. При создании сети использовать готовые блоки из `torch.nn`: слои, функции активации, функции потерь и т.д.\n",
        "3. Для оптимизации использовать любой алгоритм оптимизации из `torch.optim` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1bvXHhO7aWs"
      },
      "outputs": [],
      "source": [
        "X = torch.linspace(0, 1, 100).view(-1, 1)\n",
        "y = torch.sin(2 * np.pi * X) + 0.1 * torch.rand(X.size()) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPUW6fm5jbQd"
      },
      "source": [
        "3.3.2 Решить задачу регрессии, соблюдая следующие условия:\n",
        "\n",
        "1. Оформить нейронную сеть в виде объекта `nn.Sequential`\n",
        "2. При создании сети использовать готовые блоки из `torch.nn`: слои, функции активации, функции потерь и т.д.\n",
        "3. Для оптимизации использовать любой алгоритм оптимизации из `torch.optim` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBwbAEd57a2r"
      },
      "outputs": [],
      "source": [
        "X = torch.linspace(0, 1, 100).view(-1, 1)\n",
        "y = torch.sin(2 * np.pi * X) + 0.1 * torch.rand(X.size()) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQj0oVeLj2A1"
      },
      "source": [
        "## 3.4. Datasets and dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c82tAkXMjajm"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoFPckkp8yhz"
      },
      "source": [
        "3.4.1 Создать датасет, поставляющий данные из задачи 3.1.2. \n",
        "\n",
        "Создать `DataLoader` на основе этого датасета и проверить работоспособность.\n",
        "\n",
        "Воспользовавшись результатами 3.3.1 (или 3.3.2) обучите модель, пользуясь мини-пакетным градиентным спуском с размером пакета (`batch_size`) = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlcwQzCFRvFc"
      },
      "outputs": [],
      "source": [
        "class SinDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __len__(self):\n",
        "    pass\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxz02a3k_VQL"
      },
      "source": [
        "3.4.2 Предсказание цен алмазов\n",
        "\n",
        "3.4.2.1 Создайте датасет на основе файла diamonds.csv. \n",
        "\n",
        "1. Удалите все нечисловые столбцы\n",
        "2. Целевой столбец (`y`) - `price`\n",
        "3. Преобразуйте данные в тензоры корректных размеров\n",
        "\n",
        "3.4.2.2 Разбейте датасет на обучающий и тестовый датасет при помощи `torch.utils.data.random_split`.\n",
        "\n",
        "3.4.2.3 Обучите модель для предсказания цен при помощи мини-пакетного градиентного спуска (`batch_size = 256`). \n",
        "\n",
        "3.4.2.4 Выведите график функции потерь в зависимости от номера эпохи (значение потерь для эпохи рассчитывайте как среднее значение ошибок на каждом батче). Проверьте качество модели на тестовой выборке. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEfTNJQI8emD"
      },
      "outputs": [],
      "source": [
        "class DiamondsDataset(Dataset):\n",
        "  def __init__(self, data):\n",
        "    pass\n",
        "\n",
        "  def __len__(self):\n",
        "    pass\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE81cgQdGM7I"
      },
      "source": [
        "3.4.3 Модифицируйте метод `__init__` датасета из 3.4.2 таким образом, чтобы он мог принимать параметр `transform: callable`. Реализуйте класс `DropColsTransform` для удаления нечисловых данных из массива. Реализуйте класс `ToTensorTransorm` для трансформации массива в тензор."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J02LNj_F8qxK"
      },
      "outputs": [],
      "source": [
        "class DiamondsDataset(Dataset):\n",
        "  def __init__(self, data, transform):\n",
        "    # ....\n",
        "    self.transform = transform\n",
        "    # ....\n",
        "\n",
        "  def __len__(self):\n",
        "    pass\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # ...\n",
        "    sample = self.X[idx], self.y[idx]\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "    # ...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJii-22pHIlU"
      },
      "outputs": [],
      "source": [
        "class DropColsTransform:\n",
        "  def __init__(self, drop):\n",
        "    pass\n",
        "  \n",
        "  def __call__(self, sample):\n",
        "    X, y = sample\n",
        "    # <удаление из X столбцов self.drop>\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZZ-OKrVHnY5"
      },
      "outputs": [],
      "source": [
        "class ToTensorTransform:\n",
        "  def __call__(self, sample):\n",
        "    X, y = sample\n",
        "    # <преобразование X и y в тензоры>\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GssBjT9JHt5g"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "drop = DropColsTransform(drop=[1, 2, 3])\n",
        "to_tensor = ToTensorTransform()\n",
        "dataset = DiamondsDataset(data, transforms.compose([drop, to_tensor]))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
